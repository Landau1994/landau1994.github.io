[{"title":"LUMI-lab","url":"/2026/02/25/LUMI-lab/","content":"\n# Cell 2026 LUMI-lab: A foundation model-driven autonomous platform enabling discovery of ionizable lipid designs for mRNA delivery\n\n## Quick Summary\n> The authors present [[Cell 2026 LUMI-lab]], a fully autonomous self-driving laboratory that integrates a molecular foundation model with high-throughput robotics to discover novel [[ionizable lipids]] for [[mRNA delivery]]. By employing an iterative [[active learning]] workflow that balances exploration and exploitation, the system synthesized and screened over 1,700 lipid candidates, identifying [[brominated lipid tails]] as a potent structural motif. The top candidate, [[LUMI-6]], demonstrated superior pulmonary transfection efficiency, achieving 20.3% gene editing efficacy in mouse lung epithelial cells.\n\n## Key Points\n- Introduction of **LUMI-lab**, a closed-loop platform combining AI, robotics, and biological screening.\n- Development of **LUMI-model**, a transformer-based 3D molecular foundation model adapted for data-sparse environments via continual pretraining.\n- Autonomous synthesis and screening of >1,700 [[Lipid Nanoparticles]] (LNPs) over 10 active learning iterations.\n- Discovery of **brominated tails** as a non-intuitive structural feature that significantly enhances [[endosomal escape]] and mRNA transfection.\n- In vivo validation showed [[LUMI-6]] outperforms clinically approved benchmarks ([[SM-102]] and [[MC3]]) in pulmonary gene editing.\n\n## Methods\n### Data\n- **Pretraining Dataset**: 13,369,320 generic small molecules with 147M 3D conformations (derived from [[Uni-Mol]] dataset).\n- **Continual Pretraining Dataset**: 15,491,072 lipid-like molecules (170M conformations) generated via combinatorial enumeration of the Ugi-4CR reaction space.\n- **Experimental Data**: 1,781 distinct ionizable lipids synthesized and tested for mRNA transfection potency (mTP) in human bronchial epithelial ([[HBE]]) cells.\n\n### Model Architecture\n- **LUMI-model**: A 15-layer [[Transformer]] architecture based on [[Uni-Mol]].\n- **Input**: Atom types and 3D atomic coordinates to capture conformation-aware representations.\n- **Embeddings**: Atom-type aware Gaussian Kernel used to encode pairwise distances, ensuring invariance to global rotation/translation.\n\n### Training Strategy\n- **Stage 1: Unsupervised Pretraining**: Masked atom prediction, 3D position recovery, and contrastive learning on generic molecules.\n- **Stage 2: Continual Pretraining**: Domain adaptation on the lipid-like dataset to prioritize features relevant to LNP engineering.\n- **Stage 3: Active Learning Fine-tuning**: Supervised fine-tuning on experimental data collected during iterations.\n    - **Dual-Plate Strategy**: Each iteration synthesized two plates—one for **exploitation** (high predicted potency) and one for **exploration** (high ensemble uncertainty).\n\n## Results\n| Metric | Value | Baseline ([[SM-102]]/[[MC3]]) |\n|--------|-------|----------|\n| In vivo Gene Editing (Lung Epithelial) | 20.3% | < 5% (estimated from plots) |\n| Top 25% Pearson Correlation (Noise $\\sigma$=8) | 0.51 | ~0.37 ([[LiON]]) |\n| Transfection Efficiency (relative to LUMI-6D) | 1.8-fold higher | N/A |\n\n## Figures\n\n| Figure | Description |\n| ------ | ----------- |\n| Fig 1  | Overview of [[Cell 2026 LUMI-lab]] hardware/software architecture, demonstrating the closed-loop cycle of design, synthesis, formulation, and testing. |\n| Fig 2  | The three-stage training pipeline of [[LUMI-model]] and benchmark comparisons showing superior performance against GNNs and XGBoost. |\n| Fig 3  | Visualization of the dual-plate [[active learning]] strategy (exploitation vs. exploration) and the rapid enrichment of high-potency lipids over 10 iterations. |\n| Fig 4  | UMAP analysis revealing the clustering of [[brominated lipids]] and their high prediction/experimental performance. |\n| Fig 5  | Identification of top candidates (LUMI-1 to LUMI-6) and *in vivo* validation in mice via intratracheal administration. |\n| Fig 6  | Evaluation of [[LUMI-6]] for [[CRISPR-Cas9]] gene editing in [[Ai9 mice]], showing high efficiency in lung epithelial cells. |\n\n## Critical Analysis\n### Strengths\n- **Closed-Loop Integration**: Seamlessly connects computational prediction with robotic execution, removing human bottlenecks in the DMTA (Design-Make-Test-Analyze) cycle.\n- **Data Efficiency**: The foundation model approach allows effective learning from sparse wet-lab data (few-shot learning), a common hurdle in material discovery.\n- **Novel Insight**: The system identified [[bromination]] as a key feature, a modification not typically prioritized in expert-driven lipid design, validating the AI's ability to find non-intuitive SARs.\n- **Robust Validation**: Moved beyond *in vitro* screening to demonstrate significant functional efficacy in live animal models.\n\n### Weaknesses\n- **Fixed Formulation**: The screening used a standardized helper lipid formulation. The paper acknowledges that co-optimizing the lipid structure *and* the formulation ratio simultaneously could yield even better results.\n- **Chemical Space Limits**: Synthesis was restricted to 4-component Ugi reactions. While combinatorial, it doesn't cover all possible lipid chemistries.\n- **Generative Limits**: The model selects from an enumerated library rather than performing *de novo* generative design, potentially limiting the search space to the pre-defined building blocks.\n\n### Questions\n- How transferable is the [[LUMI-model]] to other tissue targets (e.g., liver, spleen, brain) without extensive retraining?\n- Does the bromination motif pose any long-term toxicity or metabolic accumulation risks not captured in the 28-day subchronic toxicity study?\n\n## Connections\n### Related Papers\n- [[Uni-Mol]]: The architectural backbone for the foundation model.\n- [[LiON]]: A graph-based hybrid method used as a baseline comparison.\n- Papers describing [[SM-102]] (Moderna) and [[MC3]] (Alnylam) as industry standards.\n\n### Related Concepts\n- [[Self-Driving Laboratories]] (SDLs)\n- [[Active Learning]]\n- [[Foundation Models]] in Chemistry\n- [[Lipid Nanoparticles]] (LNPs)\n- [[CRISPR-Cas9]] Delivery\n\n### Potential Applications\n- Pulmonary gene therapy (e.g., Cystic Fibrosis).\n- Rapid development of mRNA vaccines for emerging pathogens.\n- Automated discovery of materials for other biomedical applications (e.g., polymer design).\n\n## Notes\n- The \"2026\" date in the citation suggests this is a \"future-dated\" issue or pre-press release metadata provided in the prompt.\n- The use of \"Exploitation\" and \"Exploration\" plates is a clever practical implementation of Bayesian Optimization principles in a high-throughput physical setting.","tags":["note"],"categories":["genomics"]},{"title":"AlphaGenome 简介","url":"/2026/02/25/AlphaGenome/","content":"\n\n\n# Nature 2026 Advancing regulatory variant effect prediction with AlphaGenome\n\n## Quick Summary\n\n> AlphaGenome 是由 Google DeepMind 开发并发表于 _Nature_ 的一种统一的深度学习模型，旨在解决基因组序列建模中“长距离上下文”与“高分辨率预测”之间的权衡难题。该模型接受 **1 Mb** 的 DNA 序列作为输入，能够以 **单碱基分辨率** 同时预测 5,930 个（人类）和 1,128 个（小鼠）功能基因组轨道，涵盖基因表达、剪接（位点、使用率及连接）、染色质可及性、组蛋白修饰、转录因子结合以及 3D 染色体接触图谱。在 26 项变异效应预测（VEP）基准测试中，AlphaGenome 在 **25 项** 上达到或超过了现有的最佳模型（SOTA），并展示了在解释罕见病和癌症（如 T-ALL 中的 _TAL1_ 增强子突变）非编码区变异机制方面的强大能力。\n\n## Key Points\n\n- **统一的全能架构**：在一个模型中同时实现了长序列建模（1 Mb 上下文）和单碱基输出精度，打破了以往模型（如 Enformer 牺牲分辨率，SpliceAI 牺牲长度）的局限。\n    \n- **SOTA 性能**：在 22/24 个基因组轨道预测任务和 25/26 个变异效应预测任务中表现优于现有最佳模型（包括 Borzoi, Orca, Pangolin 等）。\n    \n- **剪接预测的突破**：不仅预测剪接位点，还引入了直接预测 **剪接连接（Splice Junctions）** 的机制，能更准确地识别外显子跳跃等复杂剪接变异。\n    \n- **高效蒸馏**：通过“预训练+蒸馏”策略，将大型集成模型的知识压缩到单个学生模型中，在单个 GPU 上推理时间小于 1 秒，大幅降低了应用门槛。\n    \n\n## Methods\n\n### Data\n\n- **训练数据**：来自 ENCODE, GTEx, FANTOM5, 4D Nucleome 等项目的公开数据。\n    \n- **涵盖模态**：RNA-seq, CAGE, PRO-cap, DNase-seq, ATAC-seq, ChIP-seq (TF & Histone), Hi-C/Micro-C。\n    \n- **物种**：人类 (hg38) 和 小鼠 (mm10)。\n    \n\n### Model Architecture\n\n- **主干**：U-Net 风格的编码器-解码器架构 。\n    \n    - **Encoder**：卷积层将 1 Mb 序列下采样。\n        \n    - **Transformer Tower**：处理长距离依赖关系，并生成用于预测接触图谱的 2D 嵌入。\n        \n    - **Decoder**：通过跳跃连接（Skip connections）将特征上采样回 1 bp 分辨率。\n        \n- **输出头**：针对不同模态在不同分辨率（1 bp, 128 bp, 2048 bp）输出预测结果。特别设计了能够捕捉供体-受体相互作用的剪接连接预测头 。\n    \n\n### Training Strategy\n\n- **两阶段训练**：\n    \n    1. **预训练 (Pre-training)**：在 8 个 TPUv3 核心上进行序列并行训练，学习预测实验轨道数据 。\n        \n    2. **蒸馏 (Distillation)**：训练一个学生模型来模仿“全折叠（all-fold）”教师模型集成的预测。此阶段引入了更强的增强策略（如随机突变），显著提升了模型的鲁棒性和变异效应预测能力 。\n        \n\n## Results\n\n|**Metric**|**Value (AlphaGenome)**|**Baseline (SOTA)**|**Description**|\n|---|---|---|---|\n|VEP Benchmarks|**25/26**|-|在 26 项变异效应预测基准中，25 项达到或超越 SOTA|\n|Gene Expr LFC (Pearson r)|**+14.7%**|(vs Borzoi)|细胞类型特异性基因表达预测的相对提升|\n|Contact Map (Pearson r)|**+6.3%**|(vs Orca)|3D 基因组接触图谱预测的相对提升|\n|eQTL Sign Prediction (auROC)|**0.80**|0.75 (Borzoi)|预测 eQTL 效应方向的准确性|\n|ClinVar (Deep Intronic)|**0.66**|0.64 (Pangolin)|深层内含子致病变异的分类精度 (auPRC)|\n\n## Figures\n\n|**Figure**|**Description**|\n|---|---|\n|Fig 1|模型架构、训练流程（预训练与蒸馏）概览，以及与 SOTA 模型在各项基准上的性能对比摘要 。|\n|Fig 2|展示了模型在 LDLR 等基因座上的高精度轨道预测（包括剪接连接），以及各模态预测值与观测值的高相关性 。|\n|Fig 3|剪接变异预测深入分析：展示了模型如何准确预测导致外显子跳跃的罕见变异，并在 ClinVar 基准测试中领先 。|\n|Fig 4|基因表达变异预测（eQTL）：展示了在 GTEx eQTL 效应值和方向预测上的显著提升，以及在 GWAS 信号解读中的应用 。|\n|Fig 5|染色质状态变异预测（caQTL/bQTL）：展示了对染色质可及性和转录因子结合变异的精准预测 。|\n|Fig 6|跨模态案例分析：解析 T-ALL 中 TAL1 癌基因的非编码突变机制，展示模型如何通过多模态输出揭示致病机理 。|\n\n## Critical Analysis\n\n### Strengths\n\n- **多模态整合**：成功将此前分裂的多个领域（如剪接、表达、3D结构）整合到一个统一框架中，且在各子任务上均不输于专用模型。\n    \n- **分辨率优势**：相比 Borzoi/Enformer，提供了单碱基分辨率的输出，这对于识别精细的剪接位点和 TF 结合位点至关重要。\n    \n- **机制可解释性**：不仅能预测“变异致病”，还能通过多模态输出（如“该变异破坏了 CTCF 结合并改变了 3D 结构从而影响表达”）提供机制解释 。\n    \n\n### Weaknesses\n\n- **远端调控局限**：尽管输入长达 1 Mb，但在预测距离超过 100 kb 的远端调控元件影响时，性能仍有下降，仍是一个挑战 。\n    \n- **组织特异性**：虽然表现优异，但在跨细胞背景下精确复现某些组织特异性模式方面仍有提升空间 。\n    \n- **非编码基因覆盖**：目前的训练和评估主要集中在蛋白编码基因，对 microRNAs 等非编码基因的覆盖不足 。\n    \n\n### Questions\n\n- 如何进一步扩展模型以处理更长范围（>1 Mb）的相互作用，以捕捉超长距离的增强子-启动子互作？\n    \n- 未来的版本是否会整合单细胞数据，以提高在复杂组织中细胞类型特异性的分辨率？\n    \n\n## Connections\n\n### Related Papers\n\n- **Borzoi (Linder et al., 2025)**: 上一代多模态 SOTA，AlphaGenome 的主要对比基线。\n    \n- **Enformer (Avsec et al., 2021)**: 本文第一作者之前的开创性工作，引入了 Transformer 处理长序列。\n    \n- **Orca (Zhou, 2022)**: 3D 基因组预测的专用模型，被 AlphaGenome 在接触图谱任务上超越。\n    \n- **Pangolin / SpliceAI**: 剪接预测领域的标杆，AlphaGenome 在剪接任务上的对比对象。\n    \n\n### Related Concepts\n\n- **Sequence-to-Function**: 从序列直接预测功能的建模范式。\n    \n- **In Silico Mutagenesis (ISM)**: 计算机模拟诱变，用于解析模型预测背后的序列基序（Motif）。\n    \n- **Knowledge Distillation**: 知识蒸馏，本文用于提升模型推理效率和鲁棒性的关键技术。\n    \n\n### Potential Applications\n\n- **全基因组关联分析 (GWAS) 解读**：为非编码区 GWAS 信号提供因果变异和分子机制的假设（如确定 eQTL 的方向）。\n    \n- **临床变异解读**：辅助诊断罕见病，特别是针对那些现有工具难以解释的深层内含子变异或非编码调控变异。\n    \n- **序列设计**：用于设计具有特定组织特异性的增强子或优化反义寡核苷酸（ASO）疗法 。\n    \n\n## Notes\n\n- 这是 AlphaGenome 的正式发表版本（Nature 2026），与之前的 bioRxiv 版本相比，基准测试结果更加完善（如 VEP SOTA 数从 24/26 更新为 25/26）。\n    \n- 论文明确指出，该模型并未在“个人基因组预测（personal genome prediction）”任务上进行基准测试，这是该领域模型的一个已知弱点 。","tags":["note","AIVC"],"categories":["genomics"]},{"title":"test mermaid","url":"/2025/01/15/mermaid_example/","content":"\n<!-- htmlmin:ignore -->\n<pre class=\"mermaid\">\nsequenceDiagram\nAlice-&gt;&gt;John: Hello John, how are you?\nloop Healthcheck\n    John-&gt;&gt;John: Fight against hypochondria\nend\nNote right of John: Rational thoughts!\nJohn--&gt;&gt;Alice: Great!\nJohn-&gt;&gt;Bob: How about you?\nBob--&gt;&gt;John: Jolly good!\n</pre>\n<!-- htmlmin:ignore -->\n\n\n<!-- htmlmin:ignore -->\n<pre class=\"mermaid\">\ngraph TD\n    A[开始] --> B{判断}\n    B -- Yes --> C[执行]\n    B -- No --> D[跳过]\n    C --> E[结束]\n    D --> E\n</pre>\n<!-- htmlmin:ignore -->\n\n","tags":["AI"],"categories":["implementation"]},{"title":"udl-chap3-4","url":"/2025/01/15/udl-chap3-4/","content":"\n# chapter 3: Shallow neural networks\n\n如上一章末尾所言，第3章讨论了浅层神经网络，它比线性回归稍复杂，但能描述更广泛的输入/输出关系。这章主要以浅层神经网络为例，初步介绍神经网络的基本概念。浅层神经网络的架构，激活函数（主要介绍ReLU），以及universal approximation theorem\n\n> 浅层神经网络有一个隐藏层。它们（i）计算输入的多个线性函数，（ii）将每个结果通过一个激活函数，然后（iii）取这些激活的线性组合以形成输出。浅层神经网络通过将输入空间划分为连续的分段线性区域来基于输入 \\( x \\) 进行预测 \\( y \\)。只要有足够的隐藏单元（神经元），浅层神经网络可以任意精确地近似任何连续函数。\n\n> 第4章讨论了深度神经网络，它通过增加更多的隐藏层来扩展本章中的模型。第5-7章描述了如何训练这些模型。\n\n\n# chapter 4: deep neural networks\n\n这更进一步，初步介绍了深层神经网络。\n\n\n$h_1 = a[\\beta_0 + \\Omega_1x]$\n\n$h_2 = a[\\beta_1 + \\Omega_2h_1]$\n\n$h_3 = a[\\beta_2 + \\Omega_2h_2]$\n\n$\\vdots$\n\n$h_K = a[\\beta_{K-1} + \\Omega_{K}h_{K-1}]$\n\n$y = \\beta_K + \\Omega_Kh_K$\n\n<!-- htmlmin:ignore -->\n<pre class=\"mermaid\">\ngraph LR\n    x1((x₁)) --> h11((h₁₁))\n    x1 --> h12((h₁₂))\n    x1 --> h13((h₁₃))\n    x2((x₂)) --> h11\n    x2 --> h12\n    x2 --> h13\n    h11 --> h21((h₂₁))\n    h12 --> h21\n    h13 --> h21\n    h11 --> h22((h₂₂))\n    h12 --> h22\n    h13 --> h22\n    h21 --> y((y))\n    h22 --> y\n    \n    style x1 fill:#f9f,stroke:#333,stroke-width:2px\n    style x2 fill:#f9f,stroke:#333,stroke-width:2px\n    style h11 fill:#bbf,stroke:#333,stroke-width:2px\n    style h12 fill:#bbf,stroke:#333,stroke-width:2px\n    style h13 fill:#bbf,stroke:#333,stroke-width:2px\n    style h21 fill:#fbb,stroke:#333,stroke-width:2px\n    style h22 fill:#fbb,stroke:#333,stroke-width:2px\n    style y fill:#bfb,stroke:#333,stroke-width:2px\n</pre>\n<!-- htmlmin:ignore -->\n\n最重要的是末尾的：\n\n深层神经网络与浅层神经网络的比较：\n\n+ Ability to approximate different functions \n+ Number of linear regions per parameter \n+ Depth efficiency（深层优势，作者在末尾的note部分举了一些理论的例子）\n+ Large, structured inputs (如图像，实际上只能深层)\n+ Training and generalization \n\n> 在本章中，我们首先考虑了当我们组合两个浅层网络时会发生什么。我们论证了第一个网络\"折叠\"输入空间，而第二个网络则对其应用分段线性函数。当输入空间被折叠到自身时，第二个网络的效果会被复制。\n>\n> 我们接着证明了这种浅层网络的组合是深度网络中两层的特例。我们将每一层中的 ReLU 函数解释为在多个位置裁剪输入函数，并在输出函数中创造\"新的点\"。我们引入了超参数的概念，就目前所见的网络而言，超参数包括每层中隐藏单元的数量。\n>\n> 最后，我们比较了浅层和深度网络。我们发现：(i) 两种网络都能以足够的容量近似任何函数；(ii) 深度网络每个参数产生更多的线性区域，使得某些函数能够用深度网络更有效地近似；(iv) 大型、结构化的输入（如图像）最好在多个阶段处理；以及 (v) 在实践中，最好的结果往往是使用具有多层的深度网络获得的。\n>\n> 现在我们已经理解了深度和浅层网络模型，我们将注意力转向训练它们。在下一章中，我们将讨论损失函数。对于任何给定的参数值 $\\varphi$，损失函数返回一个单一数值，表示模型输出与训练数据集的真实预测之间的不匹配程度。在第 6 章和第 7 章中，我们将讨论训练过程本身，即我们如何寻找能最小化这个损失的参数值。","tags":["note","AI"],"categories":["implementation"]},{"title":"北岛-必定有人重写爱情","url":"/2025/01/12/北岛-必定有人重写爱情/","content":"\n《我们》 北岛  \n\n失魂落魄  \n提着灯笼追赶春天  \n\n伤疤发亮，杯子转动  \n光线被创造  \n看那迷人的时刻：  \n盗贼潜入邮局  \n信发出叫喊  \n\n钉子啊钉子  \n这歌词不可更改  \n木柴紧紧搂在一起  \n寻找听众  \n\n寻找冬天的心  \n河流尽头  \n船夫等待着茫茫暮色  \n\n必有人重写爱情","tags":["art"],"categories":["others"]},{"title":"知乎上的有趣的概率论与线性代数问题汇总","url":"/2025/01/12/知乎上的有趣的概率论与线性代数问题汇总/","content":"\n# 概率论\n\n圆上随机取点，在同一个半圆内的概率有多大？ - Dylaaan的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/17491774604","tags":["note"],"categories":["genomics"]},{"title":"udl-chap1-2","url":"/2025/01/12/udl-chap1-2/","content":"\n## 信息\n本书网站见，https://udlbook.github.io/udlbook/\n\n## 如何阅读这本书\n第一章是概论，作者讲了应该如何学习这本书，\n\n> 本书中大多数剩余章节包含主体文本、注释部分和一组习题。\n>\n> 主体文本旨在自成一体，可以在不参考章节其他部分的情况下阅读。尽可能地，背景数学被纳入主体文本中。然而，对于较大的主题，如果它会分散论点的主线，背景材料会被附录处理，并在边缘提供参考。书中大多数符号遵循附录A的符号标准。然而，一些约定不太广泛使用，鼓励读者在继续之前查阅附录A。\n>\n> 主体文本包括许多关于深度学习模型和结果的新插图和可视化。我努力提供现有思想的新解释，而不仅仅是整理他人的工作。深度学习是一个新领域，有时现象并不完全清楚。我会明确指出哪些地方情况不明，以及我的解释应谨慎对待。\n>\n> 参考文献仅在章节主体中包含结果被展示的地方。相反，参考文献可以在章节末尾的注释部分找到。我通常不在主体文本中尊重历史先例；如果某个当前技术的前身不再有用，我将不会提及。然而，领域的历史发展在注释部分中进行了描述，并且希望能公平地给予相应的信用。\n>\n> 注释部分按段落组织，提供进一步阅读的指引。它们应帮助读者在子领域内定位自己，并理解其与机器学习其他部分的关系。与主体文本相比，注释部分的自成体系程度较低。根据你的背景知识和兴趣水平，你可能会发现这些部分的用处有所不同。\n>\n> 每章都有若干相关习题。在主体文本中，当应该尝试习题时，会在边缘进行引用。正如乔治·波利亚所言：\"数学，您看，不是一项观众运动。\"他是对的，我强烈建议您在阅读过程中尝试习题。在某些情况下，它们提供的见解将帮助您理解主体文本。对于在相关网站（http://udlbook.com）上提供答案的习题，会用星号进行标注。此外，帮助您理解本书中思想的Python笔记本也可以通过该网站获得，这些笔记本也在文本的边缘进行了引用。实际上，如果您觉得自己有些生疏，现在可能值得先学习一下关于背景数学的笔记本。\n>\n> 不幸的是，人工智能研究的快速发展使得本书不可避免地将是一项持续的工作。如果您发现某些部分难以理解、明显的遗漏或似乎多余的部分，请通过相关网站与我们联系。我们可以共同努力使下一版更好。\n\n## 有监督学习\n\n> 监督学习模型是一个函数 $y = f(x,\\phi)$，它将输入 $x$ 与输出 $y$ 关联起来，具体关系由参数 $\\phi$ 决定。为了训练模型，我们在训练数据集 $\\{x_i, y_i\\}$ 上定义损失函数 $L(\\phi)$，量化模型预测 $f(x_i,\\phi)$ 与观察到的输出 $y_i$ 之间的差异。接着，我们寻找最小化损失的参数，并在不同的测试数据集上评估模型，以检验其对新输入的泛化能力。\n> \n> 接下来的章节（3-9）扩展了这些概念。第3章讨论了浅层神经网络，它比线性回归稍复杂，但能描述更广泛的输入/输出关系。第4章介绍深层神经网络，能够用更少的参数描述复杂函数，并在实践中表现更好。第5章研究不同任务的损失函数，并揭示最小二乘损失的理论基础。第6和第7章讨论训练过程，第8章探讨如何衡量模型性能，第9章考虑正则化技术，以提高模型性能。\n\n","tags":["note","AI"],"categories":["implementation"]},{"title":"待完成系列博文清单","url":"/2025/01/12/待完成系列博文清单/","content":"\n## 20250112 \n\n待完成的系列博文主题中，除随机过程（这个可能和深度学习的读书笔记一起更新）外，其余两本书的阅读计划结合研究实际进行系统性完善与更新。鉴于近期研究任务繁重，拟先行积累相关研究素材与理论洞见，待条件成熟时再行更新。笔者认为，学术阅读应与实际研究相结合，方能避免思维陷入被动接受的状态，确保学术思想的独立性与创造性。同时，唯有坚定学术信念，持续践行研究规划，方能确保研究工作的系统性与完整性。\n\n- [ ] Dobrow的随机过程\n- [ ] igraph 学习笔记\n- [ ] 医学免疫学\n\n计划的读书笔记系列\n- [ ] 《Understanding Deep Learning》\n- [ ] 《ChatGPT进阶：提示工程入门》主要用于写代码，和一些简单任务\n- [ ] 《我的科研助理：ChatGPT全方位使用指南》主要用于写代码，和一些简单任务","tags":["note"],"categories":["orthers"]},{"title":"cursor与cline","url":"/2025/01/12/cursor与cline/","content":"\n\n## 结论\n就可用性而言，目前用cursor较为方便，一个比较好的实用教程是：[AI编程神器Cursor】不止写代码，5种玩法让你全面提效，小白宝藏!](https://www.bilibili.com/video/BV1rRCVYREFm/?share_source=copy_web&vd_source=6c7840ca8bc4d5854cde3763fbf77fca)\n\ncline一个搞笑的不易用之处，是其需要耗费漫长的时间，等待响应，github上有人说[充钱可以解决](https://github.com/cline/cline/issues/1226)，但是我试了，好像不太行。\n\n\n## CSDN上的一个测评\n\n> 2024年12月，我体验了一下AI编码辅助工具，本文我们将对比分析GitHub Copilot、Cursor和Cline这三款AI工具，评估它们在自动代码生成和AI辅助编码方面的优缺点。\n\n> **GitHub Copilot**\n> 是一款IDE插件，需要结合JetBrains或VS Code使用。\n>\n> **优点**\n> - 高效的代码补全：GitHub Copilot能够实时分析代码上下文并提供建议，帮助开发者快速完成代码块。\n> - 跨语言支持：支持多种编程语言，满足不同开发者的需求。\n> - 学习与成长：Copilot通过不断学习开发者的代码风格和习惯来提高建议质量。\n>\n> **缺点**\n> - 依赖性：过度依赖Copilot可能导致程序员失去自主思考和手动编写代码的能力。\n> - 隐私问题：Copilot需要访问代码库以提供建议，这可能引发隐私担忧。\n> - 成本问题：一个月的免费体验期，之后每个月10美金，或者每年100美金。\n\n> **Cursor**\n> 是一款基于VS Code开发的IDE，可单独下载安装使用。\n>\n> **优点**\n> - 提高开发效率：通过智能补全、自动错误修复和优化建议，开发者可以更快地完成代码编写和调试工作。\n> - 降低错误率：Cursor的代码审查和自动修复功能有助于避免常见的编程错误。\n> - 增强代码可读性：AI的优化建议不仅提升了代码的性能，还能帮助开发者编写更加简洁易读的代码。\n> - 实时反馈与协作：通过与AI的实时对话，开发者可以随时获得帮助。\n> - 价格：有免费版，Pro版本美月20美金。\n>\n> **缺点**\n> - 基础功能缺失：Cursor的基础功能可能不够完善，不能称之为一个可靠的IDE。\n> - 服务不稳定：Cursor的服务可能不够稳定，影响使用体验。\n\n> **Cline**\n> 是一款IDE插件，需要结合JetBrains或VS Code使用。\n>\n> **优点**\n> - 全面的项目支持：Cline不仅提供代码补全，还能执行复杂的软件开发任务，覆盖开发全流程。\n> - 灵活的模型选择：支持多种API提供商和模型，可以根据需求和预算选择最适合的模型。\n> - 成本效益高：特别是使用DeepSeek等模型时，成本显著降低。\n> - 人机协作：每一步操作都需要用户确认，保证了安全性。\n> - 成本：预付费模式，需要绑定银行卡或Paypal，但之后选择Google Gemini模型的话，可以免费使用（本文写作时仍然可以）。\n>\n> **缺点**\n> - 开源劣势：作为开源工具，Cline可能在某些高级功能上受到限制。\n> - 成本上升：由于基于token的消耗模式，随着使用频率的增加，开发成本也可能迅速上升。\n\n> **总结**\n> GitHub Copilot、Cursor和Cline各有其独特的优缺点。GitHub Copilot以其高效的代码补全和跨语言支持著称，但隐私和依赖性问题不容忽视。Cursor通过智能补全和实时协作提高了开发效率，但其基础功能和稳定性有待提升。Cline则以其全面的项目支持和灵活的模型选择脱颖而出，但成本和开源劣势也需考虑。开发者应根据自身需求和偏好选择最适合的工具。\n\n\n\n                            版权声明：本文为转载的博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。            \n                            原文链接：https://blog.csdn.net/xidianjiapei001/article/details/144374563","tags":["note","AI"],"categories":["implementation"]},{"title":"两首由deepseek生成的北岛风格的诗","url":"/2025/01/12/一首北岛风格的诗/","content":"《消解》<br>\n意义像沙漏中的沙<br>\n一粒粒坠落在虚无的深渊<br>\n说教是纸做的剑<br>\n刺向风的胸膛<br>\n碎成苍白的蝴蝶<br>\n确定在镜中模糊成雾<br>\n我们都是雾中的影子<br>\n寻找失去的形状<br>\n在消解的世界里<br>\n我们都是自己的谜题<br>\n<br>\n《习题》<br>\n爱情和尊严<br>\n同时转身<br>\n留下一个<br>\n永远解不开的方程<br>\n清晨的镜子<br>\n照见了<br>\n一个数字的死亡<br>\n最后的算式<br>\n是水泥台阶上的<br>\n血迹<br>\n现在我终于明白<br>\n人生没有标准答案<br>\n只有<br>\n永远解不完的<br>\n习题","tags":["art"],"categories":["others"]},{"title":"artin_review","url":"/2025/01/12/artin-review/","content":"2024.03-2025.01\n# 缘起\n\n见：能说一本或几本在你在学数学路上对你影响最大的一本书吗？ - 万物皆数数海拾贝的回答 - 知乎\nhttps://www.zhihu.com/question/555672024/answer/3418569722\n\n如何从群$Z_n^{\\times}$，环$\\mathbb{Z}_n$，到$\\mathbb{F}_p$，再到Galois理论。你可以将这篇书评视为关于Artin一书的一个学习纲领，特别是在这篇书评的末尾，我们还附上一些近年来出现在国内外的数学考试中的习题以及解答，这些习题没有超过Artin一书包含的内容。\n\nArtin的编排对初学者不是特别友好，比如第二版一开始就是线性代数，或许会让一些已经被线性代数产生审美疲劳的读者劝退，但是等仔细耐着性子读完之后，你会发现，这些线性代数的知识，其实与后面的抽象代数的知识并不是割裂的。笔者在将Artin的书通读了三遍之后，呈现给一个笔者认为较为容易理解的目次。笔者觉得Artin这本书，仅管缺点诸多，与Artin风格类似的，如Dummit，我觉得也很好。但是Artin这本书最大的优点，是将线性代数和抽象代数融合为一体，具体就是通过表示论的知识穿插将两者结合起来，几乎没有一本面向本科的教材，能够做的这么好。\n","tags":["note"],"categories":["math"]},{"title":"learn_python_001","url":"/2021/12/04/learn-python-001/","content":"\n\n# Learn_python_001 Top K problem\n\n### &#x1F7E7;&#x2753; The problem\n\nTop K question:\n\n输入整数数组 arr ，找出其中最小的 k 个数。例如，输入4、5、1、6、2、7、3、8这8个数字，则最小的4个数字是1、2、3、4。\n\n示例 1：\n\n输入：arr = [3,2,1], k = 2\n输出：[1,2] 或者 [2,1]\n示例 2：\n\n输入：arr = [0,1,2,1], k = 1\n输出：[0]\n \n限制：\n\n0 <= k <= arr.length <= 10000\n0 <= arr[i] <= 10000\n\n来源：力扣（LeetCode）\n链接：https://leetcode-cn.com/problems/zui-xiao-de-kge-shu-lcof\n著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明\n\n### &#x1F4C4; code\n\n有三种解法，解法二，三，是符合题目要求的两种（因为题目也考察了排序算法）。详解见https://leetcode-cn.com/problems/zui-xiao-de-kge-shu-lcof/solution/jian-zhi-offer-40-zui-xiao-de-k-ge-shu-j-9yze/\n\n```python\nclass Solution1:\n    def getLeastNumbers(self, arr: List[int], k: int) -> List[int]:\n        lstStd = arr\n        lstStd.sort()\n        res = lstStd[:k]\n        return res\n### solution2:\n### wirte your own quick sort\n### This was taken from https://leetcode-cn.com/problems/zui-xiao-de-kge-shu-lcof/solution/jian-zhi-offer-40-zui-xiao-de-k-ge-shu-j-9yze/\nclass Solution2:\n    def getLeastNumbers(self, arr: List[int], k: int) -> List[int]:\n        def quick_sort(arr, l, r):\n            if l >= r: return\n            i, j = l, r\n            while i < j:\n                while i < j and arr[j] >= arr[l]: j -= 1\n                while i < j and arr[i] <= arr[l]: i += 1\n                arr[i], arr[j] = arr[j], arr[i]\n            arr[l], arr[i] = arr[i], arr[l]\n            quick_sort(arr, l, i - 1)\n            quick_sort(arr, i + 1, r)\n        quick_sort(arr, 0, len(arr) - 1)\n        return arr[:k]\nclass Solution3:\n    def getLeastNumbers(self, arr: List[int], k: int) -> List[int]:\n        if k >= len(arr): return arr\n        def quick_sort(l, r):\n            i, j = l, r\n            while i < j:\n                while i < j and arr[j] >= arr[l]: j -= 1\n                while i < j and arr[i] <= arr[l]: i += 1\n                arr[i], arr[j] = arr[j], arr[i]\n            arr[l], arr[i] = arr[i], arr[l]\n            if k < i: return quick_sort(l, i - 1) \n            if k > i: return quick_sort(i + 1, r)\n            return arr[:k]     \n        return quick_sort(0, len(arr) - 1)\n```\n### &#x1F4cF; 测试\n\n我们用如下代码测试：\n\n```python\n### import require package\nimport numpy as np\nimport random \nimport matplotlib.pyplot as plt\nimport time\nimport seaborn as sns\nfrom typing import List, Dict, Tuple, Sequence\ndef ProgramTime(N,func):\n    lst = [random.randrange(10**7) for n in range(N)]\n    start = time.perf_counter()\n    func(lst,10)\n    runtime = (time.perf_counter() - start)\n    return runtime\nProgramTimeVec = np.vectorize(ProgramTime)\n\n### define theory function\ndef f1(n, k):\n    return k*n\ndef f2(n, k):\n    return k*n*np.log(n)\n\n### plot test curve\nn = np.arange(1, 2000)\ncolors = sns.color_palette(\"Set1\")\n\nplt.plot(n, f1(n, 1e-7), c=colors[0])\nplt.plot(n, f2(n, 1e-7), c=colors[1])\nplt.plot(n, ProgramTimeVec(n,sol1.getLeastNumbers),c=colors[2])\nplt.plot(n, ProgramTimeVec(n,sol2.getLeastNumbers),c=colors[3])\nplt.plot(n, ProgramTimeVec(n,sol3.getLeastNumbers),c=colors[4])\nplt.xlabel('Size of input (n)', fontsize=16)\nplt.ylabel('Time', fontsize=16)\n#plt.legend(['$\\mathcal{O}(n^2)$', '$\\mathcal{O}(n \\log n)$'], loc='best', fontsize=20)\nplt.legend(['$\\mathcal{O}(n)$', '$\\mathcal{O}(n \\log n)$','sol1', 'sol2','sol3'], loc='best', fontsize=20)\nfig = plt.gcf()\nfig.set_size_inches(8, 6)\nplt.savefig(\"../fig/test.png\",dpi=300)\n```\n结果如下, 可以看出，使用解法三，也就是基于快速排序的数组划分，可以实现线性时间：\n\n![figure](https://s3.bmp.ovh/imgs/2021/12/90470f3f3dcdc66c.png)\n","tags":["note","Python"],"categories":["implementation"]},{"title":"Epigenome_track_plot","url":"/2021/10/27/Epigenome-track-plot/","content":"\n# Epigenome track visulalization by R\n\nRecently, I create a new repo to visulizaiton Epigenome track by `ggplot2()` ecosytem. \n\nDetails can be found at https://github.com/Landau1994/PlotEpiTrackByR","tags":["note"],"categories":["genomics"]},{"title":"reading_note_20201206","url":"/2020/12/06/reading-note-20201206/","content":"\t\n  题目：[3D] bioRxiv 2020 3D Genome Contributes to Protein-Protein Interactome\n  - 生物问题：3D 基因组的与蛋白质互作是否存在关系？\n\t\n  - 实验设计：\n    \n    - PPI数据：收集不同数据库蛋白质互作数据，作为正样本；从不同亚细胞定位的非PPI互作蛋白中抽样，作为负样本；\n\t\t\n    - HiC数据：不同细胞系的HiC数据，采用相同方法重新处理，使得互作矩阵分辨率相同；\n\t\t\n    - 从3D基因组信息中，重建基因的空间信息，采用不同的机器学习方法预测PPI数据\n\t\n  - 分析思路：\n  \n    - 分组比较PPI gene counterparts 组和 Non-PPI组的分布图，HiCHeatmap，Gene-gene pair projections of PPIs overlaid on Hi-C heatmaps.，发现了PPI gene counterparts 在空间更为邻近；（Figure2,3,4）；\n\t\t\n    - 分析至少一个蛋白有信号肽的PPI（SigPep PPI）与无信号肽PPI(Non-SigPep PPI)的关系，得到结论是：“This can be explained that for the interacting proteins that are brought together by signal peptides, their gene counterparts can be more freely located on the 3D genome, with larger spatial distances“\n\t\t\n    - 用3维基因组的信息在不同的模型中预测PPI，发现引入3维基因组的信息之后，”\n\t\tthe prediction accuracy in terms of AUC can be significantly improved if 3D genome information is employed“（Table1)，由于是预印本，特征工程的那一部分作者并没有详细写；\n- 评论：\n    - 目前有很多研究组致力于解析3D基因组结构和疾病的关系，报道了一些3D基因组结构改变，影响转录，从而影响疾病的案例；本文的分析虽然简单，但是给出了3D基因组结构对于更为下游的蛋白质互作有影响的可能性。但是这种可能性，还需要更为solid的技术，数据和分析方法来证明；\n\t\t\n    - alphafold是基于序列信息来预测蛋白质结构，目前已经取得重大突破。目前也陆续有基于机器学习的方法，整合不同基因组学的数据进行功能基因组学研究的报道。可能在未来的研究中，结合多组学数据，像alphafold这样的AI框架，才能实现更为深刻的蛋白动态功能的预测；\n","tags":["note"],"categories":["reference"]},{"title":"mmet 2020 Age-related loss of gene-to-gene transcriptional coordination among single-cells","url":"/2020/12/06/readnote/","content":"\n  + 在HSC中没有观察到cell-cell variation随着衰老的改变；For example, despite observations of genetic and epigenteic damage in ageing haematopoietic stem cells(HSCs); cell to cell transcriptional variability is not observed.\n\t\n  + 基因共表达方法的缺陷：\n  \n    + First, co-expression networks estimates the direct or indirect correlations between pairs of genes, while an individual gene may be controlled by multiple regulators.\n  \n    + Second, each co-expression measure is designed to capture a specific feature that is not necessarily optimal for depicting all types of gene-to-gene transcriptional interrelations ( PCC, linear relatiosnhips)\n  \n    + Third, large calculated coexpresion matrices contain a considerable amount of noise, which raises an additional difficulty in explporing their differentiation across cohorts\n\t\n  + Gcl 本质上是对bcdcorr的bootstrap.\n\t\n  + 一个重要的观察和建设：\n\t  As an illustration of the coordination measured by the GCL, consider the expression profiles of cells with N genes, that are represented as points in an N-dimensional space. If the gene expression levels are not independent, the set of points do not fill the N- dimensional space but are rather located near lower-dimensional manifold.\n\t\n  + The GCL measures has two main advantages.\n    + The dependency level is defined with respect to a general dependency form, not specific relations ( such as linear)\n    + It can include high-order dependencies between multiple variables\n","tags":["note"],"categories":["reference"]},{"title":"Learn-igraph-More about igraph","url":"/2020/07/29/Learnigraph-2-MoreOnIgraph/","content":"\n\n#### 0. 说明\n\n我们接着讲更多关于对igraph对象的操作，参考[Statistical Network Analysis with\nigraph](https://sites.fas.harvard.edu/~airoldi/pub/books/BookDraft-CsardiNepuszAiroldi2016.pdf)第一章。\n\n#### 1. 创建igraph 对象\n\n使用管道\n\n``` r\nlibrary(igraph)\nlibrary(igraphdata)\nlibrary(magrittr)\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(ggnetwork)\n\n\n# Notable graphs\n# make_graph can create some notable graphs. The name of the graph (case insensitive), a character scalar must be suppliced as the edges argument, and other arguments are ignored. (A warning is given is they are specified.)\n# eg.\n# Cubical\n# The Platonic graph of the cube. A convex regular polyhedron with 8 vertices and 12 edges.\ng <- make_graph(\"Cubical\") %>%\n  set_vertex_attr(\"name\",value = LETTERS[1:4])\n\ng  %>%\n  add_layout_(with_fr()) %>%\n  plot()\n```\n\n<img src=\"/figure/posts/Learnigraph-2-MoreOnIgraph_files/figure-gfm/unnamed-chunk-1-1.png\" style=\"display: block; margin: auto;\" />\n\n补充：更为实际的案例中，需要使用数据集来创建图。**igraph**作者提供了一些根据数据集创建好的igraph对象：\n\n``` r\nlibrary(igraphdata)\n### data:Loads specified data sets, or list the available data sets.\ndata(package=\"igraphdata\")\n# Data sets in package ‘igraphdata’:\n# \n# Koenigsberg                 Bridges of Koenigsberg from Euler's times\n# UKfaculty                   Friendship network of a UK university faculty\n# USairports                  US airport network, 2010 December\n# enron                       Enron Email Network\n# foodwebs                    A collection of food webs\n# immuno                      Immunoglobulin interaction network\n# karate                      Zachary's karate club network\n# kite                        Krackhardt's kite\n# macaque                     Visuotactile brain areas and connections\n# rfid                        Hospital encounter network data\n# yeast                       Yeast protein interaction network\n```\n\n#### 2. 使用iraph对象查看边和点的信息\n\n``` r\n### already\ndata(\"macaque\")\nmacaque\n```\n\n    ## IGRAPH f7130f3 DN-- 45 463 -- \n    ## + attr: Citation (g/c), Author (g/c), shape (v/c), name (v/c)\n    ## + edges from f7130f3 (vertex names):\n    ##  [1] V1 ->V2     V1 ->V3     V1 ->V3A    V1 ->V4     V1 ->V4t    V1 ->MT    \n    ##  [7] V1 ->PO     V1 ->PIP    V2 ->V1     V2 ->V3     V2 ->V3A    V2 ->V4    \n    ## [13] V2 ->V4t    V2 ->VOT    V2 ->VP     V2 ->MT     V2 ->MSTd/p V2 ->MSTl  \n    ## [19] V2 ->PO     V2 ->PIP    V2 ->VIP    V2 ->FST    V2 ->FEF    V3 ->V1    \n    ## [25] V3 ->V2     V3 ->V3A    V3 ->V4     V3 ->V4t    V3 ->MT     V3 ->MSTd/p\n    ## [31] V3 ->PO     V3 ->LIP    V3 ->PIP    V3 ->VIP    V3 ->FST    V3 ->TF    \n    ## [37] V3 ->FEF    V3A->V1     V3A->V2     V3A->V3     V3A->V4     V3A->VP    \n    ## [43] V3A->MT     V3A->MSTd/p V3A->MSTl   V3A->PO     V3A->LIP    V3A->DP    \n    ## + ... omitted several edges\n\n原作者是这么解释的：\n\n``` \n\nThis is the standard way of showing (printing) an igraph graph object on\nthe screen. The top line of the output declares that the object is an igraph\ngraph, and also lists its most important properties. A four-character long\ncode is printed first:\n\n‘D/U’ The first character is either ‘D’ or ‘U’ and encodes whether the graph\nis directed or undireted.\n‘N’ The second letter is ‘N’ for named graphs (see Section 1.2.5). A dash\nhere means that the graph is not named.\n‘W’ The third letter is ‘W’ if the graph is weighted (in other words, if the\ngraph is a valued graph, Section 2.4). Unweighted graphs have a dash in\nthis position.\n‘B’ Finally, the fourth is ‘B’ if the graph is bipartite (two-mode, Section ??).\nFor unipartite (one-mode) graphs a dash is printed here.\n\nThis notation might seem quite dense at first, but it is easy to get used to and\nconveys much information in a small space. Then two numbers are printed,\nthese are the number of vertices and the number of edges in the graph, 45\nand 463 in our case. At the end of the line the name of the graph is printed,\nif there is any. The next line(s) list attributes, meta-data that belong to the\nvertices, edges or the graph itself. Finally, the edges of the graph are listed.\nExcept for very small graphs, this list is truncated, so that it fits to the screen.\n```\n\n一些基本量的展示，之前讲过，此外，还有更多关于边的操作：\n\n``` r\n###|V|\ngorder(macaque)\n###[1] 45\n###|E|\ngsize(macaque)\n###[1] 463\n\nV(macaque)\n# + 45/45 vertices, named, from f7130f3:\n#  [1] V1     V2     V3     V3A    V4     V4t    VOT    VP     MT     MSTd/p MSTl  \n# [12] PO     LIP    PIP    VIP    DP     7a     FST    PITd   PITv   CITd   CITv  \n# [23] AITd   AITv   STPp   STPa   TF     TH     FEF    46     3a     3b     1     \n# [34] 2      5      Ri     SII    7b     4      6      SMA    Ig     Id     35    \n# [45] 36    \nE(macaque)\n# + 463/463 edges from f7130f3 (vertex names):\n#  [1] V1 ->V2     V1 ->V3     V1 ->V3A    V1 ->V4     V1 ->V4t    V1 ->MT    \n#  [7] V1 ->PO     V1 ->PIP    V2 ->V1     V2 ->V3     V2 ->V3A    V2 ->V4    \n# [13] V2 ->V4t    V2 ->VOT    V2 ->VP     V2 ->MT     V2 ->MSTd/p V2 ->MSTl  \n# [19] V2 ->PO     V2 ->PIP    V2 ->VIP    V2 ->FST    V2 ->FEF    V3 ->V1    \n# [25] V3 ->V2     V3 ->V3A    V3 ->V4     V3 ->V4t    V3 ->MT     V3 ->MSTd/p\n# [31] V3 ->PO     V3 ->LIP    V3 ->PIP    V3 ->VIP    V3 ->FST    V3 ->TF    \n# [37] V3 ->FEF    V3A->V1     V3A->V2     V3A->V3     V3A->V4     V3A->VP    \n# [43] V3A->MT     V3A->MSTd/p V3A->MSTl   V3A->PO     V3A->LIP    V3A->DP    \n# [49] V3A->FST    V3A->FEF    V4 ->V1     V4 ->V2     V4 ->V3     V4 ->V3A   \n# [55] V4 ->V4t    V4 ->VOT    V4 ->VP     V4 ->MT     V4 ->LIP    V4 ->PIP   \n# + ... omitted several edges\n\nmacaque %>% ends(\"V1|V2\")\n# \n#      [,1] [,2]\n# [1,] \"V1\" \"V2\"\n\nmacaque %>% tail_of(\"V1|V2\")\n# + 1/45 vertex, named, from f7130f3:\n# [1] V1\n\nmacaque %>% head_of(\"V1|V2\")\n# + 1/45 vertex, named, from f7130f3:\n# [1] V2\n\nmacaque %>% neighbors(\"V1\",mode = \"out\")\n\n# + 8/45 vertices, named, from f7130f3:\n# [1] V2  V3  V3A V4  V4t MT  PO  PIP\n\nmacaque %>% neighbors(\"V1\",mode = \"in\")\n\n# + 8/45 vertices, named, from f7130f3:\n# [1] V2  V3  V3A V4  V4t MT  PO  PIP\n\nE(macaque)[.from(\"V1\")]\n```\n\n#### 3\\. 子图\n\n创建子图\n\n``` r\nV(macaque)[\"V1\",\"V2\",.nei(\"V1\"),.nei(\"V2\")] %>%\n  induced_subgraph(graph = macaque) %>%\n  summary()\n```\n\n    ## IGRAPH cb88d15 DN-- 16 156 -- \n    ## + attr: Citation (g/c), Author (g/c), shape (v/c), name (v/c)\n\n连通\n\n``` r\nis_connected(macaque,mode = \"weak\")\n```\n\n    ## [1] TRUE\n\n``` r\nis_connected(macaque,mode = \"strong\")\n```\n\n    ## [1] TRUE\n\n边和点的筛选：\n\n``` r\nV(macaque)[1:4]\n# + 4/45 vertices, named, from f7130f3:\n# [1] V1  V2  V3  V3A\nV(macaque)[c(\"V1\",\"V2\",\"V3\",\"V3A\")]\n# + 4/45 vertices, named, from f7130f3:\n# [1] V1  V2  V3  V3\n```\n\n建立边或者点的索引向量：\n\n``` r\nE(macaque)[1:10] %>% as_ids()\n# [1] \"V1|V2\"  \"V1|V3\"  \"V1|V3A\" \"V1|V4\"  \"V1|V4t\" \"V1|MT\"  \"V1|PO\"  \"V1|PIP\"\n#  [9] \"V2|V1\"  \"V2|V3\" \nV(macaque)[1:10] %>% as_ids()\n # [1] \"V1\"     \"V2\"     \"V3\"     \"V3A\"    \"V4\"     \"V4t\"    \"VOT\"    \"VP\"    \n # [9] \"MT\"     \"MSTd/p\"\n```\n\n类似于算数操作，关于点的操作汇总：\n\n``` r\ndata(\"kite\")\nV(kite)\n# + 10/10 vertices, named, from 6b7ddad:\n#  [1] A B C D E F G H I J\nV(kite)[1:3,7:10]\n# + 7/10 vertices, named, from 6b7ddad:\n# [1] A B C G H I J\nV(kite)[degree(kite) < 2]\n# + 1/10 vertex, named, from 6b7ddad:\n# [1] J\nV(kite)[.nei(\"D\")]\n# + 6/10 vertices, named, from 6b7ddad:\n# [1] A B C E F G\nV(kite)[.innei(\"D\")]\n# + 6/10 vertices, named, from 6b7ddad:\n# [1] A B C E F G\nV(kite)[.outnei(\"D\")]\n# + 6/10 vertices, named, from 6b7ddad:\n# [1] A B C E F G\nV(kite)[.inc(\"A|D\")]\n# + 2/10 vertices, named, from 6b7ddad:\n# [1] A D\nc(V(kite)[\"A\"],V(kite)[\"D\"])\n# + 2/10 vertices, named, from 6b7ddad:\n# [1] A D\nrev(V(kite))\n# + 10/10 vertices, named, from 6b7ddad:\n#  [1] J I H G F E D C B A\nunique(V(kite)[\"A\",\"A\",\"C\",\"C\"])\n# + 2/10 vertices, named, from 6b7ddad:\n# [1] A C\n\n### Set operation\n\nunion(V(kite)[1:5],v(kite)[6:10])\n# + 2/10 vertices, named, from 6b7ddad:\n# [1] A C\nintersection(V(kite)[1:7],V(kite)[5:10])\n# + 3/10 vertices, named, from 6b7ddad:\n# [1] E F G\ndifference(V(kite),V(kite)[1:5])\n# + 5/10 vertices, named, from 6b7ddad:\n# [1] F G H I J\n\n\nE(kite)\n# + 18/18 edges from 6b7ddad (vertex names):\n#  [1] A--B A--C A--D A--F B--D B--E B--G C--D C--F D--E D--F D--G E--G F--G F--H G--H\n# [17] H--I I--J\n\nE(kite,path = c(\"A\",\"D\",\"C\"))\n# + 2/18 edges from 6b7ddad (vertex names):\n# [1] A--D C--D\n\nE(kite)[ V(kite)[1:2] %--%  V(kite)[3:4] ]\n# + 3/18 edges from 6b7ddad (vertex names):\n# [1] A--C A--D B--D\n\nE(kite)[1:3,7:10]\n# + 7/18 edges from 6b7ddad (vertex names):\n# [1] A--B A--C A--D B--G C--D C--F D--E\n\nE(kite)[seq_len(gsize(kite))[seq_len(gsize(kite)) %%2 == 0]]\n# + 9/18 edges from 6b7ddad (vertex names):\n# [1] A--C A--F B--E C--D D--E D--G F--G G--H I--J\n\nE(kite)[seq_len(gsize(kite)) %%2 == 0]\n# + 9/18 edges from 6b7ddad (vertex names):\n# [1] A--C A--F B--E C--D D--E D--G F--G G--H I--J\n\nE(kite)[seq_len(gsize(kite)) %%2]\n\n# + 9/18 edges from 6b7ddad (vertex names):\n# [1] A--B A--B A--B A--B A--B A--B A--B A--B A--B\n\nE(kite)[.inc(\"D\")]\n# + 6/18 edges from 6b7ddad (vertex names):\n# [1] A--D B--D C--D D--E D--F D--G\n\nE(macaque)[.from(\"V1\")]\n# + 8/463 edges from f7130f3 (vertex names):\n# [1] V1->V2  V1->V3  V1->V3A V1->V4  V1->V4t V1->MT  V1->PO  V1->PIP\nE(macaque)[.to(\"V1\")]\n# + 8/463 edges from f7130f3 (vertex names):\n# [1] V2 ->V1 V3 ->V1 V3A->V1 V4 ->V1 V4t->V1 MT ->V1 PO ->V1 PIP->V1\n\n### The remains are same as Vertices operations\n```\n","tags":["Graph","Network"],"categories":["implementation"]},{"title":"LearnSeurat_Intergrate","url":"/2020/05/27/LearnSeurat-Intergrate/","content":"\n\n### 0. 说明\n\n更为详细的使用，可以参考Seurat官方教程，或者[Seurat进行单细胞RNA-seq数据整合](https://blog.mrdoge.cool/index.php/2020/02/28/seurat_integration/)。\n\n### 1. 快速进行\n\n实际分析中，如果细胞数目很多的话，进行整合分析的时候就会很慢，一种方式是将任务提交通过命令行提交到服务器或者集群上运行，并输出。\n\n传递命令行的参数的脚本可以这么写：\n\n```R\n    #### reference base Integration\n    .libPaths(\"/home/user/lib/R/library\")\n    library(Seurat)\n    \n    args = commandArgs(trailingOnly=TRUE)\n    # test if there is at least one argument: if not, return an error\n    if (length(args)==0) {\n      stop(\"At least one argument must be supplied (input file).n\", call.=FALSE)\n    } \n    \n    ###args[1] seu.list.rds\n    ###args[2] seu.intergrated.rds\n    ###args[3] UMAPplot.pdf\n    \n    ## program...\n    seu.list <- readRDS(file = args[1])\n    \n    for (i in names(seu.list)) {\n      seu.list[[i]] <- SCTransform(seu.list[[i]], verbose = FALSE)\n    }\n    seu.list.features <- SelectIntegrationFeatures(object.list = seu.list, \n                                                   nfeatures = 3000)\n    seu.list <- PrepSCTIntegration(object.list = seu.list, \n                                   anchor.features = seu.list.features)\n    \n    reference_dataset <- 1\n    names(seu.list)[1]\n    \n    seu.list.anchors <- FindIntegrationAnchors(object.list = seu.list, \n                                               normalization.method = \"SCT\", \n                                               anchor.features = seu.list.features, \n                                               reference = reference_dataset)\n    \n    seu.list.integrated <- IntegrateData(anchorset = seu.list.anchors, \n                                         normalization.method = \"SCT\")\n    \n    seu.list.integrated <- RunPCA(object = seu.list.integrated, verbose = FALSE)\n    seu.list.integrated <- RunUMAP(object = seu.list.integrated, umap.method = \"umap-learn\",\n                                   dims = 1:30)\n    saveRDS(seu.list.integrated,file = args[2])\n    \n    #### show Integrated result\n    plots <- DimPlot(seu.list.integrated, \n                     group.by = c(\"orig.ident\", \"cell.type\"))\n    plots & theme(legend.position = \"top\") & \n      guides(color = guide_legend(nrow = 4, byrow = TRUE, \n                                  override.aes = list(size = 2.5)))\n    ggsave(filename = args[3],width = 12,height = 6)\n```\n\n将其命名为`Integrated_ref_based.R`\n\n然后在终端输入`nohup Rscript Integrated_ref_based.R seu.list.rds\nseu.intergrated.rds UMAPplot.pdf > log.txt &`提交即可；\n\n### 2. 评论\n\n当然，这个脚本读者还可以根据自己需求加以完善。\n","tags":["R","scRNA-seq"],"categories":["implementation"]},{"title":"如何定义细胞类型","url":"/2020/05/16/如何定义细胞类型/","content":"\n生物学家的传统艺能是进行分类，即使在进入了21世纪，研究的尺度和研究的手段和林奈已经有了很大的不同，但是分类这个古老的问题依然是众多生物研究领域的基础。作为分类这个大问题中的子问题，细胞类型的鉴定是当前细胞生物学研究的一个基础问题和前沿问题。\n\n比如发育生物学和干细胞生物学的研究，确定细胞的谱系是研究细胞命运决定因素等基础研究以及相关临床转化的基础。这个基础有多重要的呢，假如某个大牛声称自己发现了某种神奇的细胞，这个细胞可以带来医学革命，然而这种类型的细胞很可能就不存在，所为的带来革命的宣传都是忽悠和骗局,基于这种类型的细胞的研究都是建在沙子上的高楼大厦。以上不是我们的想象，而是确实发生的现实案例[哈佛大学由于心肌干细胞不存在而大量撤稿，国内所有阳性指标论文是否都涉嫌造假？](https://www.zhihu.com/question/298711327)\n\n最近，Cell Stem Cell杂志上发表了一篇[综述](https://linkinghub.elsevier.com/retrieve/pii/S1934-5909(20)30137-5)，阐述了细胞身份(Cellular Identity)研究的目的，以及基于高通量测序（特别是单细胞测序），成像以及遗传学等新方法来确定细胞身份的方法。\n\n在作者看来，细胞类型鉴定研究的目的分为三条：（图片和引用的话均来自于文献）\n\n> (1) Detecting features assoicated with a cell type from a pre-defined list of candidates;\n> \n> (2) Identifying new features and cell types through unbiased approaches;\n> \n> (3) Defining Cellular Relationships\n\n从原理上，鉴定细胞身份的不同feature可以概括如下：\n\n![Figure 1. Defining Cell Types](https://imgkr.cn-bj.ufileos.com/38ef9067-49cf-4781-bcf9-9d413ce0d476.png)\n\n某个细胞类型的feature可以如何研究呢，见下：\n\n![Figure 2. Strategies to Detect Molecular Features Associated with a Cell Type](https://imgkr.cn-bj.ufileos.com/b4350e21-1b8f-44d1-bb4f-7484f6b65a0a.png)\n\n不同类型之间的细胞之间的关系的研究见下：\n\n![Figure 3. Strategies to Define Cellular Relationships](https://imgkr.cn-bj.ufileos.com/d01f185b-b288-46b1-b860-348dbd91e01e.png)\n\n当然同一类型的细胞在不同组织或者生理条件下会呈现不同的功能。\n\n![Figure 4. Cellular Functions Vary with Context](https://imgkr.cn-bj.ufileos.com/a7c9ab59-a271-4686-8738-db98cea163ec.png)\n\n讨论部分最喜欢的一段话是：\n> Together, our rapidly expanding capability to detect features and functions are revealing that \"cell type\" that were percevied as monolithic and stable in fact represent composites of multiple cells with distinguishable molecular signatures and have teh capability to adopt new features and functions in new contexts\n\n更多内容请阅读原文。","tags":["note","scRNA-seq","sc-seq","cell biology"],"categories":["reference"]},{"title":"Learn-SciBet","url":"/2020/05/15/Learn-scibet/","content":"\n\n### 1. 背景介绍\n\n细胞类型注释是scRNA-seq里非常基础的一步，常见的策略是将基于无监督分群结果的基因差异表达分析结果作为marker,结合先验知识，判断该分群为何种类型。\n\n近年来，随着scRNA-seq的数据集的逐渐积累，也有很多研究组提出了一些基于已有分群结果进行有监督的细胞类型注释的方法，比如`scmap`，或者Seurat里的`TransferData`。此外，也有[评估相关方法的benchmark研究](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1795-z)：\n\n![](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs13059-019-1795-z/MediaObjects/13059_2019_1795_Fig1_HTML.png)\n\n[SciBet](http://scibet.cancer-pku.cn/)是张泽民老师组最近新发表的一个快捷（实测真的比Seurat的`TransferData`快，而且效果确实差不多），方便的进行有监督的细胞类型注释的方法。论文见[SciBet as a portable and fast single cell type identifier](https://www.nature.com/articles/s41467-020-15523-2)，相关报道见[Nature Communications | 张泽民课题组发表单细胞转录组数据快速注释新方法](https://mp.weixin.qq.com/s/5Nmvzyk3_t9-eOawHZ-uGQ)\n\n### 2. 方法原理简介\n\n如果读者只对软件使用感兴趣的本节可以略过。\n\n根据上面提到的那篇[报道](https://mp.weixin.qq.com/s/5Nmvzyk3_t9-eOawHZ-uGQ)，如果用一句话概括SciBet的原理，应该是这样的：\n\n> 张泽民实验室的博士生李辰威、刘宝琳联合任仙文副研究员开发的SciBet则有效地解决了这一问题：他们从“同一类型的单细胞表达谱服从同一多项分布”这一基本假设出发，对训练集数据中不同细胞类型分别进行建模，进而通过极大似然估计来对测试集细胞进行有监督注释。\n\n接下来我们根据作者论文的方法部分和补充材料，学习一下这个巧妙地思路。分为如下几个部分：\n\n+ 预备知识\n+ 单细胞表达谱的统计模型\n+ 根据表达谱计算信息熵\n+ E-test\n+ 构建有监督的细胞类型分类模型\n\n我们假定读者已经修过概率论等相关课程。\n\n#### 2.1 预备知识\n\n\n预备知识1：\n\n我们需要向读者回顾关于负二项分布的知识，如果一个离散随机变量$Y$的pmf(probability mass function)为:\n\n$$ P(Y=y) = {y+r-1 \\choose y}p^r(1-p)^{y},y=0,1,\\dots,r=1,2,\\dots$$\n\n那么我们称其服从负二项分布。记为$Y \\sim NB(r,p)$。\n\n预备知识2：\n\n对于两个随机变量$X$和$Y$，在给定$X=x$下，Y的条件pmf为：\n\n$$P(Y=y|X=x)=\\frac{P(X=x,Y=y)}{P(X=x)}$$\n\n预备知识3：\n\n关于负二项分布，有如下结论：\n\n> 对于随机变量$Y,\\Lambda$，若$Y|\\Lambda \\sim Poisson(\\Lambda), \\Lambda \\sim Gamma(\\alpha,\\beta)$, 则 $Y \\sim NB(r=\\alpha,p=\\frac{\\beta}{1+\\beta})$\n其中\n> $$ Poisson(Y=y;\\Lambda=\\lambda) = \\frac{e^{-\\lambda}\\lambda^y}{y!},y \\ge 0,  y\\in N $$\n> $$ Gamma(\\Lambda=\\lambda;a,b)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta\\lambda},\\lambda>0,\\alpha >0,\\beta >0 $$\n> $$ \\int_{0}^{\\infty}\\lambda^{\\alpha-1}e^{-\\beta\\lambda}d\\lambda=\\frac{\\Gamma(\\alpha)}{\\beta^{\\alpha}} $$\n\n证明：\n\n因为：\n\n$$\\begin{aligned}\n  P(Y=y) \n  &=\\int_{0}^{\\infty}P(Y=y,\\Lambda=\\lambda)d\\lambda \\\\\n  &=\\int_{0}^{\\infty}P(Y=y|\\Lambda=\\lambda)P(\\Lambda=\\lambda)d\\lambda \\\\\n  &=\\int_{0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^y}{y!}\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta\\lambda}d\\lambda \\\\\n  &=\\frac{\\beta^{\\alpha}}{y!\\Gamma(\\alpha)}\\int_{0}^{\\infty}\\lambda^{(y+\\alpha)-1}e^{-(1+\\beta)\\lambda}d\\lambda \\\\\n  &= \\frac{\\beta^{\\alpha}}{y!\\Gamma(\\alpha)}\\frac{\\Gamma(y+\\alpha)}{(1+\\beta)^{y+\\alpha}}\\\\\n  &={y+\\alpha-1 \\choose y}(\\frac{\\beta}{1+\\beta})^{\\alpha}(\\frac{1}{1+\\beta})^{y}\n\\end{aligned}\n$$\n所以 $Y \\sim NB(r=\\alpha,p=\\frac{\\beta}{1+\\beta})$\n\n根据预备知识3，我们有：\n\n预备知识4：\n\n> 负二项分布是泊松分布和伽马分布的混合分布。\n\n预备知识5(该结论为概率论和数理统计的常识)：\n\n> 对于随机变量的pdf为$f_X(x)$，做变换$Y=g(X)$,其中$g$为单调函数，X和Y各自样本空间的$\\mathcal{X}$和$\\mathcal{Y}$分别满足：\n> $$\\mathcal{X}=\\{x|f_X(x)>0\\},\\mathcal{Y}=\\{y|y=g(x),x\\in\\mathcal{X} \\}$$\n> 若$f_X(x)$在$\\mathcal{X}$中连续且逆变换$g^{-1}(x)$在$\\mathcal{Y}$中连续可微，则新的随机变量$Y$的pdf为：\n> $$ f_Y(y) =\\begin{cases} f_X(g^{-1}(y))|\\frac{d}{dy}g^{-1}(y)|,y\\in\\mathcal{Y} \\\\ 0, otherwise \\end{cases} $$ \n\n该结论为Statistical Inference(Casella Berger著)中的定理2.1.5，证明见该书。\n\n预备知识6（只有这个结论作者在补充材料里给了证明）：\n\n> **The scaling property of the Gamma distribution**\n> \n> 若 $X \\sim Gamma(\\alpha,\\beta)$, 则$Y=kX \\sim Gamma(\\alpha,\\frac{\\beta}{k}),k>0$\n\n证明: 由题设：\n\n$$ f_X(x;\\alpha,\\beta)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x},\\alpha >0,\\beta >0 , x>0$$\n\n根据题设，由预备知识5,有：\n\n$$ \\begin{aligned}f_Y(y) &= f_X(g^{-1}(y))|\\frac{d}{dy}g^{-1}(y)| \\\\ &= f_X(y/k)|\\frac{d}{dy}(y/k)| \\\\ &=\\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}(\\frac{y}{k})^{\\alpha-1}e^{-\\frac{\\beta}{k}y}\\frac{1}{k} \\\\ &= \\frac{1}{\\Gamma(\\alpha)(k\\beta)^{\\alpha}}x^{\\alpha-1}e^{-\\frac{\\beta}{k}y} \\end{aligned} $$\n\n故$Y \\sim  Gamma(\\alpha,\\frac{\\beta}{k})$\n\n预备知识6：\n\n> 若独立同分布样本$X_1,\\dots,X_n$服从$X_i \\sim Poisson(\\lambda)$, 且观测值分别为$x_1,\\dots,x_n$，则参数$\\lambda$的矩估计量和极大似然估计量相等，均为$\\bar{x}=\\frac{1}{n}\\sum_{i=1}^nx_i$\n\n证明很简单，在很多教材也能找到，故从略。\n\n预备知识7：\n\n> 若独立同分布样本$X_1,\\dots,X_n$服从$X_i \\sim Gamma(\\alpha,\\beta)$, 且观测值分别为$x_1,\\dots,x_n$，则参数$\\alpha,\\beta$的极大似然估计满足：\n> $$ \\hat{\\beta}=\\frac{\\hat{\\alpha}}{\\bar{x}},\\bar{x}=\\frac{1}{n}\\sum_{i=1}^nx_i $$\n\n\n\n证明：\n\n由题设：\n\n$$f_{X_i}(x_i;\\alpha,\\beta)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x_{i}^{\\alpha-1}e^{-\\beta x_i}$$\n\n故取对数后极大似然函数为：\n\n$$ \\begin{aligned}l(\\alpha,\\beta) &=\\ln L(\\alpha,\\beta) \\\\ &= \\ln \\prod_{i=1}^{n} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x_{i}^{\\alpha-1}e^{-\\beta x_i} \\\\ &=n(\\alpha\\ln\\beta-\\ln\\Gamma(\\alpha))+(\\alpha-1)\\sum_{i=1}^{n}\\ln x_i - \\beta\\sum_{i=1}^{n}x_i \\end{aligned}$$\n\n由 \n$$ \\frac{\\partial}{\\partial\\beta}l(\\hat{\\alpha},\\beta) = 0$$\n\n解得 $\\hat{\\beta}=\\frac{\\hat{\\alpha}}{\\bar{x}}$\n\n预备知识8：\n\n> 若随机变量$X$的pdf为$f_X(x)$,样本空间为$\\mathcal{X}$, 定义`information generating function`:\n> $$ T(u)=\\int_{\\mathcal{X}}f_{X}^{u}(x)dx $$\n> 则：\n> $$H(X)=-T^\\prime(1)$$\n\n证明：(严格证明的话，得要考虑函数$f_X(x)$和$\\mathcal{X}$本身的性质，我们这里假定它们可以满足积分号下求导，如果要严格的话，请参考测度论相关教材)\n\n$$ \\begin{aligned}\\frac{d}{du}T(u &)=\\frac{d}{du}\\int_{\\mathcal{X}}f_{X}^{u}(x)dx \\\\ &=\\int_{\\mathcal{X}}\\frac{d}{du}e^{u\\ln f_X(x) }dx \\\\ &=\\int_{\\mathcal{X}}f_{X}^{u}(x)\\ln f_{X}(x)dx  \\end{aligned}$$\n\n而随机变量$X$的信息熵的定义为：\n\n$$ H(X)=-\\int_{\\mathcal{X}} f_{X}(x)\\ln f_{X}(x)dx$$\n\n故$H(X)=-T^\\prime(1)$\n\n预备知识9：\n\n> 若$X\\sim Gamma(\\alpha,\\beta)$，则其信息熵$S$为\n> \n> $S=\\alpha-\\ln\\beta+\\ln\\Gamma(\\alpha)+(1-\\alpha)\\psi(\\alpha),\\psi=\\frac{\\Gamma^{\\prime}(a)}{\\Gamma(a)}$\n\n由预备知识8经过计算即可证明，从略。\n\n预备知识10：\n\n> [多项分布](https://zhuanlan.zhihu.com/p/52481385)\n\n\n\n注：更多关于伽马分布的知识可见：[理解Gamma分布、Beta分布与Dirichlet分布](https://zhuanlan.zhihu.com/p/37976562)\n\n\n#### 2.2 单细胞表达谱的统计模型\n\n经过大量的统计分析和后续的实验验证（相关证据可参考这篇[文献](https://www.nature.com/articles/nmeth.2930)）有这样一个经验性的结论：\n\n**观察到单细胞基因表达的count(比如UMI count)的分布可以用负二项分布很好的拟合,且相同细胞类型的单细胞表达谱服从同一个分布。**\n\n结合2.1中的预备知识，我们可以将单细胞基因表达的count表示为泊松分布的伽马分布的混合分布。所以作者参考了[SAVER](https://www.nature.com/articles/s41592-018-0033-z)可以进行如下建模：\n\n假设我们现在有$C$个细胞，$m$个基因的原始表达谱数据，里面的数值为reads count或者是UMI count。如果我们记观察得到细胞c的某个基因i的UMI count为$Z_{ic},i=1,\\dots,m,c=1,\\dots,C$，那么对于同一类型的细胞而言，有：\n\n$$ Z_{ic} \\sim Poisson(\\lambda_is_c), \\lambda_i \\sim Gamma(\\alpha_i,\\beta_i) $$\n\n其中$\\lambda_i$表示基因$i$在细胞$c$中的真实表达量，$s_c=\\sum_{c}Z_{ic}$表示这个细胞中的UMI总数，与测序深度有关。而$\\alpha_i,\\beta_i$则是两个参数表征某个细胞类型中的基因的真实表达分布的参数。\n\n接下来的可以根据预备知识里的结论进行参数估计：\n\n由预备知识6，我们很容易得到$\\lambda_{ic}=\\frac{Z_ic}{s_c}$这也为我们常用的进行normalized的策略提供了一种依据。而由预备知识7，有$\\hat{\\beta_i}=\\frac{\\alpha_i}{E(\\lambda_i)},E(\\lambda_i)=\\frac{1}{C}\\sum_{c}\\lambda_{ic}$。\n\n\n#### 2.3 根据表达数据计算信息熵\n\n不是所有的基因都是对后续的统计学习有用的，需要进行特征选择，也就是说，挑选出那些能表示不同群细胞之间表达差异的基因。本文的新意是基于信息熵(也就是香农熵)的概念引入了新的进行特征选择的方法：E-test。在讲E-test之前，我们需要看看作者是如何实现从表达量中计算信息熵的。\n\n根据2.2我们知道可以将观测得到单细胞表达gene count$Z_{ic}$表示成$Z_{ic}$与真实表达量$\\lambda_{ic}$的混合分布，真正能反映不同细胞之间表达差异的是$\\lambda_{ic}$的分布。所以接下来要计算$\\lambda_{ic}$的分布的信息熵。\n\n对于相同类型的细胞而言，根据2.1中的结论9，真实表达量$\\lambda_i$的信息熵为：\n\n$$ S_i = \\alpha_i - \\ln\\beta_i + \\ln\\Gamma(\\alpha_i)+(1-\\alpha_i)\\psi(\\alpha_i) \\tag{1}$$\n\n用$\\hat{\\beta_i}=\\frac{\\alpha_i}{E(\\lambda_i)}$代入（1）可得：\n\n$$ S_i = \\alpha_i - \\ln \\alpha_i + \\ln E(\\lambda_i) + \\ln\\Gamma(\\alpha_i)+(1-\\alpha_i)\\psi(\\alpha_i) \\tag{2} $$\n\n记：\n$$ h_i(\\alpha_i) = \\alpha_i - \\ln \\alpha_i  + \\ln\\Gamma(\\alpha_i)+(1-\\alpha_i)\\psi(\\alpha_i) \\tag{3} $$\n\n\n并且记$C$个细胞的平均normalized表达量为$X_i$, 显然有$X_i=E(\\lambda_i)=\\frac{1}{C}\\sum_{c}\\lambda_{ic}$\n\n根据上述记号，（1）最终化简为：\n\n$$ S_i = \\ln E(\\lambda_i)+h_i = \\ln X_i + h_i \\tag{4} $$\n\n接下来，我们考虑道不同的细胞类型。若细胞$c,c=1,\\dots,C_j$属于细胞类型$j$，定义所有属于$j$的细胞的平均标准化的表达量为$X_{ij}=\\frac{1}{C_j}\\sum_{c\\in j}\\lambda_{ic}$，根据上面的结论，可得：\n\n$$ S_{ij} = \\ln X_{ij} + h_{ij} \\tag{5}$$\n\n其中\n\n$$ h_{ij}=\\alpha_{ij}-\\ln\\alpha_{ij}+\\ln\\Gamma(\\alpha_{ij})+(1-\\alpha_{ij})\\psi(\\alpha_{ij}) \\tag{6} $$\n\n$X_{ij}$是直接可以通过实验数据计算的，而$h_{ij}$则需要估计。作者假设$h_{ij}$是只是基因特异的参数，也就是$h_{ij}=h_i$。理由如下：\n\n若$\\lambda_{i,c \\in j}$为随机变量$\\lambda_{ij}\\sim\\Gamma(\\alpha_{ij},\\beta_{ij})$的观测值，如果我们记基因$i$从细胞类型$j$到细胞类型$j^\\prime$的表达量的fold change为$F_{i,j\\rightarrow j^{\\prime}}$, 并且假设$\\lambda_{ij^{\\prime}}=F_{i,j\\rightarrow j^{\\prime}}\\lambda_{ij}$，那么由2.1中的预备知识6，可知 $\\lambda_{ij^{\\prime}}\\sim\\Gamma(\\alpha_{ij},\\frac{\\beta_{ij}}{F_{i,j\\rightarrow j^{\\prime}}})$。\n故$\\alpha_{ij}=\\alpha_i$, 结合（6），$h_{ij}=h_i$\n\n最终，我们可以得到\n\n $$ S_{ij}=\\ln X_{ij}+h_{i} \\tag{7} $$\n\n#### 2.4 E-test\n\n首先，零假设为所有不同类型细胞都是从同一个细胞类型（记为 group 0）中均匀随机采样，那么基因的平均表达量为$X_{i0}=\\frac{1}{n}\\sum_{j=1}^nX_{ij}= AM_i$, 则group 0 的信息熵可以计算为：\n\n$$ S_{i0} = \\ln X_{i0}+h_i  \\tag{8}$$\n\n接着计算基因$i$在所有细胞类型$j$中与group 0 的信息熵的差之和：\n\n(8)-(7)并求和，得：\n\n$$ \\begin{aligned}\n  \\Delta S_i &= \\sum_{j=1}^{n}(S_{i0}-S_{ij}) \\\\\n  & = \\sum_{j=1}^n(\\ln X_{i0}-\\ln X_{ij}) \\\\\n  & = n(\\ln X_{i0}-\\frac{1}{n}\\ln \\prod_{j=n}^nX_{ij}) \\\\\n  & = n\\ln\\frac{AM_i}{GM_i}\n\\end{aligned} $$\n\n其中$GM_i=(\\prod_{j=n}^nX_{ij})^{\\frac{1}{n}}$\n\n利用Jesen不等式可以证明$GM_i\\le AM_i$，故$\\Delta S \\ge 0$\n\n若要进行假设检验，还需要计算$\\Delta S$的显著性，作者的策略是基于置换检验的：\n\n若所有预先定义分群的细胞类型的细胞均来自同一个样本，则对任意的标签为细胞类型$j$的细胞的size-factor normalized的表达量$X_{ij}=\\frac{1}{C_j}\\sum_{c\\in j}\\lambda_{ic}$，根据中心极限定理，单细胞数目足够多的时候，$X_{ij} \\sim N(\\mu_i,\\sigma_i)$，很容易得到参数的无偏估计$\\hat{\\mu_i}=X_{i0},\\hat{\\sigma}_i=\\frac{1}{n-1}\\sum_{j=1}^n(X_{ij}-\\hat{\\mu_i})^2$，所以置换被简化成了每次从分布$N(\\hat{\\mu_i},\\hat{\\sigma_i})$n个不同的随机数$X_{i}$。接下来就是这么生成一个$\\Delta S$的分布，然后计算这个分布中大于从真实数据中测得的$\\Delta S$比例，作为显著性。\n\n默认的Feature为500个基因。\n\n#### 2.5 构建有监督学习的模型\n\n根据2.4，在训练集中，我们可以进行特征选择选出一些“informative gene”进行模型训练。\n\n作者假设从相同的转录出的mRNA是不可区分的，而且每个mRNA的产生是相互独立的，记录对于细胞类型为$j$的细胞，基因$i$产生一个mRNA的概率为$p_{ij}$,若有$m$个informative gene 则对细胞类型$j$,我们得到了一个随机向量$\\boldsymbol{p}_j= (p_{1j},\\cdots,p_{mj} )$其中$\\sum_{i}p_{ij}=1$且其服从多项式分布,则 对于属于细胞类型$j$的细胞$y$，其后验表达谱$y=(y_1,\\dots,y_n)$可以计算为：\n\n$$ P(y|j)=\\frac{(\\sum_{i}y_i)!}{\\prod_{i}(y_i!)}\\prod_i(p_{ij}^{y_i}) $$\n\n其中概率$p_{ij}$可估计为$\\hat{p_{ij}}=\\frac{1+X_{ij}}{\\sum_{i}(1+X_{ij})}$\n\n同样的，在测试集中，未知细胞类型的细胞$y$属于细胞类型$j$的概率也为\n\n$$ P(y|j)=\\frac{(\\sum_{i}y_i)!}{\\prod_{i}(y_i!)}\\prod_i(p_{ij}^{y_i}) $$\n\n其中$p_{ij}$是训练集中学习到的参数。\n\n如果，我们引入$q_i=\\frac{y_i}{\\sum_i y_i}$,由极大似然的原则可知，细胞$y$最有可能的细胞类型$\\hat{j}$为\n\n$$ \\begin{aligned}\\hat{j} &= \\argmax_{j}(P(y|j)) \\\\ &= \\argmax_{j}(\\frac{(\\sum_{i}y_i)!}{\\prod_{i}(y_i!)}\\prod_{i}(p_{ij}^{y_i})) \\\\\n&=\\argmax_{j}(\\prod_{i}(p_{ij}^{y_i}) \\\\\n&=\\argmax_{j}(y_i\\ln p_{ij}) \\\\\n&=\\argmax_{j}(\\frac{y_i}{\\sum_{i}y_i}\\ln p_{ij}-q_i\\ln q_i) \\\\\n&=\\argmax_{j}(p_i\\ln p_{ij}-q_i\\ln q_i) \\\\\n&=\\argmax_{j}D_{KL}(q||p_j) \\end{aligned}$$\n\n#### 2.6 方法总结\n\n可以用原文献中的流图对SciBet进行总结：\n\n![](https://imgkr.cn-bj.ufileos.com/6c7712b3-9058-4a8d-a4ed-f32414601e4b.png)\n\n\n\n### 3. 软件使用\n\n#### 3.1 在线版\n\n在线版使用请按照[官网](http://scibet.cancer-pku.cn/)的[Online classification](http://scibet.cancer-pku.cn/download_references.html)教程。\n\n值得一提的是，作者提供了很从不同组织，不同实验条件的单细胞测序数据中训练好的Signature：\n\n![](https://imgkr.cn-bj.ufileos.com/bf56b36c-df96-49bb-af81-2e9118331e96.png)\n\n这些不同的signature可以供不同研究者结合自己的兴趣使用。\n\n#### 3.2 本地版\n\n##### 3.2.1 安装\n\n```R\nif (!requireNamespace(\"devtools\", quietly = TRUE)) install.packages(\"devtools\")\ndevtools::install_github(\"PaulingLiu/scibet\")\n```\n如果出现错误，请看相关[issue](https://github.com/PaulingLiu/scibet/issues/1)\n\n##### 3.2.2 作者提供的测试数据\n\n按照官网的教程[E-test and SciBet](http://scibet.cancer-pku.cn/document.html)；下载所需的数据；然后后按照其说明文档进行即可。\n\n接下来，我们看如何用使用SciBet结合Seurat进行单细胞分析。\n\n##### 3.2.3 SciBet结合Seurat\n\n为了说明问题，我们选择Seurat自带的pbmcsca数据集, 该数据集已经提供了预先定义好的细胞标签，代码如下：\n\n\n```r\nlibrary(Seurat)\nlibrary(pbmcsca.SeuratData)\nlibrary(ggplot2)\nlibrary(scibet)\nlibrary(tidyverse)\nlibrary(viridis)\n\n####0.---define utilized function--------\n####---Export expr data from 10x to tibble----\n#' @param seuv3 a seuv3 object \n#' \nmyGetExpr <- function(seuv3,...){\n\texpr <- GetAssayData(object = seuv3, slot = \"data\")\n\texpr <- as_tibble(t(as.matrix(expr)),rownames = NA)\n\treturn(expr)\n}\n\n###1.load data and preprocessing\ndata(\"pbmcsca\")\n###---avoid warning-----\npbmcsca <- UpdateSeuratObject(pbmcsca)\n###---show predifined cell type------\ntable(pbmcsca$CellType)\n```\n\n```\n# B cell              CD14+ monocyte \n# 5020                        5550 \n# CD16+ monocyte                 CD4+ T cell \n# 804                        7391 \n# Cytotoxic T cell              Dendritic cell \n# 9071                         433 \n# Megakaryocyte         Natural killer cell \n# 977                        1565 \n# Plasmacytoid dendritic cell                  Unassigned \n# 164                          46 \n```\n\n\n```r\n###---split data-----\npbmc.list <- SplitObject(pbmcsca, split.by = \"Method\")\n###---normalize-------\nfor (i in names(pbmc.list)) {\n\tpbmc.list[[i]] <- NormalizeData(pbmc.list[[i]], verbose = FALSE)\n}\nnames(pbmc.list)\n```\n\n```\n[1] \"Smart-seq2\"          \"CEL-Seq2\"            \"10x Chromium (v2) A\"\n[4] \"10x Chromium (v2) B\" \"10x Chromium (v3)\"   \"Drop-seq\"           \n[7] \"Seq-Well\"            \"inDrops\"             \"10x Chromium (v2)\"  \n```\n测试 reference base模式的效果\n\n```r\n###----2. test scibet----\n###----define reference and query-----\nreference <- pbmc.list[[1]]\nquery <- pbmc.list[[2]]\n\n\n###----test query reference mode----\nreference.expr <- myGetExpr(reference)\nquery.expr <- myGetExpr(query)\n\n\nreference.label <- as.character(reference$CellType)\ntest.label <- as.character(query$CellType)\n\nreference.expr <- cbind(reference.expr,label=reference.label)\n\nprd.label <- SciBet(train = reference.expr, test = query.expr)\nConfusion_heatmap(test.label, prd.label)\nggsave(filename = \"res/fig/learn_scibet_confusionheatmap_refmode.pdf\",\n\t   width = 6,height = 6)\n```\n\n![](https://imgkr.cn-bj.ufileos.com/a81d21b1-edf6-46a9-b46b-26afc649fc11.png)\n\n准确率为\n\n```r\nnum1 <- length(test.label)\nnum2 <- tibble(\n\tori = test.label,\n\tprd = prd.label\n) %>%\n\tdplyr::filter(ori == prd) %>%\n\tnrow(.)\n\nnum2/num1\n```\n\n```\n0.851711\n```\n\n测试用作者提供的训练好的模型\n\n```r\n###----test load_model mode-----\n\n###using 30 major cell types signature----\n\nmodel <- readr::read_csv(file = \"http://scibet.cancer-pku.cn/major_human_cell_types.csv\")\nmodel <- pro.core(model)\n\nprd <- LoadModel(model)\nprd.label <- prd(query.expr)\n\nConfusion_heatmap(test.label,prd.label)\nggsave(filename = \"res/fig/learn_scibet_confusionheatmap_signaturemode.pdf\",\n\t   width = 6,height = 6)\n\n```\n\n![](https://imgkr.cn-bj.ufileos.com/ff705079-0b71-4114-8e2e-c8508fc02adb.png)","tags":["note","scRNA-seq"],"categories":["genomics"]},{"title":"LearnSeurat_PBMC3k","url":"/2020/05/10/LearnSeurat-PBMC3k/","content":"\n\n本系列假定读者对于单细胞测序的数据分析和Seurat的官方教程有所了解。\n\n本篇研究最基础的PBMC3k。其实这里只有2700个外周血的细胞。注意到，由于取样是外周血，没有干细胞的存在，所以可以认为样品处于稳态。这个教程就是讲稳态下的单细胞测序分析是如何进行的。Seurat的官方教程的缺点之一就是没有涉及动态过程的单细胞分析如何进行。\n\n如无特殊说明，本系列的代码均可以在自己的笔记本电脑上运行；\n\n### 1. 构建Seurat object\n\n使用作者已经构建好的数据进行构建。关于Seurat更详细的文档可见[satijalab的wiki](https://github.com/satijalab/seurat/wiki)\n\n``` r\nlibrary(Seurat)\nlibrary(SeuratData)\nlibrary(ggplot2)\nlibrary(igraph)\nlibrary(tidyverse)\nlibrary(patchwork)\n### AvailableData() check avaliable data: we choose cbmc\n### InstallData('pbmc3k')\nlibrary(pbmc3k.SeuratData)\n\n### how this dataset generate?\n# ## Not run: \n# if (requireNamespace(Seurat, quietly = TRUE)) {\n#   url <- 'http://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz'\n#   curl::curl_download(url = url, destfile = basename(path = url))\n#   untar(tarfile = basename(path = url))\n#   pbmc.data <- Seurat::Read10X(data.dir = 'filtered_gene_bc_matrices/hg19/')\n#   pbmc3k <- Seurat::CreateSeuratObject(counts = pbmc.data, project = 'pbmc3k', min.cells = 3, min.features = 200)\n#   # Annotations come from Seurat's PBMC3k Guided Clustering Tutorial\n#   # https://satijalab.org/seurat/v3.0/pbmc3k_tutorial.html\n#   annotations <- readRDS(file = system.file('extdata/annotations/annotations.Rds', package = 'pbmc3k.SeuratData'))\n#   pbmc3k <- Seurat::AddMetaData(object = pbmc3k, metadata = annotations)\n#   # Clean up downloaded files\n#   file.remove(basename(path = url))\n#   unlink(x = 'filtered_gene_bc_matrices/', recursive = TRUE)\n# }\n# \n# ## End(Not run)\n\n### how Create Seurat object work?\n### by run `Seurat::CreateSeuratObject` you can get the source function\n# ## Not run:\n# Seurat::CreateSeuratObject\n# function (counts, project = \"SeuratProject\", assay = \"RNA\", \n#     min.cells = 0, min.features = 0, names.field = 1, names.delim = \"_\", \n#     meta.data = NULL) \n# {\n#     if (!is.null(x = meta.data)) {\n#         if (is.null(x = rownames(x = meta.data))) {\n#             stop(\"Row names not set in metadata. Please ensure that rownames of metadata match column names of data matrix\")\n#         }\n#         if (length(x = setdiff(x = rownames(x = meta.data), y = colnames(x = counts)))) {\n#             warning(\"Some cells in meta.data not present in provided counts matrix.\")\n#             meta.data <- meta.data[intersect(x = rownames(x = meta.data), \n#                 y = colnames(x = counts)), ]\n#         }\n#         if (is.data.frame(x = meta.data)) {\n#             new.meta.data <- data.frame(row.names = colnames(x = counts))\n#             for (ii in 1:ncol(x = meta.data)) {\n#                 new.meta.data[rownames(x = meta.data), colnames(x = meta.data)[ii]] <- meta.data[, \n#                   ii, drop = FALSE]\n#             }\n#             meta.data <- new.meta.data\n#         }\n#     }\n#     assay.data <- CreateAssayObject(counts = counts, min.cells = min.cells, \n#         min.features = min.features)\n#     Key(object = assay.data) <- paste0(tolower(x = assay), \"_\")\n#     assay.list <- list(assay.data)\n#     names(x = assay.list) <- assay\n#     init.meta.data <- data.frame(row.names = colnames(x = assay.list[[assay]]))\n#     idents <- factor(x = unlist(x = lapply(X = colnames(x = assay.data), \n#         FUN = ExtractField, field = names.field, delim = names.delim)))\n#     if (any(is.na(x = idents))) {\n#         warning(\"Input parameters result in NA values for initial cell identities. Setting all initial idents to the project name\")\n#     }\n#     ident.levels <- length(x = unique(x = idents))\n#     if (ident.levels > 100 || ident.levels == 0 || ident.levels == \n#         length(x = idents)) {\n#         idents <- rep.int(x = factor(x = project), times = ncol(x = assay.data))\n#     }\n#     names(x = idents) <- colnames(x = assay.data)\n#     object <- new(Class = \"Seurat\", assays = assay.list, \n#         meta.data = init.meta.data, active.assay = assay, active.ident = idents, \n#         project.name = project, version = packageVersion(pkg = \"Seurat\"))\n#     object[[\"orig.ident\"]] <- idents\n#     n.calc <- CalcN(object = assay.data)\n#     if (!is.null(x = n.calc)) {\n#         names(x = n.calc) <- paste(names(x = n.calc), assay, \n#             sep = \"_\")\n#         object[[names(x = n.calc)]] <- n.calc\n#     }\n#     if (!is.null(x = meta.data)) {\n#         object <- AddMetaData(object = object, metadata = meta.data)\n#     }\n#     return(object)\n# }\n# \n# Seurat:: CreateAssayObject\n# function (counts, data, min.cells = 0, min.features = 0) \n# {\n#     if (missing(x = counts) && missing(x = data)) {\n#         stop(\"Must provide either 'counts' or 'data'\")\n#     }\n#     else if (!missing(x = counts) && !missing(x = data)) {\n#         stop(\"Either 'counts' or 'data' must be missing; both cannot be provided\")\n#     }\n#     else if (!missing(x = counts)) {\n#         if (anyDuplicated(rownames(x = counts))) {\n#             warning(\"Non-unique features (rownames) present in the input matrix, making unique\", \n#                 call. = FALSE, immediate. = TRUE)\n#             rownames(x = counts) <- make.unique(names = rownames(x = counts))\n#         }\n#         if (anyDuplicated(colnames(x = counts))) {\n#             warning(\"Non-unique cell names (colnames) present in the input matrix, making unique\", \n#                 call. = FALSE, immediate. = TRUE)\n#             colnames(x = counts) <- make.unique(names = colnames(x = counts))\n#         }\n#         if (is.null(x = colnames(x = counts))) {\n#             stop(\"No cell names (colnames) names present in the input matrix\")\n#         }\n#         if (any(rownames(x = counts) == \"\")) {\n#             stop(\"Feature names of counts matrix cannot be empty\", \n#                 call. = FALSE)\n#         }\n#         if (nrow(x = counts) > 0 && is.null(x = rownames(x = counts))) {\n#             stop(\"No feature names (rownames) names present in the input matrix\")\n#         }\n#         if (!inherits(x = counts, what = \"dgCMatrix\")) {\n#             counts <- as(object = as.matrix(x = counts), Class = \"dgCMatrix\")\n#         }\n#         if (min.features > 0) {\n#             nfeatures <- Matrix::colSums(x = counts > 0)\n#             counts <- counts[, which(x = nfeatures >= min.features)]\n#         }\n#         if (min.cells > 0) {\n#             num.cells <- Matrix::rowSums(x = counts > 0)\n#             counts <- counts[which(x = num.cells >= min.cells), \n#                 ]\n#         }\n#         data <- counts\n#     }\n#     else if (!missing(x = data)) {\n#         if (anyDuplicated(rownames(x = data))) {\n#             warning(\"Non-unique features (rownames) present in the input matrix, making unique\", \n#                 call. = FALSE, immediate. = TRUE)\n#             rownames(x = data) <- make.unique(names = rownames(x = data))\n#         }\n#         if (anyDuplicated(colnames(x = data))) {\n#             warning(\"Non-unique cell names (colnames) present in the input matrix, making unique\", \n#                 call. = FALSE, immediate. = TRUE)\n#             colnames(x = data) <- make.unique(names = colnames(x = data))\n#         }\n#         if (is.null(x = colnames(x = data))) {\n#             stop(\"No cell names (colnames) names present in the input matrix\")\n#         }\n#         if (any(rownames(x = data) == \"\")) {\n#             stop(\"Feature names of data matrix cannot be empty\", \n#                 call. = FALSE)\n#         }\n#         if (nrow(x = data) > 0 && is.null(x = rownames(x = data))) {\n#             stop(\"No feature names (rownames) names present in the input matrix\")\n#         }\n#         if (min.cells != 0 | min.features != 0) {\n#             warning(\"No filtering performed if passing to data rather than counts\", \n#                 call. = FALSE, immediate. = TRUE)\n#         }\n#         counts <- new(Class = \"matrix\")\n#     }\n#     if (!is.vector(x = rownames(x = counts))) {\n#         rownames(x = counts) <- as.vector(x = rownames(x = counts))\n#     }\n#     if (!is.vector(x = colnames(x = counts))) {\n#         colnames(x = counts) <- as.vector(x = colnames(x = counts))\n#     }\n#     if (!is.vector(x = rownames(x = data))) {\n#         rownames(x = data) <- as.vector(x = rownames(x = data))\n#     }\n#     if (!is.vector(x = colnames(x = data))) {\n#         colnames(x = data) <- as.vector(x = colnames(x = data))\n#     }\n#     if (any(grepl(pattern = \"_\", x = rownames(x = counts))) || \n#         any(grepl(pattern = \"_\", x = rownames(x = data)))) {\n#         warning(\"Feature names cannot have underscores ('_'), replacing with dashes ('-')\", \n#             call. = FALSE, immediate. = TRUE)\n#         rownames(x = counts) <- gsub(pattern = \"_\", replacement = \"-\", \n#             x = rownames(x = counts))\n#         rownames(x = data) <- gsub(pattern = \"_\", replacement = \"-\", \n#             x = rownames(x = data))\n#     }\n#     if (any(grepl(pattern = \"|\", x = rownames(x = counts), \n#         fixed = TRUE)) || any(grepl(pattern = \"|\", x = rownames(x = data), \n#         fixed = TRUE))) {\n#         warning(\"Feature names cannot have pipe characters ('|'), replacing with dashes ('-')\", \n#             call. = FALSE, immediate. = TRUE)\n#         rownames(x = counts) <- gsub(pattern = \"|\", replacement = \"-\", \n#             x = rownames(x = counts), fixed = TRUE)\n#         rownames(x = data) <- gsub(pattern = \"|\", replacement = \"-\", \n#             x = rownames(x = data), fixed = TRUE)\n#     }\n#     init.meta.features <- data.frame(row.names = rownames(x = data))\n#     assay <- new(Class = \"Assay\", counts = counts, data = data, \n#         scale.data = new(Class = \"matrix\"), meta.features = init.meta.features)\n#     return(assay)\n# }\n\n###update object to avoid warning.\ndata(\"pbmc3k\")\npbmc <- UpdateSeuratObject(pbmc3k)\nrm(pbmc3k)\npbmc\n```\n\n    ## An object of class Seurat \n    ## 13714 features across 2700 samples within 1 assay \n    ## Active assay: RNA (13714 features)\n\n### 2. 基本预处理\n\n作者在原教程说： \n\n> The steps below encompass the standard pre-processing\nworkflow for scRNA-seq data in Seurat. These represent the selection and\nfiltration of cells based on QC metrics, data normalization and scaling,\nand the detection of highly variable features.\n\n#### 2.1 细胞质控\n\n三种基本的QC metrics\n\n> 1.  The number of unique genes detected in each cell.\n>       - Low-quality cells or empty droplets will often have very few\n>         genes\n>       - Cell doublets or multiplets may exhibit an aberrantly high\n>         gene count\n> 2.  Similarly, the total number of molecules detected within a cell\n>     (correlates strongly with unique genes)\n> 3.  The percentage of reads that map to the mitochondrial genome\n>       - Low-quality / dying cells often exhibit extensive\n>         mitochondrial contamination\n>       - We calculate mitochondrial QC metrics with the\n>         `PercentageFeatureSet` function, which calculates the\n>         percentage of counts originating from a set of features\n>       - We use the set of all genes starting with `MT-` as a set of\n>         mitochondrial genes\n\n注意，人的线粒体基因是“MT-”开头，而小鼠的线粒体基因是“mt-”开头\n\n``` r\n# The [[ operator can add columns to object metadata. This is a great place to stash QC stats\n\n### how does PercentageFeatureSet work\n# PercentageFeatureSet\n# function (object, pattern = NULL, features = NULL, col.name = NULL, \n#     assay = NULL) \n# {\n#     assay <- assay %||% DefaultAssay(object = object)\n#     if (!is.null(x = features) && !is.null(x = pattern)) {\n#         warning(\"Both pattern and features provided. Pattern is being ignored.\")\n#     }\n#     features <- features %||% grep(pattern = pattern, x = rownames(x = object[[assay]]), \n#         value = TRUE)\n#     percent.featureset <- colSums(x = GetAssayData(object = object, \n#         assay = assay, slot = \"counts\")[features, , drop = FALSE])/object[[paste0(\"nCount_\", \n#         assay)]] * 100\n#     if (!is.null(x = col.name)) {\n#         object <- AddMetaData(object = object, metadata = percent.featureset, \n#             col.name = col.name)\n#         return(object)\n#     }\n#     return(percent.featureset)\n# }\n\npbmc[[\"percent.mt\"]] <- PercentageFeatureSet(pbmc, pattern = \"^MT-\")\n# Show QC metrics for the first 5 cells\nhead(pbmc@meta.data, 5)\n```\n\n    ##                orig.ident nCount_RNA nFeature_RNA seurat_annotations percent.mt\n    ## AAACATACAACCAC     pbmc3k       2419          779       Memory CD4 T  3.0177759\n    ## AAACATTGAGCTAC     pbmc3k       4903         1352                  B  3.7935958\n    ## AAACATTGATCAGC     pbmc3k       3147         1129       Memory CD4 T  0.8897363\n    ## AAACCGTGCTTCCG     pbmc3k       2639          960         CD14+ Mono  1.7430845\n    ## AAACCGTGTATGCG     pbmc3k        980          521                 NK  1.2244898\n\n由于我们用的是作者给了metadata的数据，里面已经出现了细胞类型的注释，见`seurat_annotation`这一项；\n\nQC metric的可视化：\n\n``` r\n# Visualize QC metrics as a violin plot\nVlnPlot(pbmc, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol = 3)\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-3-1.png\" style=\"display: block; margin: auto;\" />\n\n``` r\n# FeatureScatter is typically used to visualize feature-feature relationships, but can be used\n# for anything calculated by the object, i.e. columns in object metadata, PC scores etc.\n\nplot1 <- FeatureScatter(pbmc, feature1 = \"nCount_RNA\", feature2 = \"percent.mt\")\nplot2 <- FeatureScatter(pbmc, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\")\nplot1 + plot2\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-4-1.png\" style=\"display: block; margin: auto;\" />\n\n最终选择的质控标准为：\n\n>   - We filter cells that have unique feature counts over 2,500 or less\n>     than 200\n>   - We filter cells that have \\>5% mitochondrial counts\n\n``` r\npbmc <- subset(pbmc, subset = nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5)\n```\n\n#### 2.2 标准化\n\n> After removing unwanted cells from the dataset, the next step is to\n> normalize the data. By default, we employ a global-scaling\n> normalization method “LogNormalize” that normalizes the feature\n> expression measurements for each cell by the total expression,\n> multiplies this by a scale factor (10,000 by default), and\n> log-transforms the result. Normalized values are stored in\n> pbmc\\[\\[“RNA”\\]\\]@data.\n\n``` r\npbmc <- NormalizeData(pbmc, normalization.method = \"LogNormalize\", scale.factor = 10000)\n```\n\n#### 2.3 特征选择\n\n哪些基因能反应不同细胞之间的异质性？是那些表达差异大的基因；\n\n注意`FindVariableFeatures`是S3 generic，泛型函数。\n如何查看一个泛型函数的源代码呢，我们先用`methods`函数匹配该范型函数的名字：\n\n``` r\nmethods(FindVariableFeatures)\n```\n\n    ## [1] FindVariableFeatures.Assay*   FindVariableFeatures.default*\n    ## [3] FindVariableFeatures.Seurat* \n    ## see '?methods' for accessing help and source code\n\n星号表明我们不能直接通过运行函数名字来查看其源代码，但是我们可以通过运行 **getAnywhere**函数来获取这个函数，\n\n``` r\ngetAnywhere(FindVariableFeatures.Seurat)\n```\n\n    ## A single object matching 'FindVariableFeatures.Seurat' was found\n    ## It was found in the following places\n    ##   registered S3 method for FindVariableFeatures from namespace Seurat\n    ##   namespace:Seurat\n    ## with value\n    ## \n    ## function (object, assay = NULL, selection.method = \"vst\", loess.span = 0.3, \n    ##     clip.max = \"auto\", mean.function = FastExpMean, dispersion.function = FastLogVMR, \n    ##     num.bin = 20, binning.method = \"equal_width\", nfeatures = 2000, \n    ##     mean.cutoff = c(0.1, 8), dispersion.cutoff = c(1, Inf), verbose = TRUE, \n    ##     ...) \n    ## {\n    ##     assay <- assay %||% DefaultAssay(object = object)\n    ##     assay.data <- GetAssay(object = object, assay = assay)\n    ##     assay.data <- FindVariableFeatures(object = assay.data, selection.method = selection.method, \n    ##         loess.span = loess.span, clip.max = clip.max, mean.function = mean.function, \n    ##         dispersion.function = dispersion.function, num.bin = num.bin, \n    ##         binning.method = binning.method, nfeatures = nfeatures, \n    ##         mean.cutoff = mean.cutoff, dispersion.cutoff = dispersion.cutoff, \n    ##         verbose = verbose, ...)\n    ##     object[[assay]] <- assay.data\n    ##     object <- LogSeuratCommand(object = object)\n    ##     return(object)\n    ## }\n    ## <bytecode: 0x0000000022fa2410>\n    ## <environment: namespace:Seurat>\n\n我们可以发现，默认的**FindVariableFeatures.Seurat**method调用了**FindVariableFeatures.Assay**：\n\n``` r\ngetAnywhere(FindVariableFeatures.Assay)\n```\n\n    ## A single object matching 'FindVariableFeatures.Assay' was found\n    ## It was found in the following places\n    ##   registered S3 method for FindVariableFeatures from namespace Seurat\n    ##   namespace:Seurat\n    ## with value\n    ## \n    ## function (object, selection.method = \"vst\", loess.span = 0.3, \n    ##     clip.max = \"auto\", mean.function = FastExpMean, dispersion.function = FastLogVMR, \n    ##     num.bin = 20, binning.method = \"equal_width\", nfeatures = 2000, \n    ##     mean.cutoff = c(0.1, 8), dispersion.cutoff = c(1, Inf), verbose = TRUE, \n    ##     ...) \n    ## {\n    ##     if (length(x = mean.cutoff) != 2 || length(x = dispersion.cutoff) != \n    ##         2) {\n    ##         stop(\"Both 'mean.cutoff' and 'dispersion.cutoff' must be two numbers\")\n    ##     }\n    ##     if (selection.method == \"vst\") {\n    ##         data <- GetAssayData(object = object, slot = \"counts\")\n    ##         if (IsMatrixEmpty(x = data)) {\n    ##             warning(\"selection.method set to 'vst' but count slot is empty; will use data slot instead\")\n    ##             data <- GetAssayData(object = object, slot = \"data\")\n    ##         }\n    ##     }\n    ##     else {\n    ##         data <- GetAssayData(object = object, slot = \"data\")\n    ##     }\n    ##     hvf.info <- FindVariableFeatures(object = data, selection.method = selection.method, \n    ##         loess.span = loess.span, clip.max = clip.max, mean.function = mean.function, \n    ##         dispersion.function = dispersion.function, num.bin = num.bin, \n    ##         binning.method = binning.method, verbose = verbose, ...)\n    ##     object[[names(x = hvf.info)]] <- hvf.info\n    ##     hvf.info <- hvf.info[which(x = hvf.info[, 1, drop = TRUE] != \n    ##         0), ]\n    ##     if (selection.method == \"vst\") {\n    ##         hvf.info <- hvf.info[order(hvf.info$vst.variance.standardized, \n    ##             decreasing = TRUE), , drop = FALSE]\n    ##     }\n    ##     else {\n    ##         hvf.info <- hvf.info[order(hvf.info$mvp.dispersion, decreasing = TRUE), \n    ##             , drop = FALSE]\n    ##     }\n    ##     selection.method <- switch(EXPR = selection.method, mvp = \"mean.var.plot\", \n    ##         disp = \"dispersion\", selection.method)\n    ##     top.features <- switch(EXPR = selection.method, mean.var.plot = {\n    ##         means.use <- (hvf.info[, 1] > mean.cutoff[1]) & (hvf.info[, \n    ##             1] < mean.cutoff[2])\n    ##         dispersions.use <- (hvf.info[, 3] > dispersion.cutoff[1]) & \n    ##             (hvf.info[, 3] < dispersion.cutoff[2])\n    ##         rownames(x = hvf.info)[which(x = means.use & dispersions.use)]\n    ##     }, dispersion = head(x = rownames(x = hvf.info), n = nfeatures), \n    ##         vst = head(x = rownames(x = hvf.info), n = nfeatures), \n    ##         stop(\"Unkown selection method: \", selection.method))\n    ##     VariableFeatures(object = object) <- top.features\n    ##     vf.name <- ifelse(test = selection.method == \"vst\", yes = \"vst\", \n    ##         no = \"mvp\")\n    ##     vf.name <- paste0(vf.name, \".variable\")\n    ##     object[[vf.name]] <- rownames(x = object[[]]) %in% top.features\n    ##     return(object)\n    ## }\n    ## <bytecode: 0x000000002dc5d050>\n    ## <environment: namespace:Seurat>\n\n千层饼的最后一层；\n\n``` r\ngetAnywhere(FindVariableFeatures.default)\n```\n\n    ## A single object matching 'FindVariableFeatures.default' was found\n    ## It was found in the following places\n    ##   registered S3 method for FindVariableFeatures from namespace Seurat\n    ##   namespace:Seurat\n    ## with value\n    ## \n    ## function (object, selection.method = \"vst\", loess.span = 0.3, \n    ##     clip.max = \"auto\", mean.function = FastExpMean, dispersion.function = FastLogVMR, \n    ##     num.bin = 20, binning.method = \"equal_width\", verbose = TRUE, \n    ##     ...) \n    ## {\n    ##     CheckDots(...)\n    ##     if (!inherits(x = object, \"Matrix\")) {\n    ##         object <- as(object = as.matrix(x = object), Class = \"Matrix\")\n    ##     }\n    ##     if (!inherits(x = object, what = \"dgCMatrix\")) {\n    ##         object <- as(object = object, Class = \"dgCMatrix\")\n    ##     }\n    ##     if (selection.method == \"vst\") {\n    ##         if (clip.max == \"auto\") {\n    ##             clip.max <- sqrt(x = ncol(x = object))\n    ##         }\n    ##         hvf.info <- data.frame(mean = rowMeans(x = object))\n    ##         hvf.info$variance <- SparseRowVar2(mat = object, mu = hvf.info$mean, \n    ##             display_progress = verbose)\n    ##         hvf.info$variance.expected <- 0\n    ##         hvf.info$variance.standardized <- 0\n    ##         not.const <- hvf.info$variance > 0\n    ##         fit <- loess(formula = log10(x = variance) ~ log10(x = mean), \n    ##             data = hvf.info[not.const, ], span = loess.span)\n    ##         hvf.info$variance.expected[not.const] <- 10^fit$fitted\n    ##         hvf.info$variance.standardized <- SparseRowVarStd(mat = object, \n    ##             mu = hvf.info$mean, sd = sqrt(hvf.info$variance.expected), \n    ##             vmax = clip.max, display_progress = verbose)\n    ##         colnames(x = hvf.info) <- paste0(\"vst.\", colnames(x = hvf.info))\n    ##     }\n    ##     else {\n    ##         if (!inherits(x = mean.function, what = \"function\")) {\n    ##             stop(\"'mean.function' must be a function\")\n    ##         }\n    ##         if (!inherits(x = dispersion.function, what = \"function\")) {\n    ##             stop(\"'dispersion.function' must be a function\")\n    ##         }\n    ##         feature.mean <- mean.function(object, verbose)\n    ##         feature.dispersion <- dispersion.function(object, verbose)\n    ##         names(x = feature.mean) <- names(x = feature.dispersion) <- rownames(x = object)\n    ##         feature.dispersion[is.na(x = feature.dispersion)] <- 0\n    ##         feature.mean[is.na(x = feature.mean)] <- 0\n    ##         data.x.breaks <- switch(EXPR = binning.method, equal_width = num.bin, \n    ##             equal_frequency = c(-1, quantile(x = feature.mean[feature.mean > \n    ##                 0], probs = seq.int(from = 0, to = 1, length.out = num.bin))), \n    ##             stop(\"Unknown binning method: \", binning.method))\n    ##         data.x.bin <- cut(x = feature.mean, breaks = data.x.breaks)\n    ##         names(x = data.x.bin) <- names(x = feature.mean)\n    ##         mean.y <- tapply(X = feature.dispersion, INDEX = data.x.bin, \n    ##             FUN = mean)\n    ##         sd.y <- tapply(X = feature.dispersion, INDEX = data.x.bin, \n    ##             FUN = sd)\n    ##         feature.dispersion.scaled <- (feature.dispersion - mean.y[as.numeric(x = data.x.bin)])/sd.y[as.numeric(x = data.x.bin)]\n    ##         names(x = feature.dispersion.scaled) <- names(x = feature.mean)\n    ##         hvf.info <- data.frame(feature.mean, feature.dispersion, \n    ##             feature.dispersion.scaled)\n    ##         rownames(x = hvf.info) <- rownames(x = object)\n    ##         colnames(x = hvf.info) <- paste0(\"mvp.\", c(\"mean\", \"dispersion\", \n    ##             \"dispersion.scaled\"))\n    ##     }\n    ##     return(hvf.info)\n    ## }\n    ## <bytecode: 0x0000000023f13fd0>\n    ## <environment: namespace:Seurat>\n\n忽略这些技术细节，进行特征选择；\n\n``` r\npbmc <- FindVariableFeatures(pbmc, selection.method = \"vst\", nfeatures = 2000)\n\n# Identify the 10 most highly variable genes\ntop10 <- head(VariableFeatures(pbmc), 10)\n\n# plot variable features with and without labels\nplot1 <- VariableFeaturePlot(pbmc)\nplot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)\nplot1 + plot2\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-11-1.png\" style=\"display: block; margin: auto;\" />\n\n#### 2.4 Scaling the data\n\n这步的目的是为了后续的PCA：\n\n> Next, we apply a linear transformation (‘scaling’) that is a standard\n> pre-processing step prior to dimensional reduction techniques like\n> PCA. The **ScaleData** function: + Shifts the expression of each gene,\n> so that the mean expression across cells is 0 + Scales the expression\n> of each gene, so that the variance across cells is 1 + This step gives\n> equal weight in downstream analyses, so that highly-expressed genes do\n> not dominate + The results of this are stored in\n> **pbmc\\[\\[“RNA”\\]\\]@scale.data**\n\n回归掉percent.mt对于PCA的影响。这步是一步限速步骤；\n\n``` r\nall.genes <- rownames(pbmc)\npbmc <- ScaleData(pbmc, features = all.genes,vars.to.regress = \"percent.mt\")\n```\n\n有一个问题后面的marker基因一定是HVG吗？\n\n#### 2.5 线性降维(PCA)\n\n> Next we perform PCA on the scaled data. By default, only the\n> previously determined variable features are used as input, but can be\n> defined using features argument if you wish to choose a different\n> subset.\n\n``` r\npbmc <- RunPCA(pbmc, features = VariableFeatures(object = pbmc))\n# Examine and visualize PCA results a few different ways\nprint(pbmc[[\"pca\"]], dims = 1:5, nfeatures = 5)\n```\n\n    ## PC_ 1 \n    ## Positive:  CST3, TYROBP, LST1, AIF1, FTL \n    ## Negative:  MALAT1, LTB, IL32, IL7R, CD2 \n    ## PC_ 2 \n    ## Positive:  CD79A, MS4A1, TCL1A, HLA-DQA1, HLA-DQB1 \n    ## Negative:  NKG7, PRF1, CST7, GZMA, GZMB \n    ## PC_ 3 \n    ## Positive:  HLA-DQA1, CD79A, CD79B, HLA-DQB1, HLA-DPA1 \n    ## Negative:  PPBP, PF4, SDPR, SPARC, GNG11 \n    ## PC_ 4 \n    ## Positive:  HLA-DQA1, CD79B, CD79A, MS4A1, HLA-DQB1 \n    ## Negative:  VIM, IL7R, S100A6, S100A8, IL32 \n    ## PC_ 5 \n    ## Positive:  GZMB, FGFBP2, S100A8, NKG7, GNLY \n    ## Negative:  LTB, IL7R, CKB, MS4A7, RP11-290F20.3\n\n``` r\nVizDimLoadings(pbmc, dims = 1:2, reduction = \"pca\")\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-14-1.png\" style=\"display: block; margin: auto;\" />\n\n``` r\nDimPlot(pbmc, reduction = \"pca\")\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-15-1.png\" style=\"display: block; margin: auto;\" />\n\n> In particular **DimHeatmap** allows for easy exploration of the\n> primary sources of heterogeneity in a dataset, and can be useful when\n> trying to decide which PCs to include for further downstream analyses.\n> Both cells and features are ordered according to their PCA scores.\n> Setting cells to a number plots the ‘extreme’ cells on both ends of\n> the spectrum, which dramatically speeds plotting for large datasets.\n> Though clearly a supervised analysis, we find this to be a valuable\n> tool for exploring correlated feature sets.\n\n``` r\n###     Plot an equal number of genes with both + and - scores.\nmypal <- rev(colorRampPalette(RColorBrewer::brewer.pal(11,\"RdBu\"))(256))\nDimHeatmap(pbmc, dims = 1, cells = 500, balanced = TRUE,fast = F)+scale_fill_gradientn(colors  = mypal)\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-16-1.png\" style=\"display: block; margin: auto;\" />\n\n``` r\nDimHeatmap(pbmc, dims = 1:15, cells = 500, balanced = TRUE)\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-17-1.png\" style=\"display: block; margin: auto;\" />\n\n#### 2.6 Determine the ‘dimensionality’ of the dataset\n\n> To overcome the extensive technical noise in any single feature for\n> scRNA-seq data, Seurat clusters cells based on their PCA scores, with\n> each PC essentially representing a ‘metafeature’ that combines\n> information across a correlated feature set. The top principal\n> components therefore represent a robust compression of the dataset.\n> However, how many componenets should we choose to include? 10? 20?\n> 100?\n\n两种统计方法，`JackStraw`和`ElbowPlot`，前者比较耗时，不再展示了，用后者\n\n``` r\nElbowPlot(pbmc)\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-18-1.png\" style=\"display: block; margin: auto;\" />\n\n作者给出了更进一步的解释\n\n> Identifying the true dimensionality of a dataset – can be\n> challenging/uncertain for the user. We therefore suggest these three\n> approaches to consider. The first is more supervised, exploring PCs to\n> determine relevant sources of heterogeneity, and could be used in\n> conjunction with GSEA for example. The second implements a statistical\n> test based on a random null model, but is time-consuming for large\n> datasets, and may not return a clear PC cutoff. The third is a\n> heuristic that is commonly used, and can be calculated instantly. In\n> this example, all three approaches yielded similar results, but we\n> might have been justified in choosing anything between PC 7-12 as a\n> cutoff.\n\n> We chose 10 here, but encourage users to consider the following:\n\n>   - Dendritic cell and NK aficionados may recognize that genes\n>     strongly associated with PCs 12 and 13 define rare immune subsets\n>     (i.e. MZB1 is a marker for plasmacytoid DCs). However, these\n>     groups are so rare, they are difficult to distinguish from\n>     background noise for a dataset of this size without prior\n>     knowledge.\n>   - We encourage users to repeat downstream analyses with a different\n>     number of PCs (10, 15, or even 50\\!). As you will observe, the\n>     results often do not differ dramatically.\n>   - We advise users to err on the higher side when choosing this\n>     parameter. For example, performing downstream analyses with only 5\n>     PCs does signifcanltly and adversely affect results.\n\n### 3. 后续分析\n\n#### 3.1 聚类\n\n**FindNeighbors**构建构建SNN-graph, 而**FindClusters**用来实现Louvain\nalgorithm，进行图聚类；\n\n``` r\nmethods(FindNeighbors)\n```\n\n    ## [1] FindNeighbors.Assay*   FindNeighbors.default* FindNeighbors.dist*   \n    ## [4] FindNeighbors.Seurat* \n    ## see '?methods' for accessing help and source code\n\n``` r\ngetAnywhere(FindNeighbors.Seurat)\n```\n\n    ## A single object matching 'FindNeighbors.Seurat' was found\n    ## It was found in the following places\n    ##   registered S3 method for FindNeighbors from namespace Seurat\n    ##   namespace:Seurat\n    ## with value\n    ## \n    ## function (object, reduction = \"pca\", dims = 1:10, assay = NULL, \n    ##     features = NULL, k.param = 20, compute.SNN = TRUE, prune.SNN = 1/15, \n    ##     nn.method = \"rann\", annoy.metric = \"euclidean\", nn.eps = 0, \n    ##     verbose = TRUE, force.recalc = FALSE, do.plot = FALSE, graph.name = NULL, \n    ##     ...) \n    ## {\n    ##     CheckDots(...)\n    ##     if (!is.null(x = dims)) {\n    ##         assay <- DefaultAssay(object = object[[reduction]])\n    ##         data.use <- Embeddings(object = object[[reduction]])\n    ##         if (max(dims) > ncol(x = data.use)) {\n    ##             stop(\"More dimensions specified in dims than have been computed\")\n    ##         }\n    ##         data.use <- data.use[, dims]\n    ##         neighbor.graphs <- FindNeighbors(object = data.use, k.param = k.param, \n    ##             compute.SNN = compute.SNN, prune.SNN = prune.SNN, \n    ##             nn.method = nn.method, annoy.metric = annoy.metric, \n    ##             nn.eps = nn.eps, verbose = verbose, force.recalc = force.recalc, \n    ##             ...)\n    ##     }\n    ##     else {\n    ##         assay <- assay %||% DefaultAssay(object = object)\n    ##         data.use <- GetAssay(object = object, assay = assay)\n    ##         neighbor.graphs <- FindNeighbors(object = data.use, features = features, \n    ##             k.param = k.param, compute.SNN = compute.SNN, prune.SNN = prune.SNN, \n    ##             nn.method = nn.method, annoy.metric = annoy.metric, \n    ##             nn.eps = nn.eps, verbose = verbose, force.recalc = force.recalc, \n    ##             ...)\n    ##     }\n    ##     graph.name <- graph.name %||% paste0(assay, \"_\", names(x = neighbor.graphs))\n    ##     for (ii in 1:length(x = graph.name)) {\n    ##         DefaultAssay(object = neighbor.graphs[[ii]]) <- assay\n    ##         object[[graph.name[[ii]]]] <- neighbor.graphs[[ii]]\n    ##     }\n    ##     if (do.plot) {\n    ##         if (!\"tsne\" %in% names(x = object@reductions)) {\n    ##             warning(\"Please compute a tSNE for SNN visualization. See RunTSNE().\")\n    ##         }\n    ##         else {\n    ##             if (nrow(x = Embeddings(object = object[[\"tsne\"]])) != \n    ##                 ncol(x = object)) {\n    ##                 warning(\"Please compute a tSNE for SNN visualization. See RunTSNE().\")\n    ##             }\n    ##             else {\n    ##                 net <- graph.adjacency(adjmatrix = as.matrix(x = neighbor.graphs[[2]]), \n    ##                   mode = \"undirected\", weighted = TRUE, diag = FALSE)\n    ##                 plot.igraph(x = net, layout = as.matrix(x = Embeddings(object = object[[\"tsne\"]])), \n    ##                   edge.width = E(graph = net)$weight, vertex.label = NA, \n    ##                   vertex.size = 0)\n    ##             }\n    ##         }\n    ##     }\n    ##     object <- LogSeuratCommand(object = object)\n    ##     return(object)\n    ## }\n    ## <bytecode: 0x000000001e49f8e0>\n    ## <environment: namespace:Seurat>\n\n``` r\npbmc <- FindNeighbors(pbmc, dims = 1:10)\npbmc <- FindClusters(pbmc, resolution = 0.5)\n```\n\n    ## Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck\n    ## \n    ## Number of nodes: 2638\n    ## Number of edges: 95930\n    ## \n    ## Running Louvain algorithm...\n    ## Maximum modularity in 10 random starts: 0.8737\n    ## Number of communities: 9\n    ## Elapsed time: 0 seconds\n\n#### 3.2 Run UMAP/tsne\n\nrun tsne\n\n``` r\npbmc <- RunTSNE(pbmc,dims = 1:10)\nDimPlot(pbmc,label = T, reduction = \"tsne\")\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-22-1.png\" style=\"display: block; margin: auto;\" />\n\ndraw snn graph on tsne-embeding\n\n``` r\ntest <- pbmc[[\"RNA_snn\"]]\n\n\nnet <- graph.adjacency(adjmatrix = as.matrix(x = test), \n                  mode = \"undirected\", weighted = TRUE, \n                  diag = FALSE)\nplot.igraph(x = net, \n            layout = as.matrix(x = Embeddings(object = pbmc[[\"tsne\"]])),\n            edge.width = E(graph = net)$weight, vertex.label = NA, \n                  vertex.size = 0)\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-23-1.png\" style=\"display: block; margin: auto;\" />\n\nrun umap\n\n``` r\n# If you haven't installed UMAP, you can do so via reticulate::py_install(packages =\n# 'umap-learn')\npbmc <- RunUMAP(pbmc,umap.method = \"umap-learn\", dims = 1:10)\n# note that you can set `label = TRUE` or use the LabelClusters function to help label\n# individual clusters\nDimPlot(pbmc,label = T, reduction = \"umap\")\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-24-1.png\" style=\"display: block; margin: auto;\" />\n\n``` r\ntest <- pbmc[[\"RNA_snn\"]]\n\nnet <- graph.adjacency(adjmatrix = as.matrix(x = test), \n                  mode = \"undirected\", weighted = TRUE, \n                  diag = FALSE)\nplot.igraph(x = net, \n            layout = as.matrix(x = Embeddings(object = pbmc[[\"umap\"]])),\n            edge.width = E(graph = net)$weight, vertex.label = NA, \n                  vertex.size = 0)\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-25-1.png\" style=\"display: block; margin: auto;\" />\n\n#### 3.3 Finding differentially expressed features (cluster biomarkers)\n\n之前分群结果做差异表达；\n\n> Seurat can help you find markers that define clusters via differential\n> expression. By default, it identifes positive and negative markers of\n> a single cluster (specified in **ident.1**), compared to all other\n> cells. **FindAllMarkers** automates this process for all clusters, but\n> you can also test groups of clusters vs. each other, or against all\n> cells.\n\n> The **min.pct** argument requires a feature to be detected at a\n> minimum percentage in either of the two groups of cells, and the\n> thresh.test argument requires a feature to be differentially expressed\n> (on average) by some amount between the two groups. You can set both\n> of these to 0, but with a dramatic increase in time - since this will\n> test a large number of features that are unlikely to be highly\n> discriminatory. As another option to speed up these computations,\n> **max.cells.per.ident** can be set. This will downsample each identity\n> class to have no more cells than whatever this is set to. While there\n> is generally going to be a loss in power, the speed increases can be\n> significiant and the most highly differentially expressed features\n> will likely still rise to the top.\n\n``` r\n# find markers for every cluster compared to all remaining cells, report only the positive ones\npbmc.markers <- FindAllMarkers(pbmc, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)\npbmc.markers %>% group_by(cluster) %>% top_n(n = 2, wt = avg_logFC)\n```\n\n    ## # A tibble: 18 x 7\n    ## # Groups:   cluster [9]\n    ##        p_val avg_logFC pct.1 pct.2 p_val_adj cluster gene    \n    ##        <dbl>     <dbl> <dbl> <dbl>     <dbl> <fct>   <chr>   \n    ##  1 1.88e-117     0.748 0.913 0.588 2.57e-113 0       LDHB    \n    ##  2 5.01e- 85     0.931 0.437 0.108 6.88e- 81 0       CCR7    \n    ##  3 0.            3.86  0.996 0.215 0.        1       S100A9  \n    ##  4 0.            3.80  0.975 0.121 0.        1       S100A8  \n    ##  5 2.61e- 81     0.886 0.981 0.65  3.58e- 77 2       LTB     \n    ##  6 1.22e- 59     0.886 0.669 0.25  1.68e- 55 2       CD2     \n    ##  7 0.            2.99  0.939 0.042 0.        3       CD79A   \n    ##  8 1.06e-269     2.49  0.623 0.022 1.45e-265 3       TCL1A   \n    ##  9 5.98e-221     2.23  0.987 0.226 8.20e-217 4       CCL5    \n    ## 10 1.42e-173     2.08  0.572 0.051 1.94e-169 4       GZMK    \n    ## 11 3.51e-184     2.30  0.975 0.134 4.82e-180 5       FCGR3A  \n    ## 12 2.03e-125     2.14  1     0.315 2.78e-121 5       LST1    \n    ## 13 3.17e-267     3.35  0.961 0.068 4.35e-263 6       GZMB    \n    ## 14 1.04e-189     3.66  0.961 0.132 1.43e-185 6       GNLY    \n    ## 15 1.48e-220     2.68  0.812 0.011 2.03e-216 7       FCER1A  \n    ## 16 1.67e- 21     1.99  1     0.513 2.28e- 17 7       HLA-DPB1\n    ## 17 7.73e-200     5.02  1     0.01  1.06e-195 8       PF4     \n    ## 18 3.68e-110     5.94  1     0.024 5.05e-106 8       PPBP\n\n可视化：\n\n``` r\nVlnPlot(pbmc, features = c(\"MS4A1\", \"CD79A\"))\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-27-1.png\" style=\"display: block; margin: auto;\" />\n\n``` r\n# you can plot raw counts as well\nVlnPlot(pbmc, features = c(\"MS4A1\", \"CD79A\"), slot = \"counts\", log = TRUE)\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-28-1.png\" style=\"display: block; margin: auto;\" />\n\n使用**FeatureScatter**获得和流式图一样的效果；\n\n``` r\nFeatureScatter(object = pbmc,\n               feature1 = \"MS4A1\",\n               feature2 = \"CD79A\")+\n  ggtitle(label = NULL)\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-29-1.png\" style=\"display: block; margin: auto;\" />\n\n用**FeaturePlot**在Embeding上展示表达量；\n\n``` r\nFeaturePlot(pbmc, features = c(\"MS4A1\", \"GNLY\", \"CD3E\", \"CD14\", \"FCER1A\", \"FCGR3A\", \"LYZ\", \"PPBP\", \n    \"CD8A\"))\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-30-1.png\" style=\"display: block; margin: auto;\" />\n\n气泡图**DotPlot**\n\n``` r\nDotPlot(object = pbmc,\n        features = c(\"MS4A1\", \"GNLY\", \"CD3E\", \"CD14\", \"FCER1A\", \"FCGR3A\", \"LYZ\", \"PPBP\",  \"CD8A\"))+\n  coord_flip()\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-31-1.png\" style=\"display: block; margin: auto;\" />\n\n**RidgePlot**\n\n``` r\nRidgePlot(object = pbmc,\n          features = c(\"MS4A1\", \"GNLY\", \"CD3E\", \"CD14\", \"FCER1A\", \"FCGR3A\", \"LYZ\", \"PPBP\",  \"CD8A\"))\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-32-1.png\" style=\"display: block; margin: auto;\" />\n\n热图**DoHeatmap**\n\n``` r\ntop10 <- pbmc.markers %>% \n  group_by(cluster) %>% top_n(n = 10, wt = avg_logFC)\n\nDoHeatmap(pbmc, features = top10$gene) + \n  scale_fill_gradientn(colors  = mypal)\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-33-1.png\" style=\"display: block; margin: auto;\" />\n\n#### 3.4 Assigning cell type identity to clusters\n\n``` r\nnew.cluster.ids <- c(\"Naive CD4 T\",\"CD14+ Mono\", \"Memory CD4 T\",  \"B\", \"CD8 T\", \"FCGR3A+ Mono\", \"NK\", \"DC\", \"Platelet\")\nnames(new.cluster.ids) <- levels(pbmc)\npbmc <- RenameIdents(pbmc, new.cluster.ids)\nDimPlot(pbmc, reduction = \"umap\", label = TRUE, pt.size = 0.5) + NoLegend()\n```\n\n<img src=\"/figure/posts/LearnSeurat_PBMC3k_files/figure-gfm/unnamed-chunk-34-1.png\" style=\"display: block; margin: auto;\" />\n\n### 4. Seurat object 详解\n\n这一部分来自wiki\n\n#### 4.1 The Seurat object\n\n一个Seurat对象有如下的`slots`:\n\n| Slot           | Function                                                                        |\n| -------------- | ------------------------------------------------------------------------------- |\n| `assays`       | A list of assays within this object                                             |\n| `meta.data`    | Cell-level meta data                                                            |\n| `active.assay` | Name of active, or default, assay                                               |\n| `active.ident` | Identity classes for the current object                                         |\n| `graphs`       | A list of nearest neighbor graphs                                               |\n| `reductions`   | A list of DimReduc objects                                                      |\n| `project.name` | User-defined project name (optional)                                            |\n| `tools`        | Empty list. Tool developers can store any internal data from their methods here |\n| `misc`         | Empty slot. User can store additional information here                          |\n| `version`      | Seurat version used when creating the object                                    |\n\n这个对象把单细胞数据的所有的基本信息都包含进去了，可以用基本的一些函数去获取这些信息。例如，我们想要知道这个数据对应多少细胞，多少基因，可以用`dim`;`ncol`;`nrow`;细胞或者feature的名字，可以用`rownames`;`colnames`;\n我们也可以通过`names`知道里面存储的如原始表达矩阵，或者降维后对象的名字。\n\n``` r\nnames(x = pbmc)\n```\n\n    ## [1] \"RNA\"     \"RNA_nn\"  \"RNA_snn\" \"pca\"     \"tsne\"    \"umap\"\n\n``` r\nrna <- pbmc[['RNA']]\n```\n\n对于Seurat对象，有一系列的函数可以对其进行操作。这些函数可以称为其所属的**methods**。多说一句，Seurat采取的是S3对象的面向对象的数据结构。\n\n可以使用如下命令访问与Seurat对象相关的操作。\n\n``` r\nutils::methods(class = 'Seurat')\n```\n\n    ##  [1] $                       $<-                     [                      \n    ##  [4] [[                      [[<-                    AddMetaData            \n    ##  [7] as.CellDataSet          as.loom                 as.SingleCellExperiment\n    ## [10] Command                 DefaultAssay            DefaultAssay<-         \n    ## [13] dim                     dimnames                droplevels             \n    ## [16] Embeddings              FindClusters            FindMarkers            \n    ## [19] FindNeighbors           FindVariableFeatures    GetAssay               \n    ## [22] GetAssayData            HVFInfo                 Idents                 \n    ## [25] Idents<-                Key                     levels                 \n    ## [28] levels<-                Loadings                merge                  \n    ## [31] Misc                    Misc<-                  names                  \n    ## [34] NormalizeData           OldWhichCells           Project                \n    ## [37] Project<-               RenameCells             RenameIdents           \n    ## [40] ReorderIdent            RunALRA                 RunCCA                 \n    ## [43] RunICA                  RunLSI                  RunPCA                 \n    ## [46] RunTSNE                 RunUMAP                 ScaleData              \n    ## [49] ScoreJackStraw          SetAssayData            SetIdent               \n    ## [52] show                    StashIdent              Stdev                  \n    ## [55] subset                  SubsetData              Tool                   \n    ## [58] Tool<-                  VariableFeatures        VariableFeatures<-     \n    ## [61] WhichCells             \n    ## see '?methods' for accessing help and source code\n\n#### 4.2 Assay\n\n> The `Assay` class stores single cell data.\n\n> For typical scRNA-seq experiments, a Seurat object will have a single\n> Assay (“RNA”). This assay will also store multiple ‘transformations’\n> of the data, including raw counts (@counts slot), normalized data\n> (@data slot), and scaled data for dimensional reduction (@scale.data\n> slot).\n\n> For more complex experiments, an object could contain multiple assays.\n> These could include multi-modal data types (CITE-seq antibody-derived\n> tags, ADTs), or imputed/batch-corrected measurements. Each of those\n> assays has the option to store the same data transformations as well.\n\n一个**Assay** 所含有的Slots\n\n| Slot            | Function                                                                     |\n| --------------- | ---------------------------------------------------------------------------- |\n| `counts`        | Stores unnormalized data such as raw counts or TPMs                          |\n| `data`          | Normalized data matrix                                                       |\n| `scale.data`    | Scaled data matrix                                                           |\n| `key`           | A character string to facilitate looking up features from a specific `Assay` |\n| `var.features`  | A vector of features identified as variable                                  |\n| `meta.features` | Feature-level meta data                                                      |\n\n**Assay**对象也可以使用以下方法\n\nSummary information about `Assay` objects can be had quickly and easily\nusing standard R functions. Object shape/dimensions can be found using\nthe `dim`, `ncol`, and `nrow` functions; cell and feature names can be\nfound using the `colnames` and `rownames` functions, respectively, or\nthe `dimnames` function.\n\n更多的方法见\n\n``` r\nutils::methods(class = 'Assay')\n```\n\n    ##  [1] [                    [[                   [[<-                \n    ##  [4] AddMetaData          DefaultAssay         DefaultAssay<-      \n    ##  [7] dim                  dimnames             FindNeighbors       \n    ## [10] FindVariableFeatures GetAssayData         HVFInfo             \n    ## [13] Key                  Key<-                merge               \n    ## [16] Misc                 Misc<-               NormalizeData       \n    ## [19] OldWhichCells        RenameCells          RunICA              \n    ## [22] RunLSI               RunPCA               ScaleData           \n    ## [25] SetAssayData         show                 subset              \n    ## [28] SubsetData           VariableFeatures     VariableFeatures<-  \n    ## [31] WhichCells          \n    ## see '?methods' for accessing help and source code\n\nData Access\n\n``` r\n# GetAssayData allows pulling from a specific slot rather than just data\nGetAssayData(object = rna, slot = 'scale.data')[1:3, 1:3]\n```\n\n    ##               AAACATACAACCAC AAACATTGAGCTAC AAACATTGATCAGC\n    ## AL627309.1       -0.06433822    -0.06968772    -0.04966479\n    ## AP006222.2       -0.02663018    -0.02065038    -0.04303249\n    ## RP11-206L10.2    -0.03015459    -0.02024084    -0.05734758\n\n``` r\nhead(x = HVFInfo(object = rna,selection.method = \"vst\"))\n```\n\n    ##                      mean    variance variance.standardized\n    ## AL627309.1    0.003411676 0.003401325             0.9330441\n    ## AP006222.2    0.001137225 0.001136363             0.9924937\n    ## RP11-206L10.2 0.001895375 0.001892500             0.9627290\n    ## RP11-206L10.9 0.001137225 0.001136363             0.9924937\n    ## LINC00115     0.006823351 0.006779363             0.9062135\n    ## NOC2L         0.107278241 0.159514698             0.7849309\n\nThe key\n\n``` r\n# Key both accesses and sets the key slot for an Assay object\n> Key(object = rna)\n\"rna_\"\n> Key(object = rna) <- 'myRNA_'\n> Key(object = rna)\n\"myRNA_\"\n# Pull a feature from the RNA assay on the Seurat level\n> head(x = FetchData(object = pbmc, vars.fetch = 'rna_MS4A1'))\n               rna_MS4A1\nAAACATACAACCAC  0.000000\nAAACATTGAGCTAC  2.583047\nAAACATTGATCAGC  0.000000\nAAACCGTGCTTCCG  0.000000\nAAACCGTGTATGCG  0.000000\nAAACGCACTGGTAC  0.000000\n```\n\nThe `DimReduc` object represents a dimensional reduction taken upon the\nSeurat object.\n\n#### 4.3 The `DimReduc` object\n\nThe `DimReduc` object represents a dimensional reduction taken upon the\nSeurat object.\n\n| Slot                         | Function                                                                        |\n| ---------------------------- | ------------------------------------------------------------------------------- |\n| `cell.embeddings`            | A matrix with cell embeddings                                                   |\n| `feature.loadings`           | A matrix with feature loadings                                                  |\n| `feature.loadings.projected` | A matrix with projected feature loadings                                        |\n| `assay.used`                 | Assay used to calculate this dimensional reduction                              |\n| `stdev`                      | Standard deviation for the dimensional reduction                                |\n| `key`                        | A character string to facilitate looking up features from a specific `DimReduc` |\n| `jackstraw`                  | Results from the `JackStraw` function                                           |\n| `misc`                       | …                                                                               |\n\n和之前的很类似\n\n``` r\npca <- pbmc[[\"pca\"]]\n# The following examples use the PCA dimensional reduction from the PBMC 3k dataset\n> pca\nA dimensional reduction object with key PC\n Number of dimensions: 20\n Projected dimensional reduction calculated: FALSE\n Jackstraw run: FALSE\n# nrow and ncol provide the number of features and cells, respectively\n# dim provides both nrow and ncol at the same time\n> dim(x = pca)\n[1] 1838 2638\n# length provides the number of dimensions calculated\n> length(x = pca)\n[1] 20\n# In addtion to rownames and colnames, one can use dimnames\n# which provides a two-length list with both rownames and colnames\n> head(x = rownames(x = rna))\n[1] \"TNFRSF4\"  \"CPSF3L\"   \"ATAD3C\"   \"C1orf86\"  \"RER1\"     \"TNFRSF25\"\n> head(x = colnames(x = rna))\n[1] \"AAACATACAACCAC\" \"AAACATTGAGCTAC\" \"AAACATTGATCAGC\" \"AAACCGTGCTTCCG\"\n[5] \"AAACCGTGTATGCG\" \"AAACGCACTGGTAC\"\n```\n\nAccess data\n\n``` r\n# The key can be used to pull cell embeddings for specific dimensions from the Seurat level\n> Key(object = pca)\n\"PC\"\n> head(x = FetchData(object = pbmc, vars.fetch = 'PC1'))\n                      PC1\nAAACATACAACCAC   5.569384\nAAACATTGAGCTAC   7.216456\nAAACATTGATCAGC   2.706629\nAAACCGTGCTTCCG -10.134042\nAAACCGTGTATGCG  -1.099311\nAAACGCACTGGTAC   1.455335\n# DefaultAssay gets the name of the Assay object used to calculate the DimReduc\n> DefaultAssay(object = pca)\n[1] \"RNA\"\n# Stdev gets the vector of standard deviations for each dimension embedded.\nStdev(object = pca)\n [1] 5.666584 4.326466 3.952192 3.638124 2.191529 1.996551 1.877891 1.798251\n [9] 1.766873 1.753684 1.731568 1.720525 1.718079 1.715879 1.707009 1.702660\n[17] 1.697318 1.692549 1.686149 1.683967\n```\n\n在其上可以执行的**method**有\n\n``` r\nutils::methods(class = \"DimReduc\")\n```\n\n    ##  [1] [              [[             Cells          DefaultAssay   DefaultAssay<-\n    ##  [6] dim            dimnames       Embeddings     IsGlobal       JS            \n    ## [11] JS<-           Key            Key<-          length         Loadings      \n    ## [16] Loadings<-     names          print          RenameCells    RunTSNE       \n    ## [21] ScoreJackStraw show           Stdev          subset        \n    ## see '?methods' for accessing help and source code\n\n#### 4.4 R面向对象编程的更多细节；\n\n关于面向对象，以及S3对象的教程，更多可见：\n\n1.  [R深入|面向对象——泛型函数](https://zhuanlan.zhihu.com/p/31160374)\n2.  [OO field guide](http://adv-r.had.co.nz/OO-essentials.html)\n3.  [R语言面向对象编程](https://dataxujing.github.io/R_oop/)\n","tags":["R","scRNA-seq"],"categories":["implementation"]},{"title":"Dobrow-chap3","url":"/2020/05/06/Dobrow-chap3/","content":"\n\nThis is a note of the textbook `Introduction to stochastic processes with R`\n\n> There exists everywhere a medium in things, determined by equilibrium.\n>                                                  —Dmitri Mendeleev\n\n承接上章最后的数值案例，本章主要讲转移步数趋于无穷时马尔可夫链的性质。\n\n### 3.1 Limiting Distribution\n\n#### 3.1.1 定义\n\n作者给了有一个定义和三个等价定义。这个含义最清楚：\n\n> A limiting distribution\nfor the Markov chain is a probability distribution 𝝀 with the property that, for any initial distribution $\\boldsymbol{\\alpha}$:\n> $$\\lim_{n\\rightarrow\\infty}\\boldsymbol{\\alpha}P^n=\\boldsymbol{\\lambda}$$\n\n这个定义与原定义的等价性也很容易理解：\n\n若对某一马尔可夫链的状态转移矩阵有：\n$$\\lim_{n\\rightarrow\\infty}P^n_{ij}=\\lambda_j$$\n并且设初始分布$\\boldsymbol{\\alpha}=(\\alpha_1,\\dots,\\alpha_n)$,状态总数为$m$\n则有：\n\n$$\n\\begin{aligned}\n  \\lim_{n\\rightarrow\\infty}\\boldsymbol{\\alpha}\\boldsymbol{P}^n &=(\\alpha_1,\\dots,\\alpha_n)\\begin{pmatrix}\n   \\lambda_1 & \\lambda_2 & \\cdots &  \\lambda_m \\\\\n   \\lambda_1 & \\lambda_2 & \\cdots &  \\lambda_m \\\\\n   \\vdots & \\vdots & \\ddots &  \\lambda_m \\\\\n   \\lambda_1 & \\lambda_2 & \\cdots &  \\lambda_m\n  \\end{pmatrix} \\\\ \n  & = \\begin{pmatrix}\n    \\lambda_1\\sum_{i=1}^n\\alpha_i \\\\\n    \\lambda_2\\sum_{i=1}^n\\alpha_i \\\\\n    \\cdots \\\\\n    \\lambda_m\\sum_{i=1}^n\\alpha_i\n  \\end{pmatrix} \\\\\n  & = \\begin{pmatrix}\n    \\lambda_1 \\\\\n    \\lambda_2 \\\\\n    \\cdots \\\\\n    \\lambda_m\n  \\end{pmatrix} \\\\\n  & = \\boldsymbol{\\lambda}\n\\end{aligned}\n$$\n\n\nEx3.1 Two state Markov Chain\n计算案例；\n\n#### 3.1.2 Proportion of Time in Each State\n\n利用计算条件期望的技术，来从状态在过程中所占的比例来理解Limit distribution\n\n\n\nEx3.2\n\n```r\n###### Simulate discrete-time Markov chain ########################\n# Simulates n steps of a Markov chain \n# markov(init,mat,n,states)\n# Generates X0, ..., Xn for a Markov chain with initiial\n#  distribution init and transition matrix mat\n# Labels can be a character vector of states; default is 1, .... k\n\nmarkov <- function(init,mat,n,labels) { \n\tif (missing(labels)) labels <- 1:length(init)\nsimlist <- numeric(n+1)\nstates <- 1:length(init)\nsimlist[1] <- sample(states,1,prob=init)\nfor (i in 2:(n+1)) \n\t{ simlist[i] <- sample(states,1,prob=mat[simlist[i-1],]) }\nlabels[simlist]\n}\n####################################################\nP <- matrix(c(0.1,0.2,0.4,0.3,0.4,0,0.4,0.2,0.3,0.3,0,0.4,0.2,0.1,0.4,0.3),\n  nrow=4, byrow=TRUE)\nlab <- c(\"Aerobics\",\"Massage\",\"Weights\",\"Yoga\")\nrownames(P) <- lab\ncolnames(P) <- lab\nP\ninit <- c(1/4,1/4,1/4,1/4) # initial distribution\nstates <- c(\"a\",\"m\",\"w\",\"y\")\n# simulate chain for 100 steps\nsimlist <- markov(init,P,100,states)\nsimlist\ntable(simlist)/100\nsteps <- 1000000\nsimlist <- markov(init,P,steps,states)\ntable(simlist)/steps\n```\n\n### 3.2 Stationary Distribution\n\n#### 3.2.1 定义\n注意Stationary Distribution的定义没有出现极限。\n\n> If the initial distributino is a stationary distribution, Then $X_0,X_1,\\cdots,X_n$ is a sequence of identically distributed random variables. But it doesn't mean that the random variables are independent.\n\n> Lemma 3.1: Limiting Distributions are stationary Distribution\n\nThe reverse is false, 反例：\n\n$$\n  P = \\begin{pmatrix}\n    0 & 1 \\\\\n    1 & 0\n  \\end{pmatrix}\n  ，\\pi=(\\frac{1}{2},\\frac{1}{2}) \n$$\n\n以及\n\n$$\n   P = \\begin{pmatrix}\n    1 & 0 \\\\\n    0 & 1\n  \\end{pmatrix}\n$$\n\n#### 3.2.2 Regular Matrices\n\n一个自然的想法是问，什么样的条件下，一个马尔可夫链有极限分布，而且极限分布就是平稳分布呢。\n\n满足如下性质的马尔可夫链是符合这个要求的\n\n> Regular Transition Matrix\n> A transition matrix $\\boldsymbol{P}$ is said to be regular if some power of $\\boldsymbol{P}$ is positive. That is $\\boldsymbol{P}^n > 0 $, for some $n\\ge 1$\n\n有定理：\n\n> Theorem 3.2: A markov chain whose transition matrix $\\boldsymbol{P}$ is regular has a limiting distribution, which is teh unique, positive, stationary distribution of the chanin.\n\nEx 3.3-3.4 具体算例，\n\n#### 3.2.3 Finding the stationary distribution\n\n本质上这是一个特征值问题。\n\n```r\n### Stationary distribution of discrete-time Markov chain\n###  (uses eigenvectors)\n###\nstationary <- function(mat) {\nx = eigen(t(mat))$vectors[,1]\nas.double(x/sum(x))\n}\n```\n\nEx 3.5-3.6; 计算技巧，令$x_1=1$\n\nEx 3.7 The Ehrenfest dog-flea model\n\nEx 3.8 Random walk on a graph;\n\nOn weighted graph\n\n> Stationiary Distribution for Random walk on a weighted graph\n> \n> Let $G=(V,E)$ be  a weighted graph with edge weight function $w(i,j)$. For random walk on G, the stationary distribution $pi$ is proportion to the sum of teh edge weights incident to each vertex. That is.\n> $$ \\pi = \\frac{w(v)}{\\sum_{z}w(z)},\\forall v\\in V$$ \n> where\n> $$ w(v) = \\sum_{z \\sim v }w(v,z) $$\n\nOn simple graph\n\n> Stationary Distribution for simple Random Walk on a graph\n> \n> For simple random walk on a weighted graph, set $w(i,j)=1,\\forall i,j \\in V $, then, $w(v)=deg(v)$,which gives\n> $$ \\pi_{v}=\\frac{\\deg(v)}{\\sum_z \\deg(z)}=\\frac{\\deg(v)}{2e} $$\n\nEx 3.9-3.10 如何计算的案例；\n\n#### 3.2.4 The Eigenvalue Connection\n\n转置，看出与特征值的关联。\n\nEx 3.10 理论案例： random walk in regular graph.。\n\n### 3.3 Can you find the way to state $a$\n\n#### 3.3.1 状态可到达与状态互通\n\n> Say that state $j$ is accessible from state i, if $P_{ij}^n > 0$. That is,there is positive probability of reaching $j$ from $i$ in a finite number of steps. State $i$ and $j$ communicate if $i$ is accessible from $j$ and $j$ is accessible from $i$\n\neg 3.11 本例讲述了用 Transition graphs 展示 Communication classes\n\n#### 3.3.2 不可约\n\n> Irreducibility\n> A Markov chain is called irreducible if it has exactly one cmmunication class. That is, all states communicate with each other\n\nEx 3.12 一个不可约链的例子；\n\n#### 3.3.3 Recurrence and Transience\n\n> Given a Markov chain $X_0,X_1,\\dots$, let $T_j=\\min\\{n>0:X_n=j\\}$be the first passage time to state $j$. If $X_n\\ne j,\\forall n>0$, see$T_j=\\infty$. Let\n> $$ f_j = P(T_j < \\infty | X_0=j)$$\n> be the probability started in $j$ eventually returns to $j$.\n> \n> State $j$ is said to be recurrent if the Markov chain started in $j$ eventually revists $j$. That is $f_j=1$\n> \n> State $j$ is said to be transient if there is positive probability that the Markov chain started in j never returns to $j$. That is $f_j < 1$\n\n如何根据状态转移矩阵判定，某一个状态是Recurrent或Transient States。用示性函数，\n\n$E(\\sum_{n=0}^{\\infty}I_n)=\\sum_{n=0}^{\\infty}E(I_n)=\\sum_{n=0}^{\\infty}P(X_n=j|X_0=i)=\\sum_{n=0}^{\\infty}P_{ij}^{n}$\n\n由此可以推出另外一个判定条件；\n\n> Recurrence, Transience\n> \n> (i) State $j$ is recurrent if and only if\n> $$ \\sum_{n=0}^{\\infty}P_{ij}^n=\\infty $$\n> \n> (ii) State j is transient if and only if\n> \n> $$\\sum_{n=0}^{\\infty}P_{ij}^n<\\infty$$\n\n> Recuurence and Transience are Class Properties\n> \n> Theorem 3.3 The states of a communication class are either all recurrent or all transient.\n> Corollary 3.4 For a finite irreducible Markov chain, all states are recuurent.\n\nEx 3.13 接下来的例子是简单的一维随机游走；这个例子可以推广到高维。\n\n#### 3.3.4 Canonical Decomposition\n\nClosed Communication Class\n\n> Lemma 3.5 A communication class is closed if it consists of all recurrent states. A finite communication class is closed only if it consits of all recurrent states.\n\n反证法即可证得；最后便可以得到，我们想定义的；\n\n> The state space S of a finite Markov chain can be partitioned into transient and reccurent states as $S=T \\cup R_1 \\cup \\cdots R_m$, where T is the set of all transient states and $R_i$ are closed communiction classes of recurrent states. This is called the canonical decomposition.\n\n注：由等价类的定义可以保障这么重排状态转移矩阵，是与原矩阵等价的。\n\n> Given a canonical decomposition, the state space can be reordered so that the Markov transition matrix has the block matrix form\n\n$$\n\t\\boldsymbol{P}=\n\t\\left(\n\t\\begin{array}{c|c}\n\t\\boldsymbol{Q} & \\ast & \\ast & \\cdots & \\ast \\\\ \\hline \n\t\\boldsymbol{O} & \\boldsymbol{P_1} & \\boldsymbol{O} &\\cdots & \\boldsymbol{O}\\\\\n  \\boldsymbol{O} & \\boldsymbol{O} & \\boldsymbol{P_2} &\\cdots & \\boldsymbol{O} \\\\\n  \\vdots & \\vdots & \\vdots &\\ddots & \\vdots \\\\\n  \\boldsymbol{O} & \\boldsymbol{O} & \\boldsymbol{O} &\\cdots & \\boldsymbol{P_m}\n\t\\end{array}\n\t\\right)\n$$\n\n其中$\\boldsymbol{O}=(p_{ij}=0),\\boldsymbol{Q}=(p_{ij})_{i,j \\in T},\\boldsymbol{P_l}=(p_{ij})_{i,j \\in R_l},l=1,2,\\cdots,m$\n\nEx3.14 具体 case;\n\n更进一步有：\n\n$$\n\t\\lim_{n\\rightarrow\\infty} \\boldsymbol{P}^n=\n\t\\left(\n\t\\begin{array}{c|c}\n\t\\boldsymbol{O} & \\ast & \\ast & \\cdots & \\ast \\\\ \\hline \n\t\\boldsymbol{O} & \\lim_{n\\rightarrow\\infty}\\boldsymbol{P_1}^n & \\boldsymbol{O} &\\cdots & \\boldsymbol{O}\\\\\n  \\boldsymbol{O} & \\boldsymbol{O} & \\lim_{n\\rightarrow\\infty}\\boldsymbol{P_2}^n &\\cdots & \\boldsymbol{O} \\\\\n  \\vdots & \\vdots & \\vdots &\\ddots & \\vdots \\\\\n  \\boldsymbol{O} & \\boldsymbol{O} & \\boldsymbol{O} &\\cdots & \\lim_{n\\rightarrow\\infty}\\boldsymbol{P_m}^n\n\t\\end{array}\n\t\\right)\n$$\n\n---\nedit: 2020-05-05\n\n### 3.4 Irreducible Markov Chains\n\n#### 3.4.1 定理\n\n假设地震台通过统计数据得到了一个不同级别的地震的状态概率矩阵，政府和大众关心的问题是，从上一次地震到即将发生的下一次地震，大概需要多少年？\n\n一个显然的直觉上的感觉是，根据地震的强度而定。不同强度不同。\n\n可以用马尔可夫链作为上述问题的数学模型。\n\n回顾停时的定义$T_j = \\min\\{n>0:X_n=j\\}$，有如下定理：\n\n> **Theorem 3.6 Limit Theorem for Finite Irreducible Markov Chains**. Assume that $X_0,X_1,\\dots$, is a finite irreducible Markov Chain. For each state $j$. let $\\mu_j=E(T_j|X_0=j)$ be the expected return time to $j$. Then, $\\mu_j$ is finite, and there exisits a unique, positve stationary distribution $\\boldsymbol{\\pi}$ such that \n> $$ \\pi_j =\\frac{1}{\\mu_j} , \\forall j$$\n> Furthermore, $\\forall i$\n> $$ \\pi_j = \\lim_{n\\rightarrow\\infty}\\frac{1}{n}\\sum_{m=0}^{n-1}P^m_{ij} $$\n\n\nEx 3.15 地震的 recurrence的例子；\n\nEx 3.16 Frog jumping 算例；\n\n#### 3.4.2 First-Step Analysis\n\nEx 3.17 一个计算Expected return time的例子；\n\n### 3.5 Periodicity\n\n如果该马尔可夫链没有平稳分布，该如何研究？\n\n具体的案例可以看第一章最后的那几个算例。\n\n答案是引入周期的概念。\n\n> **Lemma 3.7** The states of a communication class all have the same period\n\n根据这条引理，有\n\n> A Markov chain is called perioidic if is irreducible and all states have period greater than 1. A Markov chain is called aperiodic if it is irreducible and all states have period equal 1.\n\n### 3.6 Ergoidc Markov Chains \n\nA Markov chain is called ergodic if it is irreducible, aperiodic, and all states have finite expected return times.\n\n> **Fundamental Limit Theorem for Ergodic Markov Chains**\n> **Therem 3.8** Let $X_0,X_1,\\cdots$ be an ergodic Markov chain. There exists a unique, positive, stationary distribution $\\boldsymbol{\\pi}$,which is the limiting distribution of the chain. That is, \n> $$\\pi_j = \\lim_{n\\rightarrow\\infty}P^n_{ij},\\forall i,j$$\n\nEx 3.19（Modified Ehrenfrest Model）; \n\nEx 3.20 算例;\n\nEx 3.21 PageRank; (damping factor,p=0.85)\n\n---\nedit: 2020-05-06\n\n### 3.7 Time reversibility\n\n#### 3.7.1 Definition\n\nThe property of time reversibility can be explained intuitively as follows. If you were to take a movie of Markov chain moving forward in time and then run the movie backwards, you could not tell the difference between the two.\n\n换成数学上的语言就是，如果假设马尔可夫链处于稳态，这时存在：\n\n$$ P(X_0=i,X_1=j)=P(X_0=j,X_1=i) $$\n\n由全概率公式可知；\n\n> **Time Reversibility**\n> \n> An irreducible Markov chain with transition matrix P and stationary distribution $\\boldsymbol{\\pi}$, if\n> $$ \\pi_iP_{ij}=\\pi_jP_{ji},\\forall i,j $$\n\n\nEx 3.23; \n\nEx 3.24；Simple random walk on a graph is time \n\n\n\n#### 3.7.2 Reversible Markov Chains and Radom walk\n\nEvery reversible Markov chain can be considered as a random walk on a weighted graph\n\nEx 3.25 算例；\n\n#### 3.7.3 The key benifit of reversibility\n\n> **Proposition 3.9** Let $\\boldsymbol{P}$ be the transition matrix of a Markov chain. If $\\boldsymbol{x}$ is a probability distribution which satisfies\n> $$ x_iP_{ij}=x_jP_{ji},\\forall i,j $$\n> then $\\boldsymbol{x}$ is the stationary distribution, and the markov chain is reversible.\n\nEx 3.26 **Birth-and-death chain**\n\nCase: random walk with a partialy relecting boundary.\n\n### 3.8 Absorbing Chains\n\n#### 3.8.1 定义\n\n> **Absorbing State, Absorbing Chain**\n> \n> State $i$ is an absorbing state if $P_{ii}=1$. A Markov chain is called an absorbing chain if it has at leat one absorbing state.\n\n根据这个定义，一个吸收的马尔可夫链的canoical decompostion可以写为：\n$$\n  \\boldsymbol{P} = \\lim_{n\\rightarrow\\infty} \\boldsymbol{P}^n=\n\t\\left(\n\t\\begin{array}{c|c}\n\t\\boldsymbol{Q} & \\boldsymbol{R} \\\\\n  \\hline\n  \\boldsymbol{O} & \\boldsymbol{I}\n\t\\end{array}\n\t\\right)\n$$\n\nEx 3.31\n\n#### 3.8.2 Expected Number of Visits to Transient States\n\n> **Theorem 3.11** Consider an absorbing Markov chain with t transient states. Let $\\boldsymbol{F}$ be a $t\\times t$ matrix indexed by transient states. where $F_{ij}$ is the expected number of visits to $j$ given that the chain starts in $i$, Then, \n> $$F=(I-Q)^{-1}$$\n\n#### 3.8.3 Expected Time to Absorption\n\n> **Absorbing Markov Chains**\n> For an absorbing Markov chain with all states either transient or absorbing. Let $F=(I-Q)^{-1}$\n> 1. (Absorption probability) The probability that from transient state $i$ the chain is absorbed in state $j$ is $(FR)_{ij}$\n> 2. (Absorption time) The expected number of steps from transient state $i$ until the chain is absorbed in some absorbing state is $(F1)_i$  \n\n#### 3.8.4 Expected Hitting Time for Irreducible chain\n\n#### 3.8.5 Patterns in Sequences\n\n#### 3.9 Regenration and strong Markov property\n\n#### 3.10 Proofs of limiting Theorem\n\n剩下的都是常规内容。不再赘述。\n\n总的来说，这一章的章节组织很有条理，对初学者友好。而且选的例子很有启发性。\n\n---\n题外话：\n\n有个值得思考的问题，这种偏向应用的数学内容的教材，如何平衡论述的逻辑，理论以及思考的深度，以及应用的广度，以及对于读者的吸引性和实用性，都是很需要功底的。但是这方面做的好的教材真的太少了。\n\n理想的大学教师，是学术成就和教学成就都很出色，但是这毕竟是少数。\n\n有一对儿相互矛盾的命题：\n\n+ 大学生应该提高自学能力，不要指望老师手把手的教？\n+ 国家给了大学老师工资，大学生也付了学费，如果都靠自学的话，要大学干什么？\n\n依笔者看来，大学提供的是一套适合学习和研究的硬件设施，一张平静的书桌，一群志同道合的良师益友。这些环境和人，是任何其它机构或者网课代替不了的。","tags":["note","stochastic Process"],"categories":["math"]},{"url":"/2020/05/04/networkdata-quickstart/","content":"\n所选例子出自**Modern Statistics for Modern Biology**(Susan Holmes, Wolfgang\nHuber)\n\n无向图的邻接矩阵是一个0-1矩阵：\n\n```r\nlibrary(igraph)\nlibrary(ggplot2)\nlibrary(ggnetwork)\nlibrary(network)\nedges1 <- matrix(c(1,3,2,3,3,4,4,5,4,6),byrow = TRUE,ncol = 2)\n\n### generate adjacency matrix\nedges1 <- as.data.frame(edges1)\nmat <- matrix(data = 0,nrow = 6,ncol = 6)\nfor(ii in 1:6){\n  mat[edges1[ii,1],edges1[ii,2]] <- 1\n  mat[edges1[ii,2],edges1[ii,1]] <- 1\n}\n\n### Prepare data to plot\ndat_long <- reshape2::melt(mat)\ndat_long$value <- as.factor(dat_long$value)\ncolnames(dat_long) <- c(\"V1\",\"V2\",\"value\")\n### plot\ngg <- ggplot(dat_long)+\n  geom_tile(aes(V1,V2,fill=value), color=\"#7f7f7f\")+\n  scale_fill_manual(values=c(\"white\", \"black\"))+\n  coord_equal()+\n  labs(x=NULL, y=NULL)+\n  scale_x_continuous(breaks = 1:6)+\n  scale_y_reverse(breaks=1:6)+\n  theme_bw()+\n  theme(panel.grid=element_blank())+\n  theme(panel.border=element_blank())\ngg\n```\n\n<img src=\"/figure/posts/networkdata_quickstart_files/figure-gfm/unnamed-chunk-1-1.png\" style=\"display: block; margin: auto;\" />\n\n从邻接矩阵得到Graph:\n\n``` r\ng1 <- graph_from_adjacency_matrix(mat,mode = \"undirected\")\nplot(g1,vertex.size=25,edge.width=5,vertex.color=\"coral\")\n```\n\n<img src=\"/figure/posts/networkdata_quickstart_files/figure-gfm/unnamed-chunk-2-1.png\" style=\"display: block; margin: auto;\" />\n\n给定edgelist，得到Graph\n\n``` r\nedges1 <- matrix(c(1,3,2,3,3,4,4,5,4,6),byrow = TRUE,ncol = 2)\ng1 <- graph_from_edgelist(edges1,directed = F)\nplot(g1,vertex.size=25,edge.width=5,vertex.color=\"coral\")\n```\n\n<img src=\"/figure/posts/networkdata_quickstart_files/figure-gfm/unnamed-chunk-3-1.png\" style=\"display: block; margin: auto;\" />\n\n更为高级的是，从数据中计算出邻接矩阵，并且自定义可视化的layout。\n\n``` r\nlibrary(rworldmap)\n### obtain data; get the binary matrix\nload(\"D:/tmp/Moderstatdata/data/dist2009c.RData\")\ncountry09 = attr(dist2009c, \"Label\")\nmstree2009 = ape::mst(dist2009c)\n\n### calculate layout from world map\nmat = match(country09, countriesLow$NAME)\ncoords2009 = data.frame(\n  lat = countriesLow$LAT[mat],\n  lon = countriesLow$LON[mat],\n  country = country09)\nlayoutCoordinates = cbind(\n  x = jitter(coords2009$lon, amount = 15),\n  y = jitter(coords2009$lat, amount = 8))\nlabc = names(table(country09)[which(table(country09) > 1)])\nmatc = match(labc, countriesLow$NAME)\ndfc = data.frame(\n  latc = countriesLow$LAT[matc],\n  lonc = countriesLow$LON[matc],\n  labc)\ndfctrans = dfc\ndfctrans[, 1] = (dfc[,1] + 31) / 93\ndfctrans[, 2] = (dfc[,2] + 105) / 238\nggeo09 = ggnetwork(mstree2009, arrow.gap = 0, layout = layoutCoordinates)\n###plot\nggplot(ggeo09, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_edges(color = \"black\", alpha = 0.5, curvature = 0.1) +\n  geom_nodes(aes(color = vertex.names), size = 2) +\n  theme_blank() +\n  geom_label(data = dfctrans, aes(x = lonc, xend = lonc, y = latc, yend = latc,\n       label = labc, fill = labc), colour = \"white\", alpha = 0.5, size = 3) +\n   theme(legend.position = \"none\")\n```\n\n![](/figure/posts/networkdata_quickstart_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->\n","tags":["R","Graph","Network"],"categories":["implementation"]},{"title":"Calculate pi in R quikstart","url":"/2020/04/24/Calculate-pi-in-R-quikstart/","content":"\n\nR中也可以用`Rmpfr`包实现多精度的计算。例如，我们可以用如下代码实现AGM算法计算Pi到小数点后256位。\n\n``` r\nlibrary(Rmpfr)\npiMpfr <- function(prec=256, itermax = 100, verbose=TRUE) {\n  m2 <- mpfr(2, prec) # '2' as mpfr number\n  ## -> all derived numbers are mpfr (with precision 'prec')\n  p <- m2 + sqrt(m2) # 2 + sqrt(2) = 3.414..\n  y <- sqrt(sqrt(m2)) # 2^ {1/4}\n  x <- (y+1/y) / m2\n  it <- 0L\n  repeat {\n    p.old <- p\n    it <- it+1L\n    p <- p * (1+x) / (1+y)\n    if(verbose) cat(sprintf(\"it=%2d, pi^ = %s, |.-.|/|.|=%e\\n\",\n                            it, formatMpfr(p, min(50, prec/log2(10))), 1-p.old/p))\n    if (abs(p-p.old) <= m2^(-prec))\n      break\n    if(it > itermax) {\n      warning(\"not converged in\", it, \"iterations\") ; break\n    }\n    ## else\n    s <- sqrt(x)\n    y <- (y*s + 1/s) / (1+y)\n    x <- (s+1/s)/2\n  }\n  p\n}\npiMpfr(prec = 256)\n```\n\n    ## it= 1, pi^ = 3.1426067539416226007907198236183018919713562462772, |.-.|/|.|=-8.642723e-02\n    ## it= 2, pi^ = 3.1415926609660442304977522351203396906792842568645, |.-.|/|.|=-3.227958e-04\n    ## it= 3, pi^ = 3.1415926535897932386457739917571417940347896238675, |.-.|/|.|=-2.347934e-09\n    ## it= 4, pi^ = 3.1415926535897932384626433832795028841972241204666, |.-.|/|.|=-5.829228e-20\n    ## it= 5, pi^ = 3.1415926535897932384626433832795028841971693993751, |.-.|/|.|=-1.741826e-41\n    ## it= 6, pi^ = 3.1415926535897932384626433832795028841971693993751, |.-.|/|.|=0.000000e+00\n\n    ## 1 'mpfr' number of precision  256   bits \n    ## [1] 3.141592653589793238462643383279502884197169399375105820974944592307816406286163\n","tags":["R"],"categories":["implementation"]},{"title":"A Hard Rain's A-Gonna Fall","url":"/2020/04/20/A-Hard-Rain-s-A-Gonna-Fall/","content":"\n*A Hard Rain's A-Gonna Fall*是美国国宝级歌手，诗人，诺贝尔文学奖得主Bob Dylan的名曲之一，虽然原唱说不上多么好听，但是歌词很有意境，分享如下：\n\n> A Hard Rain's A-Gonna Fall\n> \n> Bob Dylan\n> \n> Oh, where have you been, my blue-eyed son?\n> \n> Oh, where have you been, my darling young one?\n> \n> I've stumbled on the side of twelve misty mountains\n> \n> I've walked and I've crawled on six crooked highways\n> \n> I've stepped in the middle of seven sad forests\n> \n> I've been out in front of a dozen dead oceans\n> \n> I've been ten thousand miles in the mouth of a graveyard\n> \n> And it's a hard, and it's a hard, it's a hard, and it's a hard\n> And it's a hard rain's a-gonna fall\n> \n> Oh, what did you see, my blue-eyed son?\n> \n> Oh, what did you see, my darling young one?\n> \n> I saw a newborn baby with wild wolves all around it\n> \n> I saw a highway of diamonds with nobody on it\n> \n> I saw a black branch with blood that kept drippin'\n> \n> I saw a room full of men with their hammers a-bleedin'\n> \n> I saw a white ladder all covered with water\n> \n> I saw ten thousand talkers whose tongues were all broken\n> \n> I saw guns and sharp swords in the hands of young children\n> \n> And it's a hard, and it's a hard, it's a hard, it's a hard\n> \n> And it's a hard rain's a-gonna fall\n> \n> And what did you hear, my blue-eyed son?\n> \n> And what did you hear, my darling young one?\n> \n> I heard the sound of a thunder, it roared out a warnin'\n> \n> Heard the roar of a wave that could drown the whole world\n> \n> Heard one person starve, I heard many people laughin'\n> \n> Heard the song of a poet who died in the gutter\n> \n> Heard the sound of a clown who cried in the alley\n> \n> And it's a hard, and it's a hard, it's a hard, it's a hard\n> \n> And it's a hard rain's a-gonna fall\n> \n> Oh, who did you meet, my blue-eyed son?\n> \n> Who did you meet, my darling young one?\n> \n> I met a young child beside a dead pony\n> \n> I met a white man who walked a black dog\n> \n> I met a young woman whose body was burning\n> \n> I met a young girl, she gave me a rainbow\n> \n> I met one man who was wounded in love\n> \n> I met another man who was wounded with hatred\n> \n> And it's a hard, it's a hard, it's a hard, it's a hard\n> \n> It's a hard rain's a-gonna fall\n> \n> Oh, what'll you do now, my blue-eyed son?\n> \n> Oh, what'll you do now, my darling young one?\n> \n> I'm a-goin' back out 'fore the rain starts a-fallin'\n> \n> I'll walk to the depths of the deepest black forest\n> \n> Where the people are many and their hands are all empty\n> \n> Where the pellets of poison are flooding their waters\n> \n> Where the home in the valley meets the damp dirty prison\n> \n> Where the executioner's face is always well-hidden\n> \n> Where hunger is ugly, where souls are forgotten\n> \n> Where black is the color, where none is the number\n> \n> And I'll tell it and think it and speak it and breathe it\n> \n> And reflect it from the mountain so all souls can see it\n> \n> Then I'll stand on the ocean until I start sinkin'\n> \n> But I'll know my song well before I start singin'\n> \n> And it's a hard, it's a hard, it's a hard, it's a hard\n> \n> It's a hard rain's a-gonna fall","tags":["art"],"categories":["others"]},{"title":"LearnSeurat_CITE_seq","url":"/2020/04/20/LearnSeurat_CITEseq/","content":"\n### 前言\n\nCITE-seq是Rahul Satija和Peter\nSmibert两个组合作开发的在单细胞精度，同时测量细胞表面蛋白表达和转录组的[技术](https://www.nature.com/articles/nmeth.4380)。该技术原理如下：\n\n![CITE-seq原理图，用抗体来源标签，实现细胞表面蛋白定量](https://imgkr.cn-bj.ufileos.com/b7e96d47-230d-4519-a758-c3aa23939b18.png)\n\n该项技术可以用于免疫相关的单细胞测序研究中。例如,\n有[研究表明](https://www.nature.com/articles/s41586-020-2134-y)称：\n\n> 他们在人和小鼠非小细胞肺癌中进行单细胞RNA测序，鉴定了一群DC，并将其命名为“富含免疫调节分子的成熟DC”（mregDC），这是由于它们共表达了免疫调节基因（Cd274，Pdcd1lg2和Cd200）和成熟基因（Cd40，Ccr7和Il12b）。\n\n这段中文报道来自[小柯机器人](http://news.sciencenet.cn/htmlpaper/2020/3/20203301537131655622.shtm)\n\nRahul\nSatija组开发的软件Seurat有一个[教程](https://satijalab.org/seurat/v3.1/multimodal_vignette.html)，可以分析CITE-seq数据。本文基于该教程对该类型数据的分析进行说明。\n\n### 数据载入\n\n首先我们需要获取数据，该数据集取样为8617个脐带血单核细胞，包含了表达谱数据和11个抗体来源标签数据（antibody-derived\ntags ,ADT)。\n\n``` r\nlibrary(Seurat)\nlibrary(SeuratData)\nlibrary(ggplot2)\nlibrary(patchwork)\n### AvailableData() check avaliable data: we choose cbmc\n### InstallData('cbmc')\nlibrary(cbmc.SeuratData)\ndata(\"cbmc\")\n### expression matrix\ncbmc[[\"RNA\"]]@counts[1:10,1:10]\n```\n\n    ## 10 x 10 sparse Matrix of class \"dgCMatrix\"\n    ##                             \n    ## A1BG     . . . . . . . . . .\n    ## A1BG-AS1 . . . . . . . . . .\n    ## A1CF     . . . . . . . . . .\n    ## A2M      . . . . . . . . . .\n    ## A2M-AS1  . . . . . . . 1 . .\n    ## A2ML1    . . . . . . . . . .\n    ## A4GALT   . . . . . . . . . .\n    ## A4GNT    . . . . . . . . . .\n    ## AAAS     . . . . . . . . . 1\n    ## AACS     . . . . . . . . . .\n\n``` r\n### ADT count matrix\n### Actually there are just 10 surface protein\ncbmc[[\"ADT\"]]@counts[1:10,1:10] \n```\n\n    ## 10 x 10 sparse Matrix of class \"dgCMatrix\"\n    ##                                                \n    ## CD3     60   52  89  55  63  82  53  42 103  56\n    ## CD4     72   49 112  66  80  78  63  59 122  70\n    ## CD8     76   59  61  56  94  57  61  55  64  80\n    ## CD45RA 575 3943 682 378 644 479 487 472 540 535\n    ## CD56    64   68  87  58 104  44  64  48 136  91\n    ## CD16   161  107 117  82 168  92  77  99 235 131\n    ## CD11c   77   65  65  44  92  63  70  75 106  69\n    ## CD14   206  129 169 136 164 122 112 111 206 204\n    ## CD19    70  665  79  49  81  44  60  58  61 107\n    ## CD34   179   79  78  83 152 103  79  86 144 193\n\n``` r\n### show default assay\nDefaultAssay(cbmc)\n```\n\n    ## [1] \"RNA\"\n\n### 根据基因表达进行聚类\n\n注意在默认参数的情况下，下述操作时对`Default Assay`进行的\n\n``` r\n# standard log-normalization\ncbmc <- NormalizeData(cbmc)\n# choose ~1k variable features\ncbmc <- FindVariableFeatures(cbmc)\n# standard scaling (no regression)\ncbmc <- ScaleData(cbmc)\n# Run PCA, select 13 PCs for tSNE visualization and graph-based clustering\ncbmc <- RunPCA(cbmc, verbose = FALSE)\n```\n\n下面的图是根据标准差来选择PCs\n\n``` r\nElbowPlot(cbmc, ndims = 50)\n```\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-3-1.png\" style=\"display: block; margin: auto;\" />\n\n聚类和t-SNE降维\n\n``` r\ncbmc <- FindNeighbors(cbmc, dims = 1:25)\ncbmc <- FindClusters(cbmc, resolution = 0.8)\n```\n\n    ## Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck\n    ## \n    ## Number of nodes: 8617\n    ## Number of edges: 347548\n    ## \n    ## Running Louvain algorithm...\n    ## Maximum modularity in 10 random starts: 0.8592\n    ## Number of communities: 19\n    ## Elapsed time: 3 seconds\n\n``` r\ncbmc <- RunTSNE(cbmc, dims = 1:25, method = \"FIt-SNE\")\n\n# Find the markers that define each cluster, and use these to annotate the clusters, we use\n# max.cells.per.ident to speed up the process\ncbmc.rna.markers <- FindAllMarkers(cbmc, max.cells.per.ident = 100, min.diff.pct = 0.3, only.pos = TRUE)\n\n# Note, for simplicity we are merging two CD14+ Monocyte clusters (that differ in expression of\n# HLA-DR genes) and NK clusters (that differ in cell cycle stage)\nnew.cluster.ids <- c(\"Memory CD4 T\", \"CD14+ Mono\", \"Naive CD4 T\", \"NK\", \"CD14+ Mono\", \"Mouse\", \"B\", \n    \"CD8 T\", \"CD16+ Mono\", \"T/Mono doublets\", \"NK\", \"CD34+\", \"Multiplets\", \"Mouse\", \"Eryth\", \"Mk\", \n    \"Mouse\", \"DC\", \"pDCs\")\nnames(new.cluster.ids) <- levels(cbmc)\ncbmc <- RenameIdents(cbmc, new.cluster.ids)\n```\n\n我们看看聚类结果：\n\n``` r\nDimPlot(cbmc, label = TRUE) + NoLegend()\n```\n\n    ## Warning: Using `as.character()` on a quosure is deprecated as of rlang 0.3.0.\n    ## Please use `as_label()` or `as_name()` instead.\n    ## This warning is displayed once per session.\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-5-1.png\" style=\"display: block; margin: auto;\" />\n\n### 蛋白表达数据处理\n\nSeurat3的assay实现多个组学或者模态的数据的存储和获取。 代码里的注释来自Seurat官网。\n\n``` r\n# Now we can repeat the preprocessing (normalization and scaling) steps that we typically run\n# with RNA, but modifying the 'assay' argument.  For CITE-seq data, we do not recommend typical\n# LogNormalization. Instead, we use a centered log-ratio (CLR) normalization, computed\n# independently for each feature.  This is a slightly improved procedure from the original\n# publication, and we will release more advanced versions of CITE-seq normalizations soon.\ncbmc <- NormalizeData(cbmc, assay = \"ADT\", normalization.method = \"CLR\")\ncbmc <- ScaleData(cbmc, assay = \"ADT\")\n```\n\n在RNA表达谱的降维Embedding中同时展示展示蛋白表达水平和基因表达水平：\n\n散点图：横纵轴为降维的坐标：\n\n``` r\n# in this plot, protein (ADT) levels are on top, and RNA levels are on the bottom\nFeaturePlot(cbmc, \n            features = c(\"adt_CD3\", \"adt_CD11c\", \n                         \"adt_CD8\", \"adt_CD16\", \n                         \"CD3E\", \"ITGAX\", \"CD8A\", \"FCGR3A\"),\n            min.cutoff = \"q05\", \n            max.cutoff = \"q95\", \n            ncol = 2)\n```\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-7-1.png\" style=\"display: block; margin: auto;\" />\n\nRidge Plot:\n\n``` r\nRidgePlot(cbmc, features = c(\"adt_CD3\", \"adt_CD8\", \"CD3E\",\"CD8A\"),ncol = 2)\n```\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-8-1.png\" style=\"display: block; margin: auto;\" />\n\n散点图：横纵轴为表达量；这个类似于FACS\n\n``` r\n# Draw ADT scatter plots (like biaxial plots for FACS). Note that you can even 'gate' cells if\n# desired by using HoverLocator and FeatureLocator\nFeatureScatter(cbmc, feature1 = \"adt_CD19\", feature2 = \"adt_CD3\")\n```\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-9-1.png\" style=\"display: block; margin: auto;\" />\n\n我们也可以看看蛋白表达和基因表达的关系：\n\n``` r\n# view relationship between protein and RNA\nFeatureScatter(cbmc, feature1 = \"adt_CD3\", feature2 = \"CD3E\")\n```\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-10-1.png\" style=\"display: block; margin: auto;\" />\n\n我们可以看看T细胞：\n\n``` r\n# Let's plot CD4 vs CD8 levels in T cells\ntcells <- subset(cbmc, idents = c(\"Naive CD4 T\", \"Memory CD4 T\", \"CD8 T\"))\nFeatureScatter(tcells, feature1 = \"adt_CD4\", feature2 = \"adt_CD8\")\n```\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-11-1.png\" style=\"display: block; margin: auto;\" />\n\n选没有标准化的原始数据我们看看，坐标轴的间距太大，会有misleading\n\n``` r\n# # Let's look at the raw (non-normalized) ADT counts. You can see the values are quite high,\n# particularly in comparison to RNA values. This is due to the significantly higher protein copy\n# number in cells, which significantly reduces 'drop-out' in ADT data\nFeatureScatter(tcells, feature1 = \"adt_CD4\", feature2 = \"adt_CD8\", slot = \"counts\")\n```\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-12-1.png\" style=\"display: block; margin: auto;\" />\n\n这里还是可以观察到dropouts现象的，据原作者说： \\> If you look a bit more closely, you’ll\nsee that our CD8 T cell cluster is enriched for CD8 T cells, but still\ncontains many CD4+ CD8- T cells. This is because Naive CD4 and CD8 T\ncells are quite similar transcriptomically, and the RNA dropout levels\nfor CD4 and CD8 are quite high. This demonstrates the challenge of\ndefining subtle immune cell differences from scRNA-seq data alone.\n\n画热图，Seurat3 加了 downsample的功能。\n\n``` r\n# Downsample the clusters to a maximum of 300 cells each (makes the heatmap easier to see for small clusters)\ncbmc.small <- subset(cbmc, downsample = 300)\n# Find protein markers for all clusters, and draw a heatmap\nadt.markers <- rownames(cbmc.small[[\"ADT\"]]@counts)\n```\n\n我们可以看看Seurat热图的默认配色（三个冒号可以看更为底层的函数）, 个人觉得并不好看。\n\n``` r\n# using code from RColorBrewer to demo the palette\nn = 200\npar(mfrow=c(3,1))\nimage(\n  1:n, 1, as.matrix(1:n),\n  col = Seurat:::PurpleAndYellow(k=n),\n  xlab = \"PurpleAndYellow n\", ylab = \"\", xaxt = \"n\", yaxt = \"n\", bty = \"n\"\n)\nimage(\n  1:n, 1, as.matrix(1:n),\n  col = colorRampPalette(c(\"navy\", \"white\", \"firebrick3\"))(n),\n  xlab = \"NavyWhite3Firebrick3 n\", ylab = \"\", xaxt = \"n\", yaxt = \"n\", bty = \"n\"\n)\nimage(\n  1:n, 1, as.matrix(1:n),\n  col = colorRampPalette(RColorBrewer::brewer.pal(11,\"RdBu\"))(n),\n  xlab = \"RdBu n\", ylab = \"\", xaxt = \"n\", yaxt = \"n\", bty = \"n\"\n)\n```\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-14-1.png\" style=\"display: block; margin: auto;\" />\n\n把默认配色换掉,见\n\n``` r\nmypal <- rev(colorRampPalette(RColorBrewer::brewer.pal(11,\"RdBu\"))(256))\n#mypal2 <- colorRampPalette(c(\"navy\", \"white\", \"firebrick3\"))(256)\nDoHeatmap(cbmc.small, \n          features = unique(adt.markers), \n          assay = \"ADT\", \n          angle = 90,size = 3)+\n  scale_fill_gradientn(colors  = mypal)\n```\n\n    ## Scale for 'fill' is already present. Adding another scale for 'fill', which\n    ## will replace the existing scale.\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-15-1.png\" style=\"display: block; margin: auto;\" />\n\n去除细胞杂质，\n\n``` r\n# You can see that our unknown cells co-express both myeloid and lymphoid markers (true at the\n# RNA level as well). They are likely cell clumps (multiplets) that should be discarded. We'll\n# remove the mouse cells now as well\ncbmc <- subset(cbmc, idents = c(\"Multiplets\", \"Mouse\"), invert = TRUE)\n```\n\n### 直接根据蛋白质表达水平进行聚类\n\n``` r\n# Because we're going to be working with the ADT data extensively, we're going to switch the\n# default assay to the 'CITE' assay.  This will cause all functions to use ADT data by default,\n# rather than requiring us to specify it each time\nDefaultAssay(cbmc) <- \"ADT\"\ncbmc <- RunPCA(cbmc, features = rownames(cbmc), reduction.name = \"pca_adt\", reduction.key = \"pca_adt_\", \n    verbose = FALSE)\n```\n\n再来看PCA(其实这里算是degenrate到线性组合了)\n\n``` r\nDimPlot(cbmc, reduction = \"pca_adt\")\n```\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-18-1.png\" style=\"display: block; margin: auto;\" />\n\n``` r\n# Since we only have 10 markers, instead of doing PCA, we'll just use a standard euclidean\n# distance matrix here.  Also, this provides a good opportunity to demonstrate how to do\n# visualization and clustering using a custom distance matrix in Seurat\nadt.data <- GetAssayData(cbmc, slot = \"data\")\nadt.dist <- dist(t(adt.data))\n\n# Before we recluster the data on ADT levels, we'll stash the RNA cluster IDs for later\ncbmc[[\"rnaClusterID\"]] <- Idents(cbmc)\n\n# Now, we rerun tSNE using our distance matrix defined only on ADT (protein) levels.\ncbmc[[\"tsne_adt\"]] <- RunTSNE(adt.dist, assay = \"ADT\", reduction.key = \"adtTSNE_\")\ncbmc[[\"adt_snn\"]] <- FindNeighbors(adt.dist)$snn\ncbmc <- FindClusters(cbmc, resolution = 0.2, graph.name = \"adt_snn\")\n```\n\n    ## Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck\n    ## \n    ## Number of nodes: 7895\n    ## Number of edges: 258146\n    ## \n    ## Running Louvain algorithm...\n    ## Maximum modularity in 10 random starts: 0.9491\n    ## Number of communities: 11\n    ## Elapsed time: 2 seconds\n\n``` r\n# We can compare the RNA and protein clustering, and use this to annotate the protein clustering\n# (we could also of course use FindMarkers)\nclustering.table <- table(Idents(cbmc), cbmc$rnaClusterID)\nclustering.table\n```\n\n    ##     \n    ##      Memory CD4 T CD14+ Mono Naive CD4 T   NK    B CD8 T CD16+ Mono\n    ##   0          1754          0        1217   29    0    27          0\n    ##   1             0       2189           0    4    0     0         30\n    ##   2             3          0           2  890    3     1          0\n    ##   3             0          4           0    2  319     0          2\n    ##   4            24          0          18    4    1   243          0\n    ##   5             1         27           4  157    2     2         10\n    ##   6             4          5           0    1    0     0          0\n    ##   7             4         59           4    0    0     0          9\n    ##   8             0          9           0    2    0     0        179\n    ##   9             0          0           1    0    0     0          0\n    ##   10            1          0           2    0   25     0          0\n    ##     \n    ##      T/Mono doublets CD34+ Eryth   Mk   DC pDCs\n    ##   0                5     2     4   24    1    2\n    ##   1                1     1     5   25   55    0\n    ##   2                0     1     3    7    2    1\n    ##   3                0     2     2    3    0    0\n    ##   4                0     0     1    2    0    0\n    ##   5               56     0     9   16    6    2\n    ##   6                1   113    81   16    5    0\n    ##   7              117     0     0    2    0    1\n    ##   8                0     0     0    1    0    0\n    ##   9                0     0     0    0    1   43\n    ##   10               2     0     0    0    0    0\n\n下面这个embeding 还是根据ADT来的（不过只要marker连续，只有10个也没有关系？）\n\n``` r\nnew.cluster.ids <- c(\"CD4 T\", \"CD14+ Mono\", \"NK\", \"B\", \"CD8 T\", \"NK\", \"CD34+\", \"T/Mono doublets\", \n    \"CD16+ Mono\", \"pDCs\", \"B\")\nnames(new.cluster.ids) <- levels(cbmc)\ncbmc <- RenameIdents(cbmc, new.cluster.ids)\n\ntsne_rnaClusters <- DimPlot(cbmc, reduction = \"tsne_adt\", group.by = \"rnaClusterID\") + NoLegend()\ntsne_rnaClusters <- tsne_rnaClusters + ggtitle(\"Clustering based on scRNA-seq\") + theme(plot.title = element_text(hjust = 0.5))\ntsne_rnaClusters <- LabelClusters(plot = tsne_rnaClusters, id = \"rnaClusterID\", size = 4)\n\ntsne_adtClusters <- DimPlot(cbmc, reduction = \"tsne_adt\", pt.size = 0.5) + NoLegend()\ntsne_adtClusters <- tsne_adtClusters + ggtitle(\"Clustering based on ADT signal\") + theme(plot.title = element_text(hjust = 0.5))\ntsne_adtClusters <- LabelClusters(plot = tsne_adtClusters, id = \"ident\", size = 4)\n\n# Note: for this comparison, both the RNA and protein clustering are visualized on a tSNE\n# generated using the ADT distance matrix.\nwrap_plots(list(tsne_rnaClusters, tsne_adtClusters), ncol = 2)\n```\n\n<img src=\"/figure/posts/LearnSeurat_CITEseq_files/figure-gfm/unnamed-chunk-20-1.png\" style=\"display: block; margin: auto;\" />\n对于该结果，作者是这么解释的：\n\n> The ADT-based clustering yields similar results, but with a few\n> differences + Clustering is improved for CD4/CD8 T cell populations,\n> based on the robust ADT data for + CD4, CD8, CD14, and CD45RA +\n> However, some clusters for which the ADT data does not contain good\n> distinguishing protein markers (i.e. Mk/Ery/DC) lose separation You\n> can verify this using FindMarkers at the RNA level, as well\n\n### 更多\n\npbmc\n10k的细胞也提供了CITE-seq的多模态数据，具体细节，请看Seurat官方[教程](https://satijalab.org/seurat/v3.1/multimodal_vignette.html)。\n","tags":["R","scRNA-seq","sc-seq"],"categories":["implementation"]},{"title":"Learn-igraph-Basic","url":"/2020/04/20/Learnigraph-1-ManuNetworkDataBasic/","content":"\n*Learn-igraph*系列是对[Statistical Analysis of Network Data with\nR](https://link.springer.com/book/10.1007/978-1-4939-0983-4)一书的学习笔记，介绍如何使用R进行网络数据分析，网络数据的处理主要是基于`igraph`包，可视化用的是`ggnet`\n\n### 0. 基本概念\n\n一些需要知道的基本概念；\n\n  - Network;\n\n  - Graph;\n\n  - Order of a graph;\n\n  - Size of a graph;\n\n  - directed graph;\n\n  - undirected graph;\n\n  - subgraph;\n\n### 1. 创建igraph class\n\n#### 1.1 无向图\n\nigraph包处理网络图的数据结构为igraph class, 最基础的创建方式如下：\n\n``` r\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(ggnetwork)\ng <- graph.formula(1-2,1-3,2-3,2-4,3-5,4-5,4-6,4-7,5-6,6-7)\nl <- layout.auto(g)\nplot(g, layout=l, vertex.color=\"skyblue\")\n```\n\n<img src=\"/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-1-1.png\" style=\"display: block; margin: auto;\" />\n\n该网络的基本信息可以通过如下方式获得：\n\n``` r\nV(g)\n###+ 7/7 vertices, named, from 27d8280:\n###[1] 1 2 3 4 5 6 7\nE(g)\n###+ 10/10 edges from 27d8280 (vertex names):\n###[1] 1--2 1--3 2--3 2--4 3--5 4--5 4--6 4--7 5--6 6--7\n###str(g)\nget.adjedgelist(g)\n\n# $`1`\n# + 2/10 edges from f3f6e64 (vertex names):\n#   [1] 1--2 1--3\n# \n# $`2`\n# + 3/10 edges from f3f6e64 (vertex names):\n#   [1] 1--2 2--3 2--4\n# \n# $`3`\n# + 3/10 edges from f3f6e64 (vertex names):\n#   [1] 1--3 2--3 3--5\n# \n# $`4`\n# + 4/10 edges from f3f6e64 (vertex names):\n#   [1] 2--4 4--5 4--6 4--7\n# \n# $`5`\n# + 3/10 edges from f3f6e64 (vertex names):\n#   [1] 3--5 4--5 5--6\n# \n# $`6`\n# + 3/10 edges from f3f6e64 (vertex names):\n#   [1] 4--6 5--6 6--7\n# \n# $`7`\n# + 2/10 edges from f3f6e64 (vertex names):\n#   [1] 4--7 6--7\n\nget.edgelist(g)\n# [,1] [,2]\n# [1,] \"1\"  \"2\" \n# [2,] \"1\"  \"3\" \n# [3,] \"2\"  \"3\" \n# [4,] \"2\"  \"4\" \n# [5,] \"3\"  \"5\" \n# [6,] \"4\"  \"5\" \n# [7,] \"4\"  \"6\" \n# [8,] \"4\"  \"7\" \n# [9,] \"5\"  \"6\" \n# [10,] \"6\"  \"7\" \nprint(g, e=TRUE, v=TRUE)\n# IGRAPH f673c51 UN-- 7 10 -- \n#   + attr: name (v/c)\n# + edges from f673c51 (vertex names):\n#   [1] 1--2 1--3 2--3 2--4 3--5 4--5 4--6 4--7 5--6 6--7\nget.adjacency(g)\n# 7 x 7 sparse Matrix of class \"dgCMatrix\"\n# 1 2 3 4 5 6 7\n# 1 . 1 1 . . . .\n# 2 1 . 1 1 . . .\n# 3 1 1 . . 1 . .\n# 4 . 1 . . 1 1 1\n# 5 . . 1 1 . 1 .\n# 6 . . . 1 1 . 1\n# 7 . . . 1 . 1 .\n```\n\n#### 1.2 有向图\n\n同样的方法，也可以用来创建有向图；\n\n``` r\ndg <- graph.formula(1-+2,1-+3,2++3)\nop <- par(mfrow=c(1,2))\nplot(g, vertex.size=10,layout=l, vertex.color=\"skyblue\")\nplot(dg,vertex.size=10,vertex.color=\"skyblue\")\n```\n\n<img src=\"/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-3-1.png\" style=\"display: block; margin: auto;\" />\n\n``` r\npar(op)\n```\n\n#### 1.3 从邻接矩阵导入图；\n\n我们选择一个神奇的数据Arecibo\\_message\\[<https://en.wikipedia.org/wiki/Arecibo_message>\\],\n来说明,有时候,信息所对应的矩阵，可能就是一张图片，而不是一个图。\n\n``` r\n###python command comes from\n###https://codegolf.stackexchange.com/questions/182924/output-the-arecibo-message\nmat <- reticulate::py_eval(\"''.join(bin(i)[3:]for i in b'`UP@JB`IDQKJjjd`@@@@@L@@Ah@@CP@@J`@@_@@@@@LNLLP@FPtXpu}}}|@@@@`@@`@@@A@@A~@@~@@@CCCcDA@DMCGM____@@@@HF@H@L@@PX@_`pO`A`@HA@HHF@`LLB@FHX@@s@@Xa`CC@`HD@``L@b@XAD@PDDA@PD@C@F@X@ck@A@P@BCx@DKi[@gI\\x7f\\\\NC\\\\@TGY@hOrAPXDFp@@@@@\\\\D@@zbjipAU@@B`@Gp@@\\x7fx@G@\\\\@X@LAh@lFXCLHhJHQHdPBJH@DHP@H@`@Dh@OOix')[1:]\")\nmat <- as.integer(unlist(strsplit(mat,split = \"\")))\nmat <- matrix(data = mat,nrow = 23,ncol = 73)\n\nexpand.matrix <- function(A){\n  m <- nrow(A)\n  n <- ncol(A)\n  B <- matrix(0,nrow = m, ncol = m)\n  C <- matrix(0,nrow = n, ncol = n)\n  cbind(rbind(B,t(A)),rbind(A,C))\n}\ng1 <- graph_from_adjacency_matrix(expand.matrix(mat),mode = \"undirected\")\nplot(g1,vertex.size=10,edge.width=2,layout=layout.circle,vertex.color=\"coral\")\n```\n\n<img src=\"/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-4-1.png\" style=\"display: block; margin: auto;\" />\n\n如果直接可视化这个图，我们什么也看不出来，然而，如果我们用将原数据视为栅格数据，那么，我们能看出这个数据的内涵是很丰富的\n\n``` r\ndat_long <- reshape2::melt(mat)\ndat_long$value <- as.factor(dat_long$value)\ncolnames(dat_long) <- c(\"V1\",\"V2\",\"value\")\n### plot\ngg <- ggplot(dat_long)+\n  geom_tile(aes(V1,V2,fill=value), color=\"#7f7f7f\")+\n  scale_fill_manual(values=c(\"black\", \"white\"))+\n  coord_equal()+\n  labs(x=NULL, y=NULL)+\n  scale_x_continuous(breaks = 1:6)+\n  scale_y_reverse(breaks=1:6)+\n  theme_bw()+\n  theme(panel.grid=element_blank())+\n  theme(panel.border=element_blank(),\n        axis.ticks=element_blank(),\n        axis.text = element_blank(),\n        legend.position = \"none\")\ngg\n```\n\n<img src=\"/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-5-1.png\" style=\"display: block; margin: auto;\" />\n\n#### 1.4 从data.frame中创建图\n\n需要两个输入，一个是边的信息，一个是节点的信息\n\n``` r\n## A simple example with a couple of actors\n## The typical case is that these tables are read in from files....\nactors <- data.frame(name=c(\"Alice\", \"Bob\", \"Cecil\", \"David\",\n                            \"Esmeralda\"),\n                     age=c(48,33,45,34,21),\n                     gender=c(\"F\",\"M\",\"F\",\"M\",\"F\"))\nrelations <- data.frame(from=c(\"Bob\", \"Cecil\", \"Cecil\", \"David\",\n                               \"David\", \"Esmeralda\"),\n                        to=c(\"Alice\", \"Bob\", \"Alice\", \"Alice\", \"Bob\", \"Alice\"),\n                        same.dept=c(FALSE,FALSE,TRUE,FALSE,FALSE,TRUE),\n                        friendship=c(4,5,5,2,1,1), advice=c(4,5,5,4,2,3))\ng <- graph_from_data_frame(relations, directed=TRUE, vertices=actors)\n\n\n## The opposite operation\nas_data_frame(g, what=\"vertices\")\n```\n\n    ##                name age gender\n    ## Alice         Alice  48      F\n    ## Bob             Bob  33      M\n    ## Cecil         Cecil  45      F\n    ## David         David  34      M\n    ## Esmeralda Esmeralda  21      F\n\n``` r\nas_data_frame(g, what=\"edges\")\n```\n\n    ##        from    to same.dept friendship advice\n    ## 1       Bob Alice     FALSE          4      4\n    ## 2     Cecil   Bob     FALSE          5      5\n    ## 3     Cecil Alice      TRUE          5      5\n    ## 4     David Alice     FALSE          2      4\n    ## 5     David   Bob     FALSE          1      2\n    ## 6 Esmeralda Alice      TRUE          1      3\n\n可视化，\n\n``` r\nplot(g,vertex.size=10,vertex.color=\"skyblue\")\n```\n\n<img src=\"/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-7-1.png\" style=\"display: block; margin: auto;\" />\n\n#### 1.5 用预定义的函数生成\n\n`igraph`里有很多带make的函数，是可以生成图的\n\n``` r\n# ls.str and lsf.str return an object of class \"ls_str\", basically the character vector of matching names (functions only for lsf.str), similarly to ls, with a print() method that calls str() on each object.\n###head(lsf.str(\"package:igraph\"))\ngrep(pattern = \"^make\",x=ls(\"package:igraph\"),value = T)\n```\n\n    ##  [1] \"make_\"                     \"make_bipartite_graph\"     \n    ##  [3] \"make_chordal_ring\"         \"make_clusters\"            \n    ##  [5] \"make_de_bruijn_graph\"      \"make_directed_graph\"      \n    ##  [7] \"make_ego_graph\"            \"make_empty_graph\"         \n    ##  [9] \"make_full_bipartite_graph\" \"make_full_citation_graph\" \n    ## [11] \"make_full_graph\"           \"make_graph\"               \n    ## [13] \"make_kautz_graph\"          \"make_lattice\"             \n    ## [15] \"make_line_graph\"           \"make_ring\"                \n    ## [17] \"make_star\"                 \"make_tree\"                \n    ## [19] \"make_undirected_graph\"\n\n我们展示其中的一些图：\n\n``` r\ng1 <- make_tree(10, 2)\ng2 <- make_bipartite_graph( rep(0:1,length=10), c(1:10))\ng3 <- make_star(10, mode = \"out\")\ng4 <- make_star(10, mode = \"in\")\nop <- par(mfrow=c(2,2))\nplot(g1,vertex.size=20,vertex.color=\"skyblue\")\nplot(g2,vertex.size=20,vertex.color=\"skyblue\")\nplot(g3,vertex.size=20,vertex.color=\"skyblue\")\nplot(g4,vertex.size=20,vertex.color=\"skyblue\")\n```\n\n<img src=\"/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-9-1.png\" style=\"display: block; margin: auto;\" />\n\n``` r\npar(op)\n```\n\n### 2. 基本操作\n\n诱导子图\n\n``` r\ng <- graph.formula(1-2,1-3,2-3,2-4,3-5,4-5,4-6,4-7,5-6,6-7)\nh <- induced.subgraph(g,1:5)\nprint(h)\n```\n\n    ## IGRAPH d91ee38 UN-- 5 6 -- \n    ## + attr: name (v/c)\n    ## + edges from d91ee38 (vertex names):\n    ## [1] 1--2 1--3 2--3 2--4 3--5 4--5\n\nExclusion：\n\n``` r\nh <- g - vertices(c(6,7))\nprint(h)\n```\n\n    ## IGRAPH d923ec9 UN-- 5 6 -- \n    ## + attr: name (v/c)\n    ## + edges from d923ec9 (vertex names):\n    ## [1] 1--2 1--3 2--3 2--4 3--5 4--5\n\nInclusion:\n\n``` r\nh <- h + vertices(c(6,7))\ng <- h + edges(c(4,6),c(4,7),c(5,6),c(6,7))\nprint(g)\n```\n\n    ## IGRAPH d928f5d UN-- 7 10 -- \n    ## + attr: name (v/c)\n    ## + edges from d928f5d (vertex names):\n    ##  [1] 1--2 1--3 2--3 2--4 3--5 4--5 4--6 4--7 5--6 6--7\n\nunion:\n\n``` r\nh1 <- h\nh2 <- graph.formula(4-6,4-7,5-6,6-7)\ng <- graph.union(h1,h2)\nprint(g)\n```\n\n    ## IGRAPH d92f82f UN-- 7 10 -- \n    ## + attr: name (v/c)\n    ## + edges from d92f82f (vertex names):\n    ##  [1] 6--7 5--6 4--7 4--6 4--5 3--5 2--4 2--3 1--3 1--2\n\n### 3. 查看/添加/修改 属性\n\n首先创建一个示例的图，\n\n``` r\n## A simple example with a couple of actors\n## The typical case is that these tables are read in from files....\nactors <- data.frame(name=c(\"Alice\", \"Bob\", \"Cecil\", \"David\",\n                            \"Esmeralda\"),\n                     age=c(48,33,45,34,21),\n                     gender=c(\"F\",\"M\",\"F\",\"M\",\"F\"))\nrelations <- data.frame(from=c(\"Bob\", \"Cecil\", \"Cecil\", \"David\",\n                               \"David\", \"Esmeralda\"),\n                        to=c(\"Alice\", \"Bob\", \"Alice\", \"Alice\", \"Bob\", \"Alice\"),\n                        same.dept=c(FALSE,FALSE,TRUE,FALSE,FALSE,TRUE),\n                        friendship=c(4,5,5,2,1,1), advice=c(4,5,5,4,2,3))\ng <- graph_from_data_frame(relations, directed=TRUE, vertices=actors)\n```\n\n我们可以通过`$`运算符来查看，添加，修改属性\n\n``` r\n###check edge attribute\nnames(edge_attr(g))\n###[1] \"same.dept\"  \"friendship\" \"advice\" \n###vertext\nnames(vertex_attr(g))\n###[1] \"name\"   \"age\"    \"gender\"\n###Vertex\n# list.vertex.attributes(g)\n# list.edge.attributes(g)\nV(g)$name\n###[1] \"Alice\"     \"Bob\"       \"Cecil\"     \"David\"     \"Esmeralda\"\nedge_attr(g)$same.dept\n###[1] FALSE FALSE  TRUE FALSE FALSE  TRUE\nedge_attr(g)$friendship\n###[1] 4 5 5 2 1 1\n```\n\n可视化如下：\n\n``` r\n## A simple example with a couple of actors\n## The typical case is that these tables are read in from files....\nactors <- data.frame(name=c(\"Alice\", \"Bob\", \"Cecil\", \"David\",\n                            \"Esmeralda\"),\n                     age=c(48,33,45,34,21),\n                     gender=c(\"F\",\"M\",\"F\",\"M\",\"F\"))\nrelations <- data.frame(from=c(\"Bob\", \"Cecil\", \"Cecil\", \"David\",\n                               \"David\", \"Esmeralda\"),\n                        to=c(\"Alice\", \"Bob\", \"Alice\", \"Alice\", \"Bob\", \"Alice\"),\n                        same.dept=c(FALSE,FALSE,TRUE,FALSE,FALSE,TRUE),\n                        friendship=c(4,5,5,2,1,1), advice=c(4,5,5,4,2,3))\ng <- graph_from_data_frame(relations, directed=TRUE, vertices=actors)\n\nV(g)$gender <- plyr::revalue(x=V(g)$gender,\n                            replace=c(\"F\"=\"Female\",\"M\"=\"Male\"))\nV(g)$gender\n```\n\n    ## [1] \"Female\" \"Male\"   \"Female\" \"Male\"   \"Female\"\n\n``` r\ng$name <- \"Toy Graph\"\nset.seed(42)\ntmp.df <- layout.graphopt(g)\nV(g)$color <- plyr::revalue(x=V(g)$gender,\n                            replace=c(\"Female\"=\"skyblue\",\n                                      \"Male\"=\"coral\"))\nplot(g,layout=tmp.df,vertex.size=20,\n     vertex.color=V(g)$color,main=\"Toy Graph\")\nlegend('right',legend=unique(V(g)$gender),pch=c(19,19),col = c(\"skyblue\",\"coral\"))\n```\n\n<img src=\"/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-16-1.png\" style=\"display: block; margin: auto;\" />\n\n``` r\nset.seed(42)\ntmp.df <- layout.graphopt(g)\ngg.net = ggnetwork(g,\n                   arrow.gap = 0.05, \n                   layout = tmp.df)\nggplot(gg.net, aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_edges(color = \"black\", \n               alpha = 0.5, curvature = 0,\n               arrow = arrow(length = unit(6, \"pt\"), \n                             type = \"closed\")) +\n    geom_nodes(aes(color = gender), size = 10) +\n  geom_nodetext(aes(label = name))+\n  scale_color_manual(values = c(\"skyblue\",\"coral\"))+\n    ggtitle(\"Toy Graph\")+\n    theme_blank()\n```\n\n<img src=\"/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-17-1.png\" style=\"display: block; margin: auto;\" />\n\n### 4. 更多关于图的概念和术语\n\n#### 4.1 概念\n\n下述概念不搬运书里的定义；忘记就查书。后面的章节会再用到这些概念，进行图的可视化与统计分析。\n\n  - multi-graph\n\n  - simple-graph:\n    可以用`is.simple()`判定，可以用`simplify()`将`multi-graph`转换为`simple-graph`.\n\n  - neighbors\n\n  - degree: The degree of a vertex v defined as the number of edges\n    incident on v;\n\n  - in-degree\n\n  - out-degree\n\n  - walk\n\n  - trails\n\n  - circuit & cylce;\n\n  - reachable\n\n  - graph connected\n\n  - component of a graph\n\n  - strong connected\n\n  - weak connected\n\n  - distance/geodesic distance\n\n  - diameter\n\n#### 4.2 一些特殊的图\n\n与第一节有重叠\n\n  - complet graph\n\n  - clique\n\n  - regular graph\n\n  - tree\n\n  - forest\n\n  - root\n\n  - ancestor\n\n  - descendant\n\n  - parents, children\n\n  - k-star\n\n  - dirrected acyclic graph(DAG)\n\n  - bipartite graph\n\n<!-- end list -->\n\n``` r\ng.bip <- graph.formula(actor1:actor2:actor3,\n                       movie1:movie2,\n                       actor1:actor2 - movie1,\n                       actor2:actor3 - movie2)\n\nV(graph = g.bip)$type <- grepl(pattern = \"^movie\",V(graph = g.bip)$name)\n\nV(g.bip)$category <- ifelse(V(graph = g.bip)$type,\"Movie\",\"Actor\")\nV(g.bip)$category\n```\n\n    ## [1] \"Actor\" \"Actor\" \"Actor\" \"Movie\" \"Movie\"\n\n``` r\ng <- g.bip\nset.seed(42)\n### using matrxi product to do layout rotate 3/2pi\ntmp.df <- layout.bipartite(g) %*% matrix(data = c(0,-1,1,0),nrow = 2)\n\ngg.net = ggnetwork(g,\n                   arrow.gap = 0.05, \n                   layout = tmp.df)\nhead(gg.net)\n```\n\n    ##   x   y   name  type category      xend      yend\n    ## 1 0 0.0 actor1 FALSE    Actor 0.9514929 0.2378732\n    ## 2 0 0.5 actor2 FALSE    Actor 0.9514929 0.2621268\n    ## 3 0 0.5 actor2 FALSE    Actor 0.9514929 0.7378732\n    ## 4 0 1.0 actor3 FALSE    Actor 0.9514929 0.7621268\n    ## 5 0 0.0 actor1 FALSE    Actor 0.0000000 0.0000000\n    ## 6 0 0.5 actor2 FALSE    Actor 0.0000000 0.5000000\n\n``` r\nggplot(gg.net, aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_edges(color = \"black\", \n               alpha = 0.5, curvature = 0\n               # ,arrow = arrow(length = unit(6, \"pt\"), \n               #               type = \"closed\")\n               ) +\n    geom_nodes(aes(color = category), size = 16) +\n  geom_nodetext(aes(label = name))+\n  scale_color_manual(values = c(\"skyblue\",\"coral\"))+\n    ggtitle(\"bipartite graph example\")+\n    theme_blank()\n```\n\n<img src=\"/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-18-1.png\" style=\"display: block; margin: auto;\" />\n\n`igraph`自带的例子：\n\n``` r\n# Random bipartite graph\ninc <- matrix(sample(0:1, 50, replace = TRUE, prob=c(2,1)), 10, 5)\ng <- graph_from_incidence_matrix(inc)\nplot(g, layout = layout_as_bipartite,vertex.size=20,\n     vertex.color=c(\"skyblue\",\"coral\")[V(g)$type+1])\n```\n\n<img src=\"/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-19-1.png\" style=\"display: block; margin: auto;\" />\n\n### 附录：R配色\n\n基本颜色：\n\n``` r\n#### code provided by\n####http://bc.bojanorama.pl/2013/04/r-color-reference-sheet/\nm <- matrix(1:660, 60, 11)\nkol <- colors()[m]\n#op <- par(mar=c(.1, .1, 2, .1))\nimage(1:11, 1:60, t(m), col=kol, axes=FALSE, ann=FALSE)\ntxtcol <- ifelse( apply(col2rgb(kol), 2, mean) < 70, \"white\", \"black\")\ntext( as.numeric(col(m)), as.numeric(row(m)), kol, cex=.8, col=txtcol)\nmtext(\"grDevices::colors\", 3, cex=2)\n```\n\n<img src=\"/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-20-1.png\" style=\"display: block; margin: auto;\" />\n\n调色版\n\n``` r\nRColorBrewer::display.brewer.all()\nmtext(\"RColorBrewer\", 3, cex=2)\n```\n\n![](/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-21-1.png)<!-- -->\n\n渐变色\n\n``` r\nlibrary(RColorBrewer)\nlibrary(colorRamps)\nlibrary(viridis)\n### manu\nrdylbu <- colorRampPalette(rev(brewer.pal(n = 11, name =\"RdYlBu\")))\nrdbu <- colorRampPalette(rev(brewer.pal(n = 11, name =\"RdBu\")))\nnavy <- colorRampPalette(c(\"navy\", \"white\", \"firebrick3\"))\njet.colors <-\n  colorRampPalette(c(\"#00007F\", \"blue\", \"#007FFF\", \"cyan\",\n                     \"#7FFF7F\", \"yellow\", \"#FF7F00\", \"red\", \"#7F0000\"))\ncold <- colorRampPalette(c('#f7fcf0','#41b6c4','#253494','#081d58','#081d58'))\nwarm <- colorRampPalette(c('#ffffb2','#fecc5c','#e31a1c','#800026','#800026'))\nwarmcold <- colorRampPalette(c(rev(cold(21)), warm(20)))\n\n\n### add manu with package function\n\nN <- 100 # ramp length\nfunnames <- rev(c(\"manu::rdylbu\",\"manu::rdbu\",\"manu::navy\",\"manu::jet.colors\",\"manu::warmcold\",\n              \"viridis::viridis\",\n              \"grDevices::rainbow\", \"grDevices::heat.colors\",\n              \"grDevices::terrain.colors\", \"grDevices::topo.colors\",\n              \"grDevices::cm.colors\", \n              \"colorRamps::blue2red\",\n              \"colorRamps::blue2green\", \"colorRamps::green2red\",\n              \"colorRamps::blue2yellow\", \"colorRamps::cyan2yellow\",\n              \"colorRamps::magenta2green\", \"colorRamps::matlab.like\",\n              \"colorRamps::matlab.like2\", \"colorRamps::primary.colors\",\n              \"colorRamps::ygobb\"))\nspl <- strsplit(funnames, \"::\")\npkgs <- sapply(spl, \"[\", 1)\nfuns <- sapply(spl, \"[\", 2)\nkolmat <- sapply(funs, do.call, list(N))\nmat <- matrix( seq(1, length(kolmat)), nrow(kolmat), ncol(kolmat))\n\n\nimage(seq(1, nrow(mat)), seq(1, ncol(mat)), mat, col=kolmat,\n      axes=FALSE, ann=FALSE)\ntext( nrow(mat)/2, seq(1, ncol(mat)), funnames)\nmtext(\"Color Ramps function\", 3, cex=2)\n```\n\n![](/figure/posts/Learnigraph-1-ManuNetworkDataBasic_files/figure-gfm/unnamed-chunk-22-1.png)<!-- -->\n","tags":["Graph","Network"],"categories":["implementation"]},{"title":"Combine pheatmap","url":"/2020/04/20/combine_pheatmap/","content":"\nTalk is cheap, this is code:\n\n``` r\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(pheatmap)\nlibrary(ggplot2)\nlibrary(colormap)\nitems=names(colormaps)\nplot_list=list()\nfor (a in items[1:8]){\n  x= pheatmap(volcano,\n              cluster_rows = F,\n              cluster_cols = F,\n              main = a,\n              height = 3,\n              width = 3,\n              border_color = NA,\n              color = colormap_pal(colormap = colormaps[[a]])(100),silent = T)\n  plot_list[[a]] = x[[4]]     ##to save each plot into a list. note the [[4]]\n}\n\ncowplot::plot_grid(plotlist = plot_list[1:8],ncol = 2,nrow = 4)\n```\n\n<img src=\"/figure/posts/combine_pheatmap_files/figure-markdown_github/unnamed-chunk-1-1.png\" style=\"display: block; margin: auto;\" />\n\ntest equation: $E=mc^2$; $P(AB)=P(A)P(B)$\n","tags":["R"],"categories":["implementation"]},{"title":"pheatmap_advanced","url":"/2020/04/20/pheatmap_advanced/","content":"\n\n\n### Case 1\n\nThe datasets were provided by\n[data-to-viz](https://www.data-to-viz.com/story/SevCatOneNumNestedOneObsPerGroup.html)\n\n``` r\nlibrary(tidyverse)\nlibrary(pheatmap)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(kableExtra)\n### dataset 1\ndata <- read.table(\"https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/13_AdjacencyDirectedWeighted.csv\", header=TRUE)\n# show data\ndata %>% head(3) %>% select(1:3) %>% kable() %>%\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\n```\n\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n\n<thead>\n\n<tr>\n\n<th style=\"text-align:left;\">\n\n</th>\n\n<th style=\"text-align:right;\">\n\nAfrica\n\n</th>\n\n<th style=\"text-align:right;\">\n\nEast.Asia\n\n</th>\n\n<th style=\"text-align:right;\">\n\nEurope\n\n</th>\n\n</tr>\n\n</thead>\n\n<tbody>\n\n<tr>\n\n<td style=\"text-align:left;\">\n\nAfrica\n\n</td>\n\n<td style=\"text-align:right;\">\n\n3.142471\n\n</td>\n\n<td style=\"text-align:right;\">\n\n0.000000\n\n</td>\n\n<td style=\"text-align:right;\">\n\n2.107883\n\n</td>\n\n</tr>\n\n<tr>\n\n<td style=\"text-align:left;\">\n\nEast Asia\n\n</td>\n\n<td style=\"text-align:right;\">\n\n0.000000\n\n</td>\n\n<td style=\"text-align:right;\">\n\n1.630997\n\n</td>\n\n<td style=\"text-align:right;\">\n\n0.601265\n\n</td>\n\n</tr>\n\n<tr>\n\n<td style=\"text-align:left;\">\n\nEurope\n\n</td>\n\n<td style=\"text-align:right;\">\n\n0.000000\n\n</td>\n\n<td style=\"text-align:right;\">\n\n0.000000\n\n</td>\n\n<td style=\"text-align:right;\">\n\n2.401476\n\n</td>\n\n</tr>\n\n</tbody>\n\n</table>\n\n```r\n### the following function were embeded in pheatmap source code\nscale_rows = function(x){\n    m = apply(x, 1, mean, na.rm = T)\n    s = apply(x, 1, sd, na.rm = T)\n    return((x - m) / s)\n}\n\nscale_mat = function(mat, scale){\n    if(!(scale %in% c(\"none\", \"row\", \"column\"))){\n        stop(\"scale argument shoud take values: 'none', 'row' or 'column'\")\n    }\n    mat = switch(scale, none = mat, row = scale_rows(mat), column = t(scale_rows(t(mat))))\n    return(mat)\n}\n\ngenerate_breaks = function(x, n, center = F){\n    if(center){\n        m = max(abs(c(min(x, na.rm = T), max(x, na.rm = T))))\n        res = seq(-m, m, length.out = n + 1)\n    }\n    else{\n        res = seq(min(x, na.rm = T), max(x, na.rm = T), length.out = n + 1)\n    }\n    \n    return(res)\n}\n\n\ndata.plot <- scale_mat(mat = data,scale = \"column\")\nbreaks <- generate_breaks(data.plot,n = 256,center = F)\n\npheatmap::pheatmap(mat = data.plot,\n                   cluster_cols = F,\n                   cluster_rows = F,\n                   scale = \"column\",border_color = \"white\",\n                   color = viridis(n = 256, alpha = 1, \n                                   begin = 0, end = 1, option = \"viridis\"),\n                   breaks = breaks)\n```\n\n\n<img src=\"/figure/posts/pheatmap_advanced_files/figure-gfm/unnamed-chunk-2-1.png\" style=\"display: block; margin: auto;\" />\n\n### case 2\n\nthe codes were adapted from\n[slowkow](https://slowkow.com/notes/pheatmap-tutorial/) Sort dendrogram\nis very important\n\n``` r\nset.seed(42)\nrandom_string <- function(n) {\n  substr(paste(sample(letters), collapse = \"\"), 1, n)\n}\n\nmat <- matrix(rgamma(1000, shape = 1) * 5, ncol = 50)\n\ncolnames(mat) <- paste(\n  rep(1:3, each = ncol(mat) / 3),\n  replicate(ncol(mat), random_string(5)),\n  sep = \"\"\n)\nrownames(mat) <- replicate(nrow(mat), random_string(3))\n\nmat %>% as.data.frame %>% head(3) %>% select(1:3) %>% kable() %>%\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\n```\n\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n\n<thead>\n\n<tr>\n\n<th style=\"text-align:left;\">\n\n</th>\n\n<th style=\"text-align:right;\">\n\n1jrqxa\n\n</th>\n\n<th style=\"text-align:right;\">\n\n1pskvw\n\n</th>\n\n<th style=\"text-align:right;\">\n\n1ojvwz\n\n</th>\n\n</tr>\n\n</thead>\n\n<tbody>\n\n<tr>\n\n<td style=\"text-align:left;\">\n\nabv\n\n</td>\n\n<td style=\"text-align:right;\">\n\n9.6964789\n\n</td>\n\n<td style=\"text-align:right;\">\n\n9.172811\n\n</td>\n\n<td style=\"text-align:right;\">\n\n2.827695\n\n</td>\n\n</tr>\n\n<tr>\n\n<td style=\"text-align:left;\">\n\nnft\n\n</td>\n\n<td style=\"text-align:right;\">\n\n0.9020955\n\n</td>\n\n<td style=\"text-align:right;\">\n\n15.575853\n\n</td>\n\n<td style=\"text-align:right;\">\n\n4.328376\n\n</td>\n\n</tr>\n\n<tr>\n\n<td style=\"text-align:left;\">\n\nxha\n\n</td>\n\n<td style=\"text-align:right;\">\n\n2.6721643\n\n</td>\n\n<td style=\"text-align:right;\">\n\n3.127039\n\n</td>\n\n<td style=\"text-align:right;\">\n\n1.765077\n\n</td>\n\n</tr>\n\n</tbody>\n\n</table>\n\nsplit data into 3 groups, and increase the values in group1\n\n``` r\ncol_groups <- substr(colnames(mat), 1, 1)\nmat[,col_groups == \"1\"] <- mat[,col_groups == \"1\"] * 5\n```\n\nmaking the heatmap\n\n``` r\n# install.packages(\"pheatmap\", \"RColorBrewer\", \"viridis\")\nlibrary(pheatmap)\nlibrary(RColorBrewer)\nlibrary(viridis)\n\n# Data frame with column annotations.\nmat_col <- data.frame(group = col_groups)\nrownames(mat_col) <- colnames(mat)\n\n# List with colors for each annotation.\nmat_colors <- list(group = brewer.pal(3, \"Set1\"))\nnames(mat_colors$group) <- unique(col_groups)\n\npheatmap(\n  mat               = mat,\n  color             = inferno(10),\n  border_color      = NA,\n  show_colnames     = FALSE,\n  show_rownames     = FALSE,\n  annotation_col    = mat_col,\n  annotation_colors = mat_colors,\n  drop_levels       = TRUE,\n  fontsize          = 14,\n  main              = \"Default Heatmap\"\n)\n```\n\n![](/figure/posts/pheatmap_advanced_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->\n\nThe default color breaks in pheatmap are uniformly distributed across\nthe range of the data.\n\nWe can see that values in group 1 are larger than values in groups 2 and\n3. However, we can’t distinguish different values within groups 2 and 3.\n\n``` r\n## ----uniform-color-breaks------------------------------------------------\n\nmat_breaks <- seq(min(mat), max(mat), length.out = 10)\n\ndat <- data.frame(values = as.numeric(mat))\n\n## ----uniform-color-breaks-detail, fig.height=2, echo=FALSE---------------\ndat_colors <- data.frame(\n  xmin = mat_breaks[1:(length(mat_breaks)-1)],\n  xmax = mat_breaks[2:length(mat_breaks)],\n  ymin = 0,\n  ymax = max(density(mat, bw = \"SJ\")$y),\n  fill = rev(inferno(length(mat_breaks) - 1)),\n  stringsAsFactors = FALSE\n)\nggplot() +\n  geom_rect(\n    data = dat_colors,\n    mapping = aes(\n      xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = fill\n    )\n  ) +\n  geom_density(\n    data = dat,\n    mapping = aes(values),\n    bw = \"SJ\", color = \"cyan\"\n  ) +\n  scale_fill_manual(values = dat_colors$fill) +\n  cowplot::theme_cowplot()+\n  theme(legend.position = \"none\") +\n  labs(title = \"Uniform breaks\")\n```\n\n<img src=\"/figure/posts/pheatmap_advanced_files/figure-gfm/unnamed-chunk-6-1.png\" style=\"display: block; margin: auto;\" />\n\nthere are 6 data points greater than or equal to 100 are represented\nwith 4 different colors.\n\n``` r\ndat2 <- as.data.frame(table(cut(\n  mat, mat_breaks\n)))\ndat2$fill <- inferno(nrow(dat2))\nggplot() +\n  geom_bar(\n    data = dat2,\n    mapping = aes(x = Var1, weight = Freq, fill = Var1),\n    color = \"black\", size = 0.1\n  ) +\n  coord_flip() +\n  scale_fill_manual(values = dat2$fill) +\n  cowplot::theme_cowplot()+\n  theme(legend.position = \"none\") +\n  labs(y = \"data points\", x = \"breaks\",\n       title = \"Number of data points per color\")\n```\n\n![](/figure/posts/pheatmap_advanced_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->\n\nIf we reposition the breaks at the quantiles of the data, then each\ncolor will represent an equal proportion of the data:\n\n``` r\nquantile_breaks <- function(xs, n = 10) {\n  breaks <- quantile(xs, probs = seq(0, 1, length.out = n))\n  breaks[!duplicated(breaks)]\n}\n\nmat_breaks <- quantile_breaks(mat, n = 11)\n```\n\nlets see\n\n``` r\ndat_colors <- data.frame(\n  xmin = mat_breaks[1:(length(mat_breaks)-1)],\n  xmax = mat_breaks[2:length(mat_breaks)],\n  ymin = 0,\n  ymax = max(density(mat, bw = \"SJ\")$y),\n  fill = rev(inferno(length(mat_breaks) - 1)),\n  stringsAsFactors = FALSE\n)\nggplot() +\n  geom_rect(\n    data = dat_colors,\n    mapping = aes(\n      xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = fill\n    )\n  ) +\n  geom_density(\n    data = dat,\n    mapping = aes(values),\n    bw = \"SJ\", color = \"cyan\"\n  ) +\n  scale_fill_manual(values = dat_colors$fill) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Quantile breaks\")\n```\n\n![](/figure/posts/pheatmap_advanced_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->\n\n``` r\ndat2 <- as.data.frame(table(cut(\n  mat, mat_breaks\n)))\ndat2$fill <- inferno(nrow(dat2))\nggplot() +\n  geom_bar(\n    data = dat2,\n    mapping = aes(x = Var1, weight = Freq, fill = Var1),\n    color = \"black\", size = 0.1\n  ) +\n  coord_flip() +\n  scale_fill_manual(values = dat2$fill) +\n  theme(legend.position = \"none\") +\n  labs(y = \"data points\", x = \"breaks\",\n       title = \"Number of data points per color\")\n```\n\n![](/figure/posts/pheatmap_advanced_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->\n\nWhen we use quantile breaks in the heatmap, we can clearly see that\ngroup 1 values are much larger than values in groups 2 and 3, and we can\nalso distinguish different values within groups 2 and 3:\n\n``` r\npheatmap(\n  mat               = mat,\n  color             = inferno(length(mat_breaks) - 1),\n  breaks            = mat_breaks,\n  border_color      = NA,\n  show_colnames     = FALSE,\n  show_rownames     = FALSE,\n  annotation_col    = mat_col,\n  annotation_colors = mat_colors,\n  drop_levels       = TRUE,\n  fontsize          = 14,\n  main              = \"Quantile Color Scale\"\n)\n```\n\n![](/figure/posts/pheatmap_advanced_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->\n\nWe can also transform data\n\n``` r\npheatmap(\n  mat               = log10(mat),\n  color             = inferno(10),\n  border_color      = NA,\n  show_colnames     = FALSE,\n  show_rownames     = FALSE,\n  annotation_col    = mat_col,\n  annotation_colors = mat_colors,\n  drop_levels       = TRUE,\n  fontsize          = 14,\n  main              = \"Log10 Transformed Values\"\n)\n```\n\n<img src=\"/figure/posts/pheatmap_advanced_files/figure-gfm/unnamed-chunk-12-1.png\" style=\"display: block; margin: auto;\" />\n\nsort dendrograms\n\n``` r\nlibrary(dendsort)\n\nmat_cluster_cols <- hclust(dist(t(mat)))\n\n\nsort_hclust <- function(...) as.hclust(dendsort(as.dendrogram(...)))\n\nmat_cluster_cols <- sort_hclust(mat_cluster_cols)\nplot(mat_cluster_cols, main = \"Sorted Dendrogram\", xlab = \"\", sub = \"\")\n```\n\n<img src=\"/figure/posts/pheatmap_advanced_files/figure-gfm/unnamed-chunk-13-1.png\" style=\"display: block; margin: auto;\" />\n\nsort Dendrogram heatmap\n\n``` r\nmat_cluster_rows <- sort_hclust(hclust(dist(mat)))\npheatmap(\n  mat               = mat,\n  color             = inferno(length(mat_breaks) - 1),\n  breaks            = mat_breaks,\n  border_color      = NA,\n  cluster_cols      = mat_cluster_cols,\n  cluster_rows      = mat_cluster_rows,\n  show_colnames     = FALSE,\n  show_rownames     = FALSE,\n  annotation_col    = mat_col,\n  annotation_colors = mat_colors,\n  drop_levels       = TRUE,\n  fontsize          = 14,\n  main              = \"Sorted Dendrograms\"\n)\n```\n\n<img src=\"/figure/posts/pheatmap_advanced_files/figure-gfm/unnamed-chunk-14-1.png\" style=\"display: block; margin: auto;\" />\n\nchange colnames angle\n\n``` r\npheatmap(\n  mat               = mat,\n  color             = inferno(length(mat_breaks) - 1),\n  breaks            = mat_breaks,\n  border_color      = NA,\n  cluster_cols      = mat_cluster_cols,\n  cluster_rows      = mat_cluster_rows,\n  show_colnames     = TRUE,\n  show_rownames     = FALSE,\n  annotation_col    = mat_col,\n  angle_col = 90,\n  fontsize_col  = 8,\n  annotation_colors = mat_colors,\n  drop_levels       = TRUE,\n  fontsize          = 10,\n  main              = \"Sorted Dendrograms\"\n)\n```\n\n<img src=\"/figure/posts/pheatmap_advanced_files/figure-gfm/unnamed-chunk-15-1.png\" style=\"display: block; margin: auto;\" />\n","tags":["R"],"categories":["implementation"]},{"title":"rmarkdown-test","url":"/2020/04/20/rmarkdown-test/","content":"\nThis post test blogdown, reference this\n[repo](https://github.com/yihui/blogdown-hexo) This post generate by\n*blogdown::new\\_post(title = “Rmarkdown\\_test”,ext=“.Rmd”)*\n\n# R Markdown\n\nThis is an R Markdown document. Please note this page was **not**\nrendered using the [**rmarkdown**](http://rmarkdown.rstudio.com) package\nor [Pandoc](http://pandoc.org). The R Markdown document is compiled to\nMarkdown through **knitr**, and the Markdown document is rendered to\nHTML through [Hexo’s Markdown\nrenderer](https://github.com/hexojs/hexo-renderer-marked).\n\nYou can embed an R code chunk like this:\n\n``` r\nsummary(cars)\n##      speed           dist       \n##  Min.   : 4.0   Min.   :  2.00  \n##  1st Qu.:12.0   1st Qu.: 26.00  \n##  Median :15.0   Median : 36.00  \n##  Mean   :15.4   Mean   : 42.98  \n##  3rd Qu.:19.0   3rd Qu.: 56.00  \n##  Max.   :25.0   Max.   :120.00\nfit <- lm(dist ~ speed, data = cars)\nfit\n## \n## Call:\n## lm(formula = dist ~ speed, data = cars)\n## \n## Coefficients:\n## (Intercept)        speed  \n##     -17.579        3.932\n```\n\n# Including Plots\n\nYou can also embed R plots:\n\n``` r\npar(mar = c(0, 1, 0, 1))\npie(\n  c(280, 60, 20),\n  c('Sky', 'Sunny side of pyramid', 'Shady side of pyramid'),\n  col = c('#0292D8', '#F7EA39', '#C4B632'),\n  init.angle = -50, border = NA\n)\n```\n\n![](/figure/posts/rmarkdown-test_files/figure-gfm/pie-1.png)<!-- -->\n","tags":["R"],"categories":["implementation"]},{"title":"阿里数学竞赛预赛2020年的一道概率题学习","url":"/2020/04/18/阿里数学竞赛预赛2020年的一道概率题学习/","content":"\n阿里巴巴全球数学竞赛是阿里办的一项数学竞赛。出题范围和往届预选赛题目可以在其[官网](https://damo.alibaba.com/alibaba-global-mathematics-competition?lang=zh)下查看 。\n\n2020年的预选赛，有道题目是这样的：\n\n> 考虑一个由从左道右的n个小方格组成的$1\\times n$的区域，从左到右依次在每个小方格种一棵树，一共种$n$棵。树的种类只有两种：胡杨和樟子松。假设在第一个小方格种植的数是胡杨的概率是r。后续的种树的规则为：如果前一个小方格种的是胡杨，则本格种胡杨的概率为$s$;如果前一个小方格种的是樟子松，则本格种樟子松的概率为$t,0<r,s,t<1$\n> \n> (a) 假设$r=1/3,s+t\\ne1$。是否存在$s,t$使得$\\forall i,2 \\le i \\le n$,在第$i$个小方格种植的树是胡杨的概率都等于一个跟$i$无关的常数？如果存在，请给出$s，t$满足的关系；如果不存在，请说明理由。\n> \n> (b) 假设$r=\\frac{1}{3},s=\\frac{3}{4},t=\\frac{4}{5}$。假设我们观察到第2019个小方格里种植的树是胡杨，但我们观察不到其它小方格里种植的是哪种树。请问第一个小方格里种植的树是胡杨的概率是多少？\n\n这道题考察的其实是马尔科夫链相关的知识，第一问是说什么条件下，题目给定的马尔可夫链在第二步就能达到平稳分布；第二问是从第n步逆推最起始的概率。当然，直接的工具是条件概率和全概率公式。\n\n解答（根据官方答案，有改动）：\n首先，我们需要将文字信息转换为便于处理的数学记号，记“E”表示胡杨，“S” 表示樟子松。令$X_k$表示种在第$k$个方格的树的种类(根据题意，这是一个随机变量)，令$p_k=P(X_k = E)$,则由题设，有：\n$$ p_1 = r \\tag{1} $$\n且由全概率公式\n$$\\begin{aligned}\n  p_k &= P(X_k = E) \\\\\n  &= P(X_k=E|X_{k-1}=E)P(X_{k-1}=E) \\\\\n  & + P(X_k=E|X_{k-1}=S)P(X_{k-1}=S) \\\\\n  &= sp_{k-1}+(1-t)(1-p_{k-1}) \\\\\n  &= (1-t)+(s+t-1)p_{k-1},\\forall k\\ge2 \n  \\end{aligned}$$\n令$k=2$, 我们有：\n\n$$\\begin{aligned}\n    p_2 &=(1-t)+(s+t-1)p_{1} \\\\\n    &=(1-t)+(s+t-1)r \\\\\n    &=\\frac{3+2-2t}{3} \n  \\end{aligned}$$\n若$\\forall k \\ge 2,p_k=p_2$成立，则,\n\n\n\n$$ p_k = \\frac{1-t}{2-s-t},\\forall k \\ge 2 $$\n\n整理可得\n\n$$(s-2t+1)(s+t-1)=0 $$\n\n因为$s+t\\ne1$，所以当$s-2t+1=0$时，$\\forall k \\ge 2, p_k = p_2$\n\n(b) 题目给的$r,s,t$不满足(a)中的条件，所以我们需要求出一般条件下的情况。\n\n令$q_k = P(X_k=E|X_1 = E)$,则需要求的概率是：\n\n$$\n  P(X_1=E|X_n=E)=\\frac{P(X_n=E|X_1=E)}{P(X_n=E)}=\\frac{rq_n}{p_n},n=2019 \n$$\n\n(a)中已经求出了$p_k$得递推式, 仿照(a)的步骤，我们可以求出\n\n$$\n q_k=\\begin{cases}\n   1 , & k = 1 \\\\\n   (1-t)+(s+t-1)q_{k-1}, & k \\ge 2 \n \\end{cases}\n$$\n\n解上述递推式，可得：\n\n$$\n  \\frac{q_n}{(s+t-1)}=\\frac{(1-t)[\\frac{1}{s+t-1}-\\frac{1}{(s+t-1)^n}]}{s+t-2}+\\frac{1}{s+t-1}\n$$\n\n类似的，由(2)可得\n$$\n  \\frac{p_n}{(s+t-1)}=\\frac{(1-t)[\\frac{1}{s+t-1}-\\frac{1}{(s+t-1)^n}]}{s+t-2}+\\frac{r}{s+t-1}\n$$\n\n所以有：\n$$\n   \\begin{aligned}\n    P(X_1=E|X_n=E) &= \\frac{rq_n}{p_n} \\\\\n    &=r\\frac{\\frac{(1-t)[\\frac{1}{s+t-1}-\\frac{1}{(s+t-1)^n}]}{s+t-2}+\\frac{1}{s+t-1}}{ \\frac{(1-t)[\\frac{1}{s+t-1}-\\frac{1}{(s+t-1)^n}]}{s+t-2}+\\frac{r}{s+t-1}},n=2019\n  \\end{aligned}\n$$\n\n将给入条件带入(其实不用计算，因为$n=2019,q_n \\approx p_n$），可得$P(X_1=E|X_n=E)\\approx r =  1/3$\n\n","tags":["note","stochastic Process","Probability"],"categories":["math"]},{"title":"机器学习在生物学有应用吗","url":"/2020/04/16/机器学习在生物学有应用吗/","content":"\n说明：转自站长[知乎回答](https://www.zhihu.com/question/41428117/answer/1156065522)。\n\n当然有应用，而且是很广泛的应用，周志华老师的[《机器学习》](《机器学习》 \"周志华\")中的第1章的绪论的1.6节应用现状中这样写到：  \n\n> 机器学习还为许多交叉学科提供了重要的技术支撑。例如，“生物信息学”试图利用信息技术来研究生命现象和规律，而基因组计划的实施和基因药物的美好前景让人们为之心潮澎湃。生物信息学研究涉及从“生命现象”到“规律发现”的整个过程，其间必然包括数据获取、数据管理、数据分析、仿真实验等环节，而“数据分析”恰是机器学习技术的舞台，各种机器学习技术已经在这个舞台上大放异彩。\n\n在本回答中，我们将结合具体的案例，分三部分论述机器学习（包含深度学习）在生物研究的应用。第一部分，我们先对机器学习在生命科学领域的研究做一个全景的介绍。第二部分，我们再结合具体案例如何应用机器学习推动相关生物研究，以及相关生物研究中出现的问题如何催生新的机器学习算法。第三部分我们将进行回顾和反思，探讨未来的机器学习将如何更好的推动生物研究。\n\n在正式讨论之前，我们借用周志华老师的《机器学习》一书来对机器学习下一个描述性的定义：\n\n> 机器学习正是这样一门学科，它致力于研究如何通过计算的手段，利用经\n验来玫善系统自身的性能在计算机系统中，\"经验\"通常以\"数据\"形式存\n在，因此机器学习所研究的主要内容，是关于在计算机上从数据中产生\"模\n型\" (model) 的算法，即\"学习算法\" (learning algorithm). 有了学习算法，我们把经验数据提供给它，它就能基于这些数据产生模型;在面对新的情况时(例如看到一个没剖开的西瓜)，模型会给我们提供相应的判断(例如好瓜) .如果说\n计算机科学是研究关于\"算法\"的学问，那么类似的，可以说机器学习是研究\n关于\"学习算法\"的学问.\n\n在下面的论述中，我们将从概况以及具体的生物场景看到这个定义还是很合理的。\n\n本回答假定读者已经了解过一些机器学习和生物的概念。 \n\n### 一. 机器学习在生物研究中的应用概览\n\n#### 1. 基本流程\n\n一般来说，在生物研究中，一项应用机器学习中的算法的研究可以分为如下五步流程：\n1. 设计实验，收集数据\n2. 数据清洗\n3. 特征选择\n4. 模型构建\n5. 模型评估\n\n如下面的流程图，来自[Deep learning for computational biology](https://www.embopress.org/doi/10.15252/msb.20156651 \"Deep learning for computational biology\")所示\n\n![](https://imgkr.cn-bj.ufileos.com/288cd49c-6d39-414d-991c-cd62172f19d6.png)\n\n\n#### 2. 有监督学习与无监督学习\n\n模型构建的方法，按照研究的问题可以分为，有监督和无监督的。\n有监督学习是指**一类针对有标签的数据来预测无标签数据的标签的算法**，如果我们把连续数值变量也视为标签的话，那么回归也是有监督学习。而无监督学习是指**一类针对无标签的数据进行规律发现的算法**。除此之外，也有半监督学习，即在 \n\n一个典型的有监督的问题是分类问题，一个典型的无监督问题是聚类问题。在这个回答我们将介绍这两类问题的具体的场景。下图来自综述[Deep learning for computational biology]()\n\n![](https://imgkr.cn-bj.ufileos.com/a8545fe7-dfa9-4fd3-8516-8f8ad51ded9a.png)\n\n#### 3. 三类基本数据\n\n大多数生物研究主要对序列数据，矩阵或者张量数据，成像数据这三类基本的数据上进行机器学习算法的应用。\n\n##### 3.1 序列数据\n\n最基本的生物数据之一，通常为DNA序列，RNA序列，蛋白质序列。\n在人类基因组计划早期的问题是，如何快速进行基因组注释，该问题可以表示如下,图片来自[Machine learning applications in genetics and genomics](https://www.nature.com/articles/nrg3920 \"Machine learning applications in genetics and genomics\")：\n\n![](https://imgkr.cn-bj.ufileos.com/a401af41-2bf4-43f3-bdaf-e453ed95e82b.png)\n\n基因组注释是一个有监督或者半监督的问题，因为一段序列是不是基因可以通过EST(表达序列标签)来判定，其他特征可以通过一些生化或者分子实验来标定，所以我们可以得到数据标签。\n\n此外，序列数据更为常见的是要分析一些分子演化的问题，例如最近大家关注的新冠病毒的分子演化。这方面的案例和相关讨论可见：[剑桥大学研究称新冠病毒分三个变种，A 类病毒为「爆发根源」，更多发现于美国和澳洲，这一结论靠谱吗？](https://www.zhihu.com/question/386740743 \"剑桥大学研究称新冠病毒分三个变种，A 类病毒为「爆发根源」，更多发现于美国和澳洲，这一结论靠谱吗？\")\n\n这是一个无监督的问题，例如，我们其实并不知道新冠病毒可以分为几个变种，我们需要在数据中看出它能分成几类，然后再通过其他证据证明这种分类是合理的。\n\n##### 3.2 矩阵数据\n\n芯片技术和后续的高通量测序技术带来了很多种矩阵数据,这类矩阵通常是对某类型生物特征（基因，蛋白，表观修饰，染色质互作）的丰度汇总而成的。最典型矩阵数据是基因表达谱，基因表达谱矩阵可以通过RNA-seq数据进行比对后的转录本定量产生，基本流程和常见分析策略如下（图片来自[Enter the Matrix: Factorization Uncovers Knowledge from Omics](https://linkinghub.elsevier.com/retrieve/pii/S0168-9525(18)30124-0 \"Enter the Matrix: Factorization Uncovers Knowledge from Omics\"))：\n\n![](https://imgkr.cn-bj.ufileos.com/bdd2cf7f-00d1-4735-90a9-45206a6d4f00.png)\n\n这类数据的分析通常是无监督或者半监督的，我们通常想通过矩阵数据去发现一些可用于诊断的分子marker。\n\n##### 3.3 成像数据\n\n从数据存储的本质上讲，成像数据还是矩阵数据（不过考虑到多通道图像的存在，称为张量数据更为贴切），但是内涵上是不同的，成像数据表达更多的是生物体内部空间位置（还有形状或者结构）的信息。例如，一张蛋白亚细胞定位的图像，可以反映某标记的感兴趣的蛋白质位于细胞中的什么位置，如果我们有很多这样的图片，明智的方法是先标记一部分数据，训练一个卷积神经网络，然后再对剩下的图片进行预测，如下图所示，： \n\n![图片来自综述[2]()](https://imgkr.cn-bj.ufileos.com/2b1dca0a-9369-4ea0-8712-bd0a650e751e.png)\n\n#### 4. 关于深度学习及其在生物研究中的应用\n\n深度学习到底是什么呢，按照Yann LeCun, Yoshua Bengio，Geoffrey Hinton三位专家合写的[综述](https://www.nature.com/articles/nature14539 \"Deep Learning\")的定义：\n> Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned For.\n\n拙译为：深度学习方法是一种基于多种层级进行表示的表示学习方法。其表示能力是通过组合简单的非线性的模块实现的。每一个小模块都可以把第一层的原始数据转换为更稍微抽像的特征。通过足够多的这样的转换进行组合，可以学习到非常复杂的函数(功能)。\n\n目前，在基因组学的不同层级，均有深度学习的应用案例：\n\n![](https://imgkr.cn-bj.ufileos.com/2436e54e-084f-4598-8c33-3fd44573caa2.png)\n\n图片来自[A primer on deep learning in genomics](https://www.nature.com/articles/s41588-018-0295-5 \"A primer on deep learning in genomics\"),想了解更多，请阅读这篇文章。\n\n#### 5. 常见不同机器算法的实现软件\n\n针对不同的学习算法，在R中的可用的机器学习包如下，图片来自[Machine learning for Big Data analytics in plants](https://www.sciencedirect.com/science/article/abs/pii/S1360138514002192 \"Machine learning for Big Data analytics in plants\"):\n\n![](https://imgkr.cn-bj.ufileos.com/4bb8fef8-d541-478f-8031-bc56bd1ce489.png)\n\npython上的常用相关软件包如下.图片来自[Best Python Libraries for Machine Learning and Deep Learning](https://towardsdatascience.com/best-python-libraries-for-machine-learning-and-deep-learning-b0bd40c7e8c \"Best Python Libraries for Machine Learning and Deep Learning\")：\n\n\n![Best Python Libraries for Machine Learning and Deep Learning](https://imgkr.cn-bj.ufileos.com/0b60d14c-3193-40f4-9d3d-da31e4511e61.png)\n\n\n### 二. 机器学习在生物研究中的应用案例\n\n#### 1. 基于机器学习的差异表达网络分析\n\n生物学家很感兴趣的一个问题是，不同条件下哪些基因表达会发生变化，这样他们可以深入研究其中的分子机制，进而找到一些可以找到一些增强或者减弱他们想要研究表型的靶点。\n\n常见的思路是做假定基因表达服从一个分布，然后根据这个假设构建统计量，计算统计显著性，设置cutoff来筛选发生差异表达的基因。\n\n但是这样做可能存在问题，例如cutoff为 $p < 0.05$,那些被判定为统计不显著的基因就真的和表型相关的差异表达基因吗？有无更好的替代方法？\n\n文献[Machine Learning–Based Differential Network Analysis: A Study of Stress-Responsive Transcriptomes in Arabidopsis](http://www.plantcell.org/content/26/2/520/tab-figures-data \"Machine Learning–Based Differential Network Analysis: A Study of Stress-Responsive Transcriptomes in Arabidopsis\")提供了一种思路。假定我们对于模式植物拟南芥响应各种胁迫条件感兴趣，我们可以用基于机器学习的策略对于之前的差异表达方法做出改进，分为如下步骤：\n\n1）数据收集，清洗以及正负样本构建：\n收集不同胁迫条件下的基因表达谱(基因芯片数据），进行预处理和标准化，收集之前报导过的和相关的基因作为正样本，将表达谱中不发生变化的基因作为负样本，剩下的基因的表达谱作为无标签样本；\n\n2）特征提取：\n通过共表达网络的策略从表达谱中提取特征。在构建共表达网络的之后，采用随机森林的方法把未标签的样本中的“noninformative” genes（不表达，持续表达，与胁迫无关的基因)过滤掉了，减少了共表达网络构建的无用信息。计算每个基因在共表达网络中的PageRank等统计量，作为特征；\n\n3）模型构建：\n根据2）中计算的特征，从分好的正负样本中，再次随机森林构建模型；\n\n4）模型评估\n和limma等方法比较；\n\n5）模型预测，并进行验证\n将训练好的模型应用于无标记的基因上，预测出和新的胁迫相关的基因，并通过TDNA插入实验验证。\n\n上述步骤可以概括如下，\n![](https://imgkr.cn-bj.ufileos.com/d6922ac4-dcca-4705-ad3c-9092da5c7813.png)\n\n图片来自[Machine learning for Big Data analytics in plants]()\n\n#### 2. 干细胞分化路径重构与流形学习\n\n\n案例1是有监督学习的例子，我们接下来看无监督学习的案例。\n\n生物学有一个很著名的模型叫做waddington landscape，该模型描述了干细胞在分化过程可以类比于一个有质量的小球自发沿着山坡从山顶滚下山谷的过程，不同的山底表示了细胞的终末分化状态，而不同的分支点的存在则是细胞命运决定的节点。这个运动的过程中，细胞的基因表达会发生变化，如果我们假定基因表达“相近”的细胞在路径上也挨得很近，那么在基因表达的高维数据中应该嵌入了低维的分化路径，则我们能通过流形学习的技术从基因表达数据中重构出分化的路径，如下图所示，\n\n![](https://imgkr.cn-bj.ufileos.com/80ff5e2b-bc24-4584-8e75-d7c71107e5d4.png)\n\n图片来自[Manifold learning-based methods for analyzing single-cell RNA-sequencing data](https://www.sciencedirect.com/science/article/pii/S2452310017301877 \"Manifold learning-based methods for analyzing single-cell RNA-sequencing data\")\n\n具体来说，流形学习是如何进行的呢？可以结合如下的案例进行理解。现在有两个变量组成的一个数据集，我们将其画在直角坐标系中，可以看出样本点中存在一个螺旋的趋势，也就是说这个二维数据集中似乎嵌入了一个一维流形。如何通过计算的方式将其找出来呢。直觉告诉我们，必须先计算每两个样本点之间距离。我们在样本点之间的距离之后呢，会发现这个距离里样本点的局部邻近关系和整体邻近关系混淆在了一起，这个时候，我们可以使用叫做核函数的技巧，将距离转换为邻近关系。得到局部的距离之后呢，我们把相邻的点连起来，这样便可以最终得到那个样本点中包含的螺旋的一维流形了。\n\n![](https://imgkr.cn-bj.ufileos.com/a55bbb0c-41f6-4fc2-bc73-f80aaa4e2f9b.png)\n\n图片来自[Manifold learning-based methods for analyzing single-cell RNA-sequencing data]()\n\n附注：粗浅的来说，所谓流形就是一个局部看起来像是欧几里得空间的拓扑空间。每个属于这个n维流形的点的邻域都可以与一个n维欧氏空间建立一一映射的关系。（更为严谨的定义请看拓扑学教材）。流形学习一般是用来学习高维数据内部的低维结构。最基础流形学习算法是PCA。\n\n\n以最近发表的一种同时实现生物高维数据可视化和路径推断的算法[PHATE](https://www.nature.com/articles/s41587-019-0336-3 \"Visualizing structure and transitions in high-dimensional biological data\")为例，该算法的流程如下，（图片来自原文献）：\n\n![](https://imgkr.cn-bj.ufileos.com/2ed9f281-dd11-4ff2-a524-1c6c1218d4d6.png)\n\n该算法的基本流程和其他的流形学习方法大致类似，但是他们的创新之处是引入了随机游走，计算扩散概率，以及最终讲欧式距离转化为信息距离来进行embeding。\n\n篇幅所限，我们不会在这里谈很多该算法的计算细节，感兴趣的读者可看知乎上中文的介绍：[Nat. Biotechnol | PHATE：高维生物数据的可视化方法\n](https://zhuanlan.zhihu.com/p/102456357 \"Nat. Biotechnol | PHATE：高维生物数据的可视化方法\")，或者直接阅读原始文献。\n\n\n#### 3.冷冻电镜中的图像处理\n\n这部分，笔者不是专家，只是为了拓展视野在里记录。\n\n基础知识推荐大家看下coursera上面的加州理工的冷冻电镜的课程，尤其是Tomography那一节。\n关于冷冻电镜的背景大家请看\n[为什么冷冻电镜 (Cryo-EM) 去年突然火了？是有什么技术突破吗？](https://daily.zhihu.com/story/4303110 \"为什么冷冻电镜 (Cryo-EM) 去年突然火了？是有什么技术突破吗？\") \n以及[什么是2015年最受科学界关注的新技术？](https://zhuanlan.zhihu.com/p/20555975 \"什么是2015年最受科学界关注的新技术？\")\n当然还有nature的[新闻稿](https://www.nature.com/news/the-revolution-will-not-be-crystallized-a-new-method-sweeps-through-structural-biology-1.18335 \"The revolution will not be crystallized: a new method sweeps through structural biology\")\n\n根据nature这篇新闻稿，冷冻电镜取得突破性进展主要要归功于两个人：Richard Henderson和Sjors Scheres还有他们所在的实验室：UK Medical Research Council Laboratory of Molecular Biology (LMB)。Richard Henderson和他的同事 Nigel Unwin 在1975年的一片文章（Molecular structure determination by electron microscopy of unstained crystalline specimens）中为冷冻电镜技术做出了奠基性的贡献。而新发展的直接电子探测器使得对大分子的高速动态成像成为可能。新技术带来的大数据使得Sjors Scheres有了在[方法学](https://www.sciencedirect.com/science/article/abs/pii/S0022283605001932?via%3Dihub \"Maximum-likelihood Multi-reference Refinement for Electron Microscopy Images\")和[软件](https://www.sciencedirect.com/science/article/pii/S1047847712002481 \"RELION: Implementation of a Bayesian approach to cryo-EM structure determination\")上的突破。\n\n那么，冷冻电镜带来的结构生物学的革命是如何实现的？答案是借用到机器学习的思想与方法的，如下面这张图所示：\n\n\n\n![](https://imgkr.cn-bj.ufileos.com/c046c016-c2bd-4c8f-ab90-e7db1021b824.png)\n\n\n\n（来自[How cryo-EM is revolutionizing structural biology](http://www.sciencedirect.com/science/article/pii/S096800041400187X \"How cryo-EM is revolutionizing structural biology\"))\n\n第一步，将要解析的蛋白分离纯化制样之后，用高速动态成像的记录蛋白的各种构象;\n第二步，处理图像数据，把取向相同的小颗粒re-align，借用贝叶斯的思想；从而将粗颗粒的模型精细化;\n第三步，如果是混样的情况，也可以利用分类或者聚类的方法，将混样中存在的不同结构的蛋白构像解析出来。\n\n第二步的基于贝叶斯的re-align和精细化可以概括如下：\n\n![](https://imgkr.cn-bj.ufileos.com/84bb8588-fb72-4f61-83f2-0c9b3758e6b0.png)\n\n\n（图片来自[A Bayesian View on Cryo-EM Structure Determination](https://www.sciencedirect.com/science/article/pii/S0022283611012290?via%3Dihub \"A Bayesian View on Cryo-EM Structure Determination\")）\n\n策略为通过傅里叶变换的方法用计算机重构出粗略的结构模型然后把这个粗略的结构模型与成千上万的成像数据比对，得到每个图像之间的相对位置。通过作者改进的机器学习中常用的贝叶斯方法，将粗略的结构模型调整为新的一个更精确的结构，如此迭代以精炼我们的模型，文章提到对于核糖体的结构的解析他们迭代了25次。这整个的过程就是所谓的取“平均”了，不过是基于机器学习的方法，结合先验的知识来取得“平均”和进行光滑，取得精细结构。\n\n这部分不是很懂，写的不好，欢迎成像和图像处理方面的专家指正。\n\n### 三. 回顾反思\n\n\n在上述论述中，我们介绍的机器学习在生物研究应用案例都只在问这样一类型问题：”某一生物现象是什么？“，不过对于人类社会发展而言更有直接意义的问题是，”认识这一生物现象可能的模式之后我们该怎么办“，问这类问题的人一般都是医生或者药企的科学家。当然，目前也有这方面的成熟流程可以参考:\n\n![](https://imgkr.cn-bj.ufileos.com/248d4059-519b-49f0-a7e7-e790b2e68403.png)\n\n（图片来自[Applications of machine learning in\ndrug discovery and development](https://www.nature.com/articles/s41573-019-0024-5 \"Applications of machine learning in drug discovery and development\")\n\n最近也有科学家用深度学习的方法，发现了新的抗生素：\n\n\n![](https://imgkr.cn-bj.ufileos.com/8cff3e43-91bd-4d9d-801b-f0db91db8e3e.png)\n\n感兴趣的读者可以看这篇文献。\n\n此外，个人理解，机器学习就是一种智能的数据挖掘技术，它依据先验的知识建立预测模型来识别大数据中的有用信息。所以只要有大数据和前期积累的先验知识，就有机器学习方法用武之地。\n\n说几句与题目无关的话，个人感觉其实这个题目也可以回答学生物的人多学点基础的数学和物理知识有用吗？我觉得是有用的，比如你想搞清楚冷冻电镜成像的原理，你必须懂点物理知识（干涉衍射之类的）还得懂点数学物理方法（如傅里叶变换与它的逆）。当然想要进行机器学习，当然得有统计学和数据的可视化方法的数学基础和计算机编程基础（Python或者R）了。学科之间其实是可以互通有无的，然而这点常常被目光短浅的一些人忽略了，希望关注这个问题的人可以能多从这个角度来学习，思考问题，解决问题。\n\n### 附：日志\n\n+ 2016.3 创建回答\n+ 2016.4.14 用周志华老师《机器学习》补充前言\n+ 2020.4.12 原回答因「违反知乎社区管理规定」被删除。\n+ 2020.4.13-15 按照知乎社区管理规定做出修订。重新提交。\n+ 2020.04.16 修改排版错误\n\n\n\n","tags":["note","machine learning"],"categories":["reference"]},{"title":"读关于增强子研究的现状与未来的一篇综述","url":"/2020/04/08/读关于增强子研究的现状与未来的一篇综述/","content":"\n1. 文章信息\n\n  题目：[Towards a comprehensive catalogue of validated and target- linked human enhancers](https://www.nature.com/articles/s41576-019-0209-0)\n\n2. 内容\n   \n   回顾了Enhancer biology的研究历史，概述了现有的研究技术，提出了target-linked的研究框架。\n\n文章最有意思的一个总结图如下:\n\n![](https://imgkr.cn-bj.ufileos.com/ed2ff9bd-7fd7-449b-bfd9-661b67c76f37.png)\n\n\n3. 总结与评述：\n\t\t\t\n  + 像Enhancer biology这样的分子机制，带有很强的各方面的异质性，与其寻求一个comprehensive 的理解，不如做透彻在某一种非常重要的疾病，例如Cancer中的调控作用？Enhancer+single cell，细胞内部调控与细胞间调控的研究都很重要。\n\t\n  + 里面关于ENCODE的各种组学技术，以及3D genome的技术和CRISPR-screen的优缺点介绍很好。\n\n\n","tags":["sc-seq","genomics","genetics","ENCODE","HiC","ChIP-seq","ATAC-seq"],"categories":["reference"]},{"title":"关于肿瘤的免疫治疗靶点","url":"/2020/04/08/关于肿瘤的免疫治疗靶点/","content":"\n今天读到一个很清楚的综述的翻译笔记，讲肿瘤免疫治疗靶点的，读者可以移步到\n[生物信息学专业需要什么样的生物知识？](https://mp.weixin.qq.com/s/VzxTUFGK6ZdrHPYHpPGTzA)阅读。\n\n[原文献](https://www.nature.com/articles/s41585-019-0226-y)里有张总结的图很不错：\n![](https://imgkr.cn-bj.ufileos.com/4544cdb9-7c57-4efb-974c-aed98bc9dab2.jpg)可以作为本文的总结。\n\n","tags":["note","biology","immunology"],"categories":["reference"]},{"title":"quote_20200405","url":"/2020/04/05/quote-20200405/","content":"\n> 时间就像一条河流，\n  在这我们顺流而下，\n  遇到现实，\n  需要决策，\n  但我们无法停留，也无法回避，\n  只能以最好的方式应付。——《原则》\n","tags":["note"],"categories":["others"]},{"title":"cxt_Chap6_Cytokines","url":"/2020/04/05/cxt-Chap6-Cytokines/","content":"\n如无特别说明，引用部分出自《医学免疫学》（第七版，曹雪涛主编，人卫社出版）第六章，细胞因子。\n\n### 1. 细胞因子的定义\n> 细胞因子是由免疫细胞及组织细胞分泌的在细胞间发挥相互调控作用的的一类小分子可溶性蛋白质，通过结合响应受体调节细胞生长分化和效应，调控免疫应答，在一定条件下也参与炎症等多种疾病的发生。\n\n>作用方式：自分泌方式，旁分泌方式，内分泌方式；\n\n>功能特点：多效性，重叠性，协同性，拮抗性；\n\n### 2. 细胞因子的种类\n>根据结构和功能可以分为如下六大类：\n- 白细胞介素(interleukin, IL)，IL1-IL38。\n- 集落刺激因子(colony-stimulating factor, CSF)，是指能够刺激多能造血干细胞和不同分化阶段的造血祖细胞分化和增殖的细胞因子。主要包括粒细胞-巨噬细胞集落刺激因子(GM-CSF), 巨噬细胞集落刺激因子（M-CSF)，红细胞生成素(EPO)，干细胞因子(SCF)和血小板生成素(TPO)等。分别诱导造血干细胞或祖细胞分化增殖为相应的细胞。\n- 干扰素（interferon, IFN), 因具有干扰病毒复制的功能而得名。IFN根据其结构特征及生物学活性可分为I型、II型和III型。I型IFN主要包括IFN-$\\alpha$、IFN-$\\beta$, 主要由病毒感染的细胞、pDC细胞等产生；II型IFN即IFN-$\\gamma$，主要由活化T细胞和NK细胞产生。III型IFN包括IFN-$\\lambda 1$(IL-29)，IFN-$\\lambda 2$(IL-28A)和IFN-$\\lambda 3$(IL-28B)，主要由DC细胞产生。IFN具有抗病毒、抗细胞增殖、抗肿瘤和免疫调节等作用。\n-  肿瘤坏死因子(tumor necrosis factor, TNF)家族。肿瘤坏死因子因最初被发现其能造成肿瘤组织坏死而得名，包括TNF-$\\alpha$和TNF-$\\beta$，前者主要由活化的单核/巨噬细胞产生，后者主要由活化的T细胞产生，又称淋巴毒素(lymphotoxin, LT)。TNF家族目前已经发现TRAIL(TNF related apoptosis-inducing ligand)、FasL、CD40L等30余种细胞因子。TNF家族成员在调节免疫应答、杀伤靶细胞和诱导细胞凋亡等过程中发挥重要作用。\n-  生长因子（growth factor，GF)泛指一类可促进相应细胞生长和分化的细胞因子。其种类较多，包括转化生长因子-$\\beta$(transforming growth factor-$\\beta$,TGF-$\\beta$)、血管内皮细胞生长因子(VEGF)、表皮生长因子(EGF)、成纤维细胞生长因子(FGF)、神经生长因子(NGF)、血小板生长因子(PDGF)等。\n-  趋化因子(chemokine) 是一类结构相似，分子量约8~12kD，具有趋化功能的细胞因子。几乎所有的趋化因子都含有由2对或一对保守的半胱氨酸残基(C)形成的分子内二硫化键。可以根据靠近氨基端的C的个数以及排列顺序将趋化因子分为四个家族：1）C亚家族：氨基酸端只有1个C，该分子内只有一个分子内二硫化键；2）CC亚家族：氨基端2个C相邻；3）CXC亚家族：氨基酸2个C被1个氨基酸残基隔开；4）CX3C亚家族：氨基端2个C被3个氨基酸残基隔开，羧基端跨细胞膜。\n已经发现的趋化因子有，CXCL1 ~ 16，CCL1 ~ 28，XCL1 ~ 2，CX3CL1.\n\n### 3. 细胞因子受体\n\n> 细胞因子受体可以根据其结构特点被分为如下六个家族：\n- I型细胞因子受体家族，也称为血细胞生辰素受体家族(hematopoietin receptor family)， 通过 JAK-STAT通路转导信号；\n- II型细胞因子受体家族，也称为干扰素受体家族(interferon receptor family)，也是通过JAK-STAT通路转导信号；\n- 肿瘤坏死因子受体家族(tumor necrosis factor family)，主要通过TRAF-NF-kB，TRAF-AP-1 通路转导信号；\n- 免疫球蛋白超家族受体(Ig superfamily receptor, IgSFR)，会结合集落刺激因子；\n- IL-17受体家族(IL-17 receptor family)，主要通过TRAF-NF-kB通路转导信号；\n- 趋化因子受体家族(chemokine receptor family) ，属于GPCR中的一员。\n\n### 4.remark\n\n可否从基因表达调控出发，来描述细胞因子的功能？\n\n2025.01.12 comment\n这些都是知识性的，或者字典里的内容，我觉得需要结合组学研究实践，才能变得有意义。","tags":["note","biology","immunology"],"categories":["genomics"]},{"title":"HCL papar","url":"/2020/04/05/HCL-papar/","content":"\n- 题目：Construction of human cell landscape at single-cell level\n\n- 科学问题：如何构建人类细胞图谱\n\n- 实验：用其自己开发的Microwell-seq，该研究充分利用Microwell-seq成本低廉，双细胞污染率低和细胞普适性广等优势，建立了70多万个单细胞的转录组数据库，鉴定了人体100余种细胞大类和800余种细胞亚类。基于该数据库，团队开发了scHCL单细胞比对系统用于人体细胞类型的识别，并搭建了人类细胞蓝图网站http://bis.zju.edu.cn/HCL/（国家基因库镜像https://db.cngb.org/HCL/）。\n\n- 分析：“tSNE+图聚类”（fig1b,fig1c,fig1d); EC,Epi, Stromal cell分泌Ligand的能力（Fig2c)主要是Adult的细胞（微环境？）；路径分析（Fig3)；调控分析（Fig4)\n\n- 讨论：图谱式工作；证明了概念的可行性；但是不如HCA那么激动人心？\n","tags":["note","scRNA-seq","biology"],"categories":["reference"]},{"title":"Dobrow-chap2","url":"/2020/04/05/Dobrow-chap2/","content":"对于 `Introduction to stochastic processes with R`一书的笔记\n\n> Let us finish the article and the whole book with a good example of dependent trials, which approximately can be considered as a simple chain.\n>                                            –Andrei Andreyevich Markov\n\nChap2: Markov Chains: First steps\n\n本章讲马尔可夫链\n\n### Introduction \n\n1. 引入的案例：\n   这一节，用一个类似大富翁的游戏来引入马尔可夫夫性。\n2. 马尔可夫链的形式化定义为\n    > Markov Chain\n    > Let $\\mathcal{S}$ be a discrete set. A Markov chain is a sequence of random variables $X_0,X_1,\\dots$ taking values in $\\mathcal{S}$ with the property that\n    > $$ P(X_{n+1}=j|X_0=x_0,\\dots,X_{n-1}=x_{n-1},X_n=i) \\\\ = P(X_{n+1}=j|X_n=i) \\tag{2.1} $$ \n    > for all $x_0,\\dots,x_{n-1},i,j\\in\\mathcal{S},n\\ge0$ The set $\\mathcal{S}$ is the state space of the Markov chain. 、\n\n3. $X_n=i$ 称为在时刻n到达状态i。\n   \n4. 时间齐性马尔可夫链：\n   $$ P(X_{n+1}=j|X_{n}=i)=P(X_1=j|X_0=i) \\tag{2.2}$$\n\n5. transition matrix:\n   \t1. n步转移矩阵计算（矩阵乘法）\n   \t2. 若干例子：\n   \t\t1) 收敛于一个各行相等的矩阵；\n   \t\t2) 不收敛，进入跳跃的状态；\n   \t\t3) 收敛于一个各行不相等的矩阵\n\n第五部分从直观上为下一章铺路。","tags":["note","stochastic Process"],"categories":["math"]},{"title":"Dobrow_chap1","url":"/2020/04/04/Dobrow-chap1/","content":"This is a note of the textbook `Introduction to stochastic processes with R`\n\n> We demand rigidly defined areas of doubt and uncertainty!\n>      –Douglas Adams, The Hitchhiker’s Guide to the Galaxy\n\n### Introduction and preview\n\n> We demand rigidly defined areas of doubt and uncertatinty  –Douglas Adams, The Hitchhiker’s Guide to the Galaxy\n\n#### 1.1 DETERMINISTIC AND STOCHASTIC MODELS\n\n+ Consider a simple exponential growthmodel\nwhere the random arises？\nThe deterministic model does not address the uncertainty present in the reproduction\nrate of individual organisms.\n\n+ In many biological processes, the exponential distribution is a common choice for modeling the times of births and deaths.\n\n+ Ex1.1 PageRank: random walks on graphs\n\n+ Ex1.2 Spread of infectious disease\nSIR model: Susceptible-infected-removed\nReed-Frost model: Stochastic SIR model in discrete time.\n\n#### 1.2 What is a stochastic process\n\n+ The author said\n    > A stochastic process, also called a random process, is simply one in which outcomes are uncertain. By contrast, in a deterministic system there is no randomness. In a deterministic system, the same output is always produced from a given input.\n\n+ A stochastic process is specified by its index and state sapce, and by the dependency relationships among its random variables\n    > Stochastic process: A stochastic process is a collection of random variables $\\{X_t,t \\in I\\}$.The set I is the index set of the process. The random variables are defined on a commmon state space S.\n\n+ Ex 1.3 Monopoly\n+ EX 1.4 Discrete time, continous state space\n+ Ex 1.5 Continuous time, discrete state space\n: arrival process, Poisson process\n+ Ex 1.6 Random walk and gambler's ruin: Random walk, discrete-time stochastic process whose state space is $\\mathbb{Z}$\n+ Ex 1.7 Brownian motion: Brownian motion is a continuous-time, contiuous state space stochastic process\n\n#### 1.3 Monte Carlo Simulation\n\n+ Given a random experiment and event A, a Monte Carlo estimate of $P(A)$ is obtained by repeating random experiment many times and taking the proportion of trials in which A occurs as an approximation for $P(A)$\n\n+ Strong law of large numbers\n     \n     $$ \\lim_{n\\rightarrow\\infty}\\frac{X_1+\\dots+X_n}{n}=P(A), a.s. \\tag{1.1}$$\n\n#### 1.4 Conditional Probability\n\n> The simplest stochastic process is a sequence of i.i.d. random variables. Such\nsequences are often used to model random samples in statistics. However, most\nreal-world systems exhibit some type of dependency between variables, and an\nindependent sequence is often an unrealistic model.\n\n> Thus, the study of stochastic processes really begins with conditional\nprobability—conditional distributions and conditional expectation. These will\nbecome essential tools for all that follows.\n\n+ Conditional Probability: \n  \n  $P(A|B) = \\frac{P(A\\bigcap B)}{P(B)}$\n\n+ Law of Total probability:\n  \n  Let $B_1,\\dots,B_k$ be a sequence of events that partition the sample space. That is, the $B_i$ are mutually exclusive(disjoint) and their union is equal to $\\Omega$. Then, for many event A,\n  $$ P(A)=\\sum_{i=1}^{k}P(A\\bigcap B_i)=\\sum_{i=1}^{k}P(A|B_i)P(B_i) $$\n\n+ Ex1.8 Disease tests\n\n+ Ex1.9 Find the probability that it is a heart\n\n+ Ex1.10 Gambler's ruin\n  let $p_k$ denote the probability of reaching n when the gambler's fortune is k.\n  $$ p_k = p_{k+1}(\\frac{1}{2})+p_{k-1}(\\frac{1}{2}) $$\n  or \n  $$ p_{k+1}-p_k = p_k - p_{k-1},  k = 1,\\dots, n-1 \\tag{1.2}$$\n  \n  using $p_0 =0, p_n = 1$\n\n  we have $p_k=kp_1=\\frac{k}{n},k=0,\\dots,n$\n  \n  The gambler's ruin is \\frac{n-k}{n} \n\n+ Bayes Rule\n\n  Given a countable sequence of events $B_1,B_2,\\dots$ which partition the sample space, a more general form of Bayes' rule is\n\n  $$P(B_i|A)=\\frac{P(A|B_i)P(B_i)}{\\sum_jP(A|B_j)P(B_j)}$$ \n\n+ Ex 1.11 The probability that teh employee is in fact lying.\n\n+ Conditional Distribution\n  \n  joint density function:\n\n  $P(X \\le x, Y \\le y) = \\int_{-\\infty}^{x}\\int_{-\\infty}^{y}f(x,t)dtds$\n  \n  + Discrete Case:\n    \n    P(Y=y|X=x)=\\frac{X=x,Y=y}{P(X=x)}\n    \n  + Continuous Case:\n  \n    For continuous randomo variables X and Y, the conditional density function of Y given X=x is \n\n    $f_{Y|X}(y|x)=\\frac{f(x,y)}{f_X(x)}$\n\n    $P(Y \\in R | X = x)=\\int_Rf_{Y|X}(y|x)dy$\n\n#### 1.5 Conditional Expectation of Y given X=x\n\n+ Conditional Expectation of Y given X=x\n\n    $$E(Y|X=x)= \\begin{cases}\\sum_{y}yP(Y=y|X=x),&\\text{discrete} \\\\\n    \\int_{-\\infty}^{\\infty}yf_{Y|X}(y|x)dy, & \\text{continuous}   \n    \\end{cases} $$","tags":["note","stochastic Process"],"categories":["math"]},{"title":"几个重要假设检验的推导","url":"/2018/08/01/几个重要假设检验的推导/","content":"\n## 前言\n\n假设检验是我们在日常研究中，经常碰到的统计问题。对于追求实用与效率的科研人员来说，各种不同的假设检验是可以用软件，点点鼠标，或者写写代码，就可以完成的。\n\n不过，对于我们这些想要在生物信息领域深入和进阶，并且最终有所建树的学生来说，我们光会拧螺丝和用板子，用轮子是不够的，当有新的技术，新的需求出来之后，我们得要造新轮子，开发新方法。因此，我们还是得学学火箭是咋飞起来和板子以及轮子是咋造出来的知识。\n\n我们以学习和介绍研究中比较基础的$\\chi^2,t,F$三种检验的所对应的分布推导，开始我们的进阶之旅。\n\n说明：本文的推导来自《概率统计讲义》第三版附录二，陈家鼎等编著，高等教育出版社出版。略微有所修改，阅读本文，只需修过本科阶段非数学专业的三门基础数学课：高等数学(不是很深，也不是很浅的数学分析)，线性代数，概率论与数理统计。\n## 正交矩阵与正态分布\n\n在线性代数课程中，我们知道，若$n$阶方阵$A=(a_{ij})_{n\\times n}$满足$A^TA=I$，写成标量的形式就是：\n\n$$\n\\sum_{k=1}^{n} a_{ki}a_{kj} = \\begin{cases}  \n1 & i=j\\\\\n0 & i\\ne j\n\\end{cases} \\quad (1)\n$$      \n\n此时，我们称方阵$A$为正交矩阵。而且，通过线性代数的课程，我们知道，正交矩阵满足如下性质：\n + 1-1 设A是正交矩阵，则$AA^T=I$，并且结合（1）可得：\n\n   $$\n   \\sum_{k=1}^{n} a_{kj} a_{lj} = \\begin{cases} 1 & k=l\\\\\n   0 & k\\ne l\n   \\end{cases} \\quad (2)\n   $$\n \n + 1-2 设A是正交矩阵，则$A^T$也是正交矩阵，并且$|A|=1$或$|A|=-1$，其中$|\\cdot|$表示行列式。\n\n + 1-3 若$A=(a_{ij})_{n\\times n}$是正交矩阵，而$x_1,x_2,\\dots,x_n$是任意n个实数，对于\n\n   $$ y_i = \\sum_{i=1}^{n} a_{ik}x_k  \\quad i=1,2,\\dots,n $$ \n   我们有\n   $$ \\sum_{i=1}^{n}y^2_i = \\sum_{i=1}^{n} x_i^2 \\quad (3)$$ \n\n很抱歉，开头罗列了这么多线性代数的事实，不过，也没办法，要做菜，我们得先备料不是吗。下面我们开始做菜了。\n\n**定理1** 设$X_1,X_2,\\dots,X_n$相互独立，且都服从$N(0,\\sigma^2)$，又$A=(a_{ij})$是正交矩阵，构造随机变量\n$$ Y_i = \\sum_{i=1}^{n}a_{ij}X_j \\quad (1\\le i\\le n) $$\n**证明** 因$X_i$的分布密度是$\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2\\sigma^2}x^2}$,且$X_i$是独立同分布样本（i.i.d.），故$X_1,X_2,\\dots,X_n$联合密度为：$(\\frac{1}{\\sqrt{2\\pi}\\sigma})^ne^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}x^2}$\n\n构造n维空间中的区域D:\n$$ D=\\{(x_1,x_2,\\dots,x_n)|a_i < \\sum_{j=1}^{n}a_{ij}x_j < b,i=1,2\\dots,n\\} $$\n则有：\n\n$$\n\\begin{aligned}\n  & P\\{a_1 < Y_1 < b_1,a_2 < Y_2 < b_2, \\dots, a_n < Y_n < b_n \\} \\\\\n  & =P\\{ (X_1,X_2,\\dots,X_n)\\in D \\}\\\\\n  & = \\int_{D}(\\frac{1}{\\sqrt{2\\pi}\\sigma})^ne^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}x^2}dx_1dx_2\\dots dx_n\n\\end{aligned}\n$$\n\n注意到\n$$ y_i = \\sum_{k=1}^{n}a_{ik}x_k \\quad (i=1,2,\\dots,n)$$\n于是（利用正交矩阵的性质）\n$$ x_i = \\sum_{k=1}^{n}a_{ki}y_k \\quad (i=1,2,\\dots,n)$$\n容易验证，变换的雅可比式为\n$$ J(\\frac{x_1,x_2,\\dots,x_n}{y_1,y_2,\\dots,y_n})= |A^T|=1,-1 $$\n\n又$\\sum_{k=1}^{n}x^2_{k}=\\sum_{k=1}^n y_{k}^2$故\n\n$$\n\\begin{aligned}\n  &\\int_D(\\frac{1}{\\sqrt{2\\pi}\\sigma})^ne^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}x^2}dx_1dx_2\\dots dx_n \\\\\n  &= \\int_{a_1}^{b_1}\\int_{a_2}^{b_2}\\dots\\int_{a_n}^{b_n}(\\frac{1}{\\sqrt{2\\pi}\\sigma})^ne^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}y^2} \\cdot |J(\\frac{x_1,x_2,\\dots,x_n}{y_1,y_2,\\dots,y_n})| dy_1dy_2\\dots dy_n  \\\\\n  &=\\int_{a_1}^{b_1}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{-y_1^2}{2\\sigma^2}}dy_1\\cdot\\int_{a_2}^{b_2}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{-y_2^2}{2\\sigma^2}}dy_2 \\cdots \\int_{a_n}^{b_n}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{-y_n^2}{2\\sigma^2}}dy_n \\\\\n  &=  P\\{a_1 < Y_1 < b_1\\} \\cdot P\\{a_2 < Y_2 < b_2\\} \\cdots P\\{a_n < Y_n < b_n\\}\n\\end{aligned}\n$$\n故$Y_1,Y_2,\\dots,Y_n$相互独立，且不难看出，都服从$N(0,\\sigma^2)$。定理1证毕。\n\n**定理2**设$X_1,X_2,\\dots,X_n$相互独立，且$X \\sim N(\\mu,\\sigma^2)$。$A=(a_{ij})$是n阶正交矩阵,构造随机变量，\n$$ Y_{i}=\\sum_{k=1}^{n}a_{ik}X_k \\quad (i=1,2,\\dots,n) $$\n则$Y_1,Y_2,\\dots,Y_n$相互独立，且\n$$ Y_{i} \\sim N(\\sum_{k=1}^{n}a_{ik}\\mu_k,\\sigma^2) $$\n**证明**令$Z_i = X_i - \\mu$，则$Z_1,Z_2,\\dots,Z_n$相互独立，都服从$N(0,\\sigma^2)$,根据定理1知，$\\sum_{k=1}^{n}a_{1k}Z_k,\\sum_{k=1}^{n}a_{2k}Z_k,\\dots,\\sum_{k=1}^{n}a_{nk}Z_k$相互独立。\n且\n$$ \\sum_{k=1}^{n}a_{ik}Z_k \\sim N(0,\\sigma^2)$$\n但是\n$$ Y_i = \\sum_{k=1}^{n}a_{ik}Z_k + \\sum_{k=1}^{n}a_{ik}\\mu_k $$\n故$Y_1,Y_2,\\dots,Y_n$相互独立，且$Y_i \\sim N(\\sum_{k=1}^{n}a_{ik}\\mu_k,\\sigma^2) \\quad  (i=1,2,\\dots,n)$\n\n## 关于$\\chi^2$分布\n\n前面的的都是小菜，接下来上主菜。我们要开始证明一系列很fancy的定理\n\n**定理3** 设$X_1,X_2,\\dots,X_n$相互独立，并且都服从$N(0,1)$,则$\\xi=\\sum_{i=1}^n X^{2}_{i}$服从$n$个自由度的$\\chi^2$分布，其PDF(probability density function)为\n$$ p(u) = k_n(u)= \\begin{cases}  \n\\frac{1}{2^{\\frac{n}{2}}\\Gamma(\\frac{n}{2})}u^{\\frac{n}{2}-1}e^{-\\frac{u}{2}} & u>0\\\\\n0 & u\\le 0 \n\\end{cases} \\quad (4)$$\n**证明** 我们证明的策略是，先求出CDF(cumulative distribution function)$F(u)=P\\{\\xi \\le u\\}$，然后利用中值定理，证明$F^\\prime(u)=k_n(u)$。\n\n显然，当$u\\le 0,F(u)=0,F^\\prime(u)=k_n(u)$\n\n当$u>0$时，由于$X_1,X_2,\\dots,X_n$相互独立，故$X_1,X_2,\\dots,X_n$联合密度为$(\\frac{1}{\\sqrt{2\\pi}})^ne^{-\\frac{1}{2}\\sum_{i=1}^{n}x^2}$，\n故\n$$ \n\\begin{aligned}\n F(u) &= P\\{ \\sum_{i=1}^{n}X_{i}^2 \\le u \\} \\\\\n      &=  \\int_{\\sum_{i=1}^{n}x_{i}^2 \\le u}(\\frac{1}{\\sqrt{2\\pi}})^ne^{-\\frac{1}{2}\\sum_{i=1}^{n}x^2}dx_1dx_2\\dots dx_n\n\\end{aligned}  \n$$\n故对于$h>0$,有\n$$ \n\\begin{aligned}\n F(u+h) - F(u)  =& \\int_{u < \\sum_{i=1}^{n}x_{i}^2 \\le u+h}(\\frac{1}{\\sqrt{2\\pi}})^ne^{-\\frac{1}{2}\\sum_{i=1}^{n}x^2}dx_1dx_2\\dots dx_n \\\\\n \\le& \\int_{u < \\sum_{i=1}^{n}x_{i}^2 \\le u+h}(\\frac{1}{\\sqrt{2\\pi}})^ne^{-\\frac{u}{2}} dx_1dx_2\\dots dx_n \\\\\n =& (\\frac{1}{\\sqrt{2\\pi}})^n e^{-\\frac{u}{2}} \\int_{u < \\sum_{i=1}^{n}x_{i}^2 \\le u+h} dx_1dx_2\\dots dx_n \n\\end{aligned}  \n$$\n$$ \n\\begin{aligned}\n F(u+h) - F(u)\n\\ge& \\int_{u < \\sum_{i=1}^{n}x_{i}^2 \\le u+h}(\\frac{1}{\\sqrt{2\\pi}})^ne^{-\\frac{u+h}{2}} dx_1dx_2\\dots dx_n \\\\\n=& (\\frac{1}{\\sqrt{2\\pi}})^n e^{-\\frac{u+h}{2}} \\int_{u < \\sum_{i=1}^{n}x_{i}^2 \\le u+h} dx_1dx_2\\dots dx_n\n\\end{aligned}  \n$$\n令$S(x) = \\int_{\\sum_{i=1}^{n}x_{i}^2 \\le x} dx_1dx_2\\dots dx_n \\quad (x>0)$\n则\n$$ \n(\\frac{1}{\\sqrt{2\\pi}})^n e^{-\\frac{u+h}{2}}\\cdot\\frac{S(u+h)-S(u)}{h} \\\\\n\\le \\frac{F(u+h)-F(u)}{h} \\le \\\\\n(\\frac{1}{\\sqrt{2\\pi}})^n e^{-\\frac{u}{2}}\\cdot\\frac{S(u+h)-S(u)}{h}\n $$\n问题现在变为如何求$S(x)$\n\n做代换$x_i= y_i \\sqrt{x}$,则\n$$ dx_i = \\sqrt{x} dy_i $$\n由此 \n$$ S(x) = \\int_{\\sum_{i=1}^{n}y_{i}^2 \\le 1} (\\sqrt{x})^n dy_1dy_2\\dots dy_n = x^{\\frac{n}{2}}\\cdot C_n $$\n有趣的是，我们可以看出\n$$ C_n = \\int_{\\sum_{i=1}^{n}y_{i}^2 \\le 1}  dy_1dy_2\\dots dy_n $$ \n是$n$维单位球体的体积。不过在我们的问题中，我们可以看出它只和$n$有关的量。故$S(x)=\\frac{n}{2}C_nx^{\\frac{n}{2}-1}$\n\n根据之前的不等式，结合中值定理：\n$$ \\lim_{h\\rightarrow0^{+}} \\frac{F(u+h)-F(u)}{h} = (\\frac{1}{\\sqrt{2\\pi}})^nC_n\\frac{n}{2}u^{\\frac{n}{2}-1}e^{-\\frac{n}{2}} $$\n$$ \\lim_{h\\rightarrow0^{-}} \\frac{F(u+h)-F(u)}{h} = (\\frac{1}{\\sqrt{2\\pi}})^nC_n\\frac{n}{2}u^{\\frac{n}{2}-1}e^{-\\frac{n}{2}} $$\n所以\n$$ F^{\\prime}(u) = B_n u^{\\frac{n}{2}-1}e^{-\\frac{u}{2}} $$\n综上\n$$ p(u) = k_n(u)= \\begin{cases}  \nB_nu^{\\frac{n}{2}-1}e^{-\\frac{u}{2}} & u>0\\\\\n0 & u\\le 0 \n\\end{cases} \\quad $$\n由归一化条件$\\int_{-\\infty}^{+\\infty}p(u)du=1$知$\\int_{0}^{+\\infty}B_nu^{\\frac{n}{2}-1}e^{-\\frac{u}{2}}du=1$\n而在数学分析的知识告诉我们\n$\\int_{0}^{+\\infty}u^{\\frac{n}{2}-1}e^{-\\frac{u}{2}}du=2^{\\frac{n}{2}}\\Gamma(\\frac{n}{2})$\n\n$$ B_n = \\frac{1}{2^{\\frac{n}{2}}\\Gamma(\\frac{n}{2})} $$\n定理得证。\n\n这个定理的一个副产物是，告诉了我们$n$维单位球体的体积$C_n=\\frac{\\pi^\\frac{n}{2}}{\\Gamma(\\frac{n}{2}+1)}$\n\n**推论** 若$\\xi \\sim \\chi^2(n)$，则有$E(\\xi)=n$\n\n**证明** 由定理1，结合数学期望的性质，知$E(\\xi)=E(\\sum_{i=1}^{n}X_i^2)=\\sum_{i=1}^{n}E(X_i^2)=\\sum_{i=1}^{n}(D(X_i)+(E(X_i))^2)=\\sum_{i=1}^{n}1=n$ \n\n$\\quad \\Box$\n\n**定理4** 若$\\xi$与$\\eta$相互独立，且$\\xi \\sim\\chi^2(n_1),\\eta \\sim\\chi^2(n_2)$，则$\\xi+\\eta \\sim \\chi^2(n_1+n_2)$\n\n**证明** 设$\\xi,\\eta，\\xi+\\eta$的分布函数分别为$p_1(x),p_2(x),p(x)$，我们先分别不加证明的引用概率论和Gamma函数的两个结论：\n\n1).已知(X,Y)的联合密度是$p(x,y)$，$Z=Y+Y$的PDF为：\n$$ \np_z(z)=\\int_{-\\infty}^{\\infty}p(x,z-x)dx\n $$\n2).$\\int_{0}^{1}v^{p-1}(1-v)^{q-1}dv=\\frac{\\Gamma(p)\\Gamma(q)}{\\Gamma(p+q)}$(p,q为正整数)\n\n下面开始证明：\n当 $x \\le 0$时，$P(\\xi+\\eta \\le 0)=0,p(x)=0$,定理成立。\n\n当 $x > 0$时，\n\n$$ \\begin{aligned}\np(x) &= \\int_{0}^{x}\\frac{1}{2^{\\frac{n_1}{2}}\\Gamma(\\frac{n_1}{2})}u^{\\frac{n_1}{2}-1}e^{-\\frac{u}{2}}\\frac{1}{2^{\\frac{n_2}{2}}\\Gamma(\\frac{n_2}{2})}(x-u)^{\\frac{n_2}{2}-1}e^{-\\frac{x-u}{2}}du \\\\\n&=\\frac{e^{-\\frac{x}{2}}x^{\\frac{n_1+n_2}{2}-1}}{2^{\\frac{n_1+n_2}{2}}\\Gamma(\\frac{n_1}{2})\\Gamma(\\frac{n_2}{2})}\\int_{0}^{1}v^{\\frac{n_1}{2}-1}(1-v)^{\\frac{n_2}{2}-1}dv （v=\\frac{u}{x}) \\\\\n&=\\frac{e^{-\\frac{x}{2}}x^{\\frac{n_1+n_2}{2}-1}}{2^{\\frac{n_1+n_2}{2}}\\Gamma(\\frac{n_1}{2})\\Gamma(\\frac{n_2}{2})}\\cdot\\frac{\\Gamma(\\frac{n_1}{2})\\Gamma(\\frac{n_2}{2})}{\\Gamma(\\frac{n_1+n_2}{2})} \\\\\n&=\\frac{1}{2^{\\frac{n_1+n_2}{2}}\\Gamma(\\frac{n_1}{2})\\Gamma(\\frac{n_2}{2})}x^{\\frac{n_1+n_2}{2}}e^{-\\frac{x}{2}}\n\\end{aligned} \n$$\n\n综上：\n\n$$ \np(x) = \\begin{cases}\n\\frac{1}{2^{\\frac{n_1+n_2}{2}}\\Gamma(\\frac{n_1}{2})\\Gamma(\\frac{n_2}{2})}x^{\\frac{n_1+n_2}{2}}e^{-\\frac{x}{2}} & x>0\\\\\n0 & x\\le 0\n\\end{cases}\n $$ \n $\\Box$\n\n **定理5** 若$x_1,x_2,\\dots,x_n$相互独立，且都服从分布$N(0,1)$,则有如下三条结论：\n \n 1. $\\bar{X}=\\frac{X_1+X_2+\\dots\n +X_n}{n}\\sim N(0,\\frac{1}{n})$ \n 2. $\\sum_{i=1}^{n}(X_i-\\bar{X}) \\sim \\chi^2(n-1)$\n 3. $\\bar{X}$与$\\sum_{i=1}^{n}(X_i-\\bar{X})$相互独立\n\n**证明** 构造正交矩阵\n\n$$ \n  \\begin{bmatrix}\n   \\frac{1}{\\sqrt{n}} & \\frac{1}{\\sqrt{n}} & \\frac{1}{\\sqrt{n}} & \\dots & \\frac{1}{\\sqrt{n}}\\\\\n   \\frac{1}{\\sqrt{1\\cdot 2}} & \\frac{-1}{\\sqrt{1\\cdot 2}} & 0 &\\dots & 0  \\\\\n   \\frac{1}{\\sqrt{2\\cdot 3}} & \\frac{1}{\\sqrt{2\\cdot 3}} & \\frac{-2}{\\sqrt{2\\cdot 3}} &\\dots & 0  \\\\\n   \\vdots & \\vdots & \\vdots & \\ddots & 0 \\\\\n   \\frac{1}{\\sqrt{(n-1)n}} & \\frac{1}{\\sqrt{(n-1)n}} & \\frac{1}{\\sqrt{(n-1)n}} &\\dots & \\frac{-(n-1)}{\\sqrt{(n-1)n}} \n  \\end{bmatrix} \n $$\n 由此正交矩阵，我们可以构造随机变量：\n\n $$ \n \\begin{aligned}\n   Y_1 &= \\frac{1}{\\sqrt{n}}(X_1+X_2+\\dots+X_n) \\\\\n   Y_2 &= \\frac{1}{\\sqrt{1\\cdot 2}}(X_1-X_2) \\\\\n   Y_3 &= \\frac{1}{\\sqrt{2\\cdot 3}}(X_1+X_2-2X_3) \\\\\n   \\dots \\\\\n    Y_n &= \\frac{1}{\\sqrt{(n-1)n}}(X_1+X_2+\\dots+X_{n}-(n-1)X_n)\n \\end{aligned}\n  $$\n  有定理1可知，$Y_1,Y_2,\\dots,Y_n$相互独立，且都服从$N(0,1)$，\n  我们发现$Y_1 \\sim N(0,1)$，因此$\\bar{X}=\\frac{1}{\\sqrt{n}}Y_1\\sim N(0,\\frac{1}{n})$，第一条结论得证。\n\n  由于$\\sum_{i=1}^{n}X_i^{2}=\\sum_{i=1}^{n}Y_i^{2}$故\n\n  $$ \n \\begin{aligned}\n   \\sum_{i=1}^{n}(X_i-\\bar{X}) &= \\sum_{i=1}^{n}X_i^2-n\\bar{X} \\\\\n    &= \\sum_{i=1}^{n}Y_i^2-n(\\frac{1}{\\sqrt{n}}Y_i)^2 \\\\\n    &= \\sum_{i=1}^{n}Y_i^2-Y^2_i \\\\\n    &= \\sum_{i=2}^{n}Y_i^2 \\sim \\chi^2(n-1)\n \\end{aligned}\n  $$\n  第二条结论得证。\n\n  由于$Y_1,Y_2,\\dots,Y_n$相互独立，且\n  $$ \n  \\bar{X}=\\frac{1}{\\sqrt{n}}Y_1, \\sum_{i=1}^{n}(X_i-\\bar{X})^2=\\sum_{i=2}^{n}Y^{2}_{i}\n   $$\n  故$\\bar{X}$与$\\sum_{i=1}^{n}(X_i-\\bar{X})^2$独立，第三条结论得证\n  $\\Box$\n\n  **推论** 若$x_1,x_2,\\dots,x_n$相互独立，且都服从分布$N(\\mu,\\sigma^2)$,则有如下三条结论：\n \n 1. $\\bar{X}=\\frac{X_1+X_2+\\dots\n +X_n}{n}\\sim N(\\mu,\\frac{\\sigma^2}{n})$ \n 2. $\\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(X_i-\\bar{X}) \\sim \\chi^2(n-1)$\n 3. $\\bar{X}$与$\\sum_{i=1}^{n}(X_i-\\bar{X})$相互独立\n\n## 关于t分布\n**定理6** 设$\\xi,\\eta$相互独立，且$\\xi \\sim N(0,1),\\eta \\sim \\chi^2(n)$， 则$\\zeta=\\frac{\\xi}{\\sqrt{\\frac{\\eta}{n}}} \\sim t(n)$，其PDF为：\n$$ \np(u)=t_n(u) = \\frac{\\Gamma(\\frac{n+1}{2})}{\\Gamma(\\frac{n}{2})\\sqrt{n\\pi}}(1+\\frac{u^2}{n})^{-\\frac{n+1}{2}} \\quad (5)\n $$\n**证明** 与定理3证明的思路类似，设$F(u) = P\\{\\zeta \\le u\\}$证明$F^{\\prime}(u)=t_n(u)$, 由已知：\n$$ \n\\begin{aligned}\n  F(u) \n  &= P\\{\\frac{\\xi}{\\sqrt{\\frac{\\eta}{n}}} \\le u\\} \\\\\n  &= P\\{\\frac{\\xi}{\\sqrt{\\eta}} \\le \\frac{u}{\\sqrt{n}}\\} \\\\\n  &= \\iint_{\\frac{x}{\\sqrt{y}}\\le\\frac{u}{\\sqrt{n}}} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\\cdot\\frac{1}{2^{\\frac{n}{2}}\\Gamma(\\frac{n}{2})}y^{\\frac{n}{2}-1}e^{-\\frac{y}{2}}dxdy \\\\\n  &= \\iint_{t \\le \\frac{u}{\\sqrt{n}},y > 0} \\frac{1}{\\sqrt{2\\pi}2^{\\frac{n}{2}}\\Gamma(\\frac{n}{2})}s^{\\frac{n}{2}-1}\\cdot e^{-\\frac{1}{2}(1+t^2)s}|J(\\frac{x,y}{s,t})|dsdt \\quad (t=\\frac{x}{\\sqrt{y}},y=s) \\\\\n  &= \\int_{-\\infty}^{\\frac{u}{\\sqrt{n}}} \\frac{dt}{\\sqrt{\\pi}\\Gamma(\\frac{n}{2})}\\int_{0}^{+\\infty}(\\frac{s}{2})^{\\frac{n-1}{2}}e^{-\\frac{s}{2}(1+t^2)}d(\\frac{s}{2}) \\\\\n  &= \\int_{-\\infty}^{\\frac{u}{\\sqrt{n}}} \\frac{1}{\\sqrt{\\pi}\\Gamma(\\frac{n}{2})} \\cdot \\frac{\\Gamma(\\frac{n+1}{2})}{(1+t^2)^{\\frac{n+1}{2}}}dt \\\\\n  &= \\int_{-\\infty}^{u} \\frac{1}{\\sqrt{n\\pi}\\Gamma(\\frac{n}{2})} \\cdot \\frac{\\Gamma(\\frac{n+1}{2})}{(1+\\frac{v^2}{n})^{\\frac{n+1}{2}}}dv \\quad (v=t\\sqrt{n})\n\\end{aligned}\n $$\n 故$F^{\\prime}(u)= t_n(u) \\quad \\Box$ \n \n 定理5,6可以用来证明下面这个在统计学里很有作用的定理：\n\n **定理7** 设$X_1,X_2,\\dots,X_n (n \\ge 2)$相互独立，且都服从$N(\\mu,\\sigma^2)$,则$T=\\frac{\\bar{X}-\\mu}{\\sqrt{\\frac{S^2}{n}}} \\sim t(n-1)$其中\n $$ \\bar{X}=\\frac{X_1+X_2+\\dots+X_n}{n}, S^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\bar{X})^2$$\n\n **证明** 构造随机变量\n $$ \n \\xi = \\frac{\\bar{X}-\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}},\\eta=\\frac{1}{\\sigma^2}\\cdot\\sum_{i=1}^{n}(X_i-\\bar{X})^2\n  $$\n 根据定理5的推论，我们知道$\\xi,\\eta$相互独立，且$\\xi\\sim N(0,1),\\eta\\sim \\chi^2(n-1)$\n 故根据定理6，$\\frac{\\xi}{\\sqrt{\\frac{\\eta}{n-1}}}\\sim t(n-1)$\n 故 \n $$ T=\\frac{\\bar{X}-\\mu}{\\sqrt{\\frac{S^2}{n}}}=\\frac{\\frac{\\bar{X}-\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}}{\\sqrt{\\frac{S^2}{\\sigma^2}}}=\\frac{\\xi}{\\sqrt{\\frac{\\eta}{n-1}}} \\sim t(n-1) \\quad \\Box$$\n\n ## 关于F分布\n\n **定理8** 设$\\xi,\\eta$相互独立，且$\\xi\\sim \\chi^2(n_1),\\eta\\sim \\chi^2(n_2)$ 则 $\\zeta=\\frac{\\frac{\\xi}{n_1}}{\\frac{\\eta}{n_2}} \\sim F(n_1,n_2)$ 其PDF为：\n $$ \n p(u) = f_{n_1,n_2}(u) = \\begin{cases}\n   \\frac{\\Gamma(\\frac{n_1+n_2}{2})}{\\Gamma(\\frac{n_1}{2})\\Gamma(\\frac{n_2}{2})}(\\frac{n_1}{n_2})^{\\frac{n_1}{2}}u^{\\frac{n_1}{2}-1}(1+\\frac{n_1}{n_2}u)^{-\\frac{n_1+n_2}{2}} & u > 0 \\\\\n   0 & u \\le 0\n \\end{cases}\n  $$\n**证明** 跟之前一样，令$F(u)=P\\{\\xi\\le u\\}$ ，证明$F^{\\prime}(u)=f_{n_1,n_2}(u)$ \n\n当 $u \\le 0, F(u)=0$,\n$$ \n\\begin{aligned}\n  F(u) \n  &= P\\{\\xi\\le u\\} \\\\\n  &= P\\{\\frac{\\frac{\\xi}{n_1}}{\\frac{\\eta}{n_2}}\\le u\\} \\\\\n  &= P\\{\\frac{\\xi}{\\eta}\\le \\frac{n_1}{n_2}u\\} \\\\\n  &= \\iint_{\\frac{x}{y}\\le\\frac{n_1}{n_2}u,x>0,y>0}\\frac{1}{2^{\\frac{n_1}{2}}\\Gamma(\\frac{n_1}{2})}x^{\\frac{n_1}{2}-1}e^{-\\frac{x}{2}}\\frac{1}{2^{\\frac{n_2}{2}}\\Gamma(\\frac{n_2}{2})}y^{\\frac{n_2}{2}-1}e^{-\\frac{y}{2}}dxdy \\\\\n  &= \\iint_{0 < t \\le \\frac{n_1}{n_2}u,s>0} \\frac{e^{-\\frac{s}{2}(1+t)}s^{\\frac{n_1+n_2}{2}-2}}{2^{\\frac{n_1+n_2}{2}}\\Gamma(\\frac{n_1}{2})\\Gamma(\\frac{n_2}{2})}t^{\\frac{n_1}{2}-1}\\cdot|J(\\frac{x,y}{s,t})|dsdt \\quad x=st,y=s \\\\\n  &= \\int_{0}^{\\frac{n_1}{n_2}u}\\frac{t^{\\frac{n_1}{2}-1}}{2^{\\frac{n_1+n_2}{2}}\\Gamma(\\frac{n_1}{2})\\Gamma(\\frac{n_2}{2})}dt\\int_{0}^{+\\infty}s^{\\frac{n_1+n_2}{2}-1}t^{\\frac{n_1}{2}-1}dsdt \\\\\n  &= \\int_{0}^{\\frac{n_1}{n_2}u}\\frac{\\Gamma(\\frac{n_1+n_2}{2})}{2^{\\frac{n_1+n_2}{2}}\\Gamma(\\frac{n_1}{2})\\Gamma(\\frac{n_2}{2})}t^{\\frac{n_1}{2}-1}(1+t)^{-\\frac{n_1+n_2}{2}}dt \\\\\n  &= \\int_{0}^{u}\\frac{\\Gamma(\\frac{n_1+n_2}{2})}{2^{\\frac{n_1+n_2}{2}}\\Gamma(\\frac{n_1}{2})\\Gamma(\\frac{n_2}{2})}(\\frac{n_1}{n_2})^{\\frac{n_1}{2}}v^{\\frac{n_1}{2}-1}(1+\\frac{n_1}{n_2}v)^{-\\frac{n_1+n_2}{2}}dv \\quad  (t=\\frac{n_1}{n_2}v)\n\\end{aligned}\n$$\n故$F^{\\prime}(u)=f_{n_1,n_2}(u)$  $\\quad \\Box$\n\n**定理9** 设$X_1,X_2,\\dots,X_{n_1},Y_1,Y_2,\\dots,Y_n$, 这$n_1+n_2$个随机变量相互独立，且都服从$N(\\mu,\\sigma^2)$,则\n$$ \\zeta=\\frac{\\frac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\bar{X})^2}{\\frac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\bar{Y})^2} \\sim F(n_1-1,n_2-1)$$ \n\n**证明** 构造随机变量\n\n$$\n\\xi=\\frac{1}{\\sigma^2}\\cdot\\sum_{i=1}^{n}(X_i-\\bar{Y})^2, \\eta=\\frac{1}{\\sigma^2}\\cdot\\sum_{i=1}^{n}(Y_i-\\bar{Y})^2 \n$$\n由之前的结论，我们知道$\\xi\\sim\\chi^2(n_1-1),\\eta\\sim\\chi^2(n_2-1)$, 接下来证明$\\xi,\\eta$的独立性，构造随机变量：\n$$ U_i = \\frac{X_i-\\mu}{\\sigma},V_i=\\frac{Y_i-\\mu}{\\sigma} \n$$\n则\n$$ \n\\xi=\\sum_{i=1}^{n_1}(U_i-\\bar{U}),\\eta=\\sum_{i=1}^{n_2}(V_i-\\bar{V})\n $$\n由已知$U_1,U_2,\\dots,U_{n_1},V_1,V_2,\\dots,V_{n_2}$ 相互独立，且都服从$N(0,1)$,于是其联合分布密度为\n$$ \np(u_1,u_2,\\dots,u_{n_1},v_1,v_2,\\dots,v_{n_2}) \\\\\n=(\\frac{1}{\\sqrt{2\\pi}})^{n_1+n_2}\\cdot e^{-\\frac{1}{2}(\\sum_{i=1}^{n_1}u^2_{i}+\\sum_{i=1}^{n_2}v^2_{i})}\n$$\n所以,对于任意的实数$a,b,c,d$\n$$ \n\\begin{aligned}\n  P\\{a< \\xi < b, c < \\eta < d \\}\n  =& \\int\\limits_{a<\\sum_{i=1}^{n_1}(u_i-\\bar{u})^2 < b,c < \\sum_{i=1}^{n_1}(v_i-\\bar{v})^2 < d  }p(u_1,u_2,\\dots,u_{n_1}v_1,v_2,\\dots,v_{n_2})du_1 \\dots du_{n_1}dv_1\\dots dv_{n_2} \\\\\n  =& \\int_{a<\\sum_{i=1}^{n_1}(u_i-\\bar{u})^2 < b}(\\frac{1}{\\sqrt{2\\pi}})^{n_1}e^{-\\frac{1}{2}\\sum_{i=1}^{n_1}u_i^2}du_1\\dots du_{n_1} \\cdot \\\\\n  &\\int_{a<\\sum_{i=1}^{n_1}(v_i-\\bar{v})^2 < b}(\\frac{1}{\\sqrt{2\\pi}})^{n_1}e^{-\\frac{1}{2}\\sum_{i=1}^{n_1}v_i^2}dv_1\\dots dv_{n_1} \\\\\n  =& P\\{a < \\xi < b\\} \\cdot P\\{c < \\eta < d\\} \n\\end{aligned}\n $$\n 独立性得证。\n\n 再结合定理8，\n \n $$\\zeta=\\frac{\\frac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\bar{X})^2}{\\frac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\bar{Y})^2} = \\frac{\\frac{\\xi}{n_1-1}}{\\frac{\\eta}{n_2-1}} \\sim F(n_1-1,n_2-1) \\quad \\Box\n $$\n\n\n\n\n","tags":["note","Probability"],"categories":["math"]},{"title":"Hello World","url":"/2018/07/29/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","tags":["hexo","NodeJS"],"categories":["implementation"]},{"title":"学习，研究，创新，分享","url":"/about/index.html","content":"\n#### 为何建站\n\n科研从兴趣出发，用良好的习惯坚持和成长。本站旨在记录成长点滴。\n\n#### 为何写作\n\n写作可以抵抗遗忘和衰朽，古人云：\n\n> 盖文章，经国之大业，不朽之盛事。年寿有时而尽，荣乐止乎其身，二者必至之常期，未若文章之无穷。\n                                                             ——曹丕《典论·论文》节选\n\n而在科研中，写作可以整理文献综述、材料方法，已经取得的数据结果。更重要的是理清下一步研究思路和技术路线。\n\n此外，勤练写作，可以提高研究者表达和推广自己观点和研究成果的能力。\n\n最终希望大家都能做学问或者学自己感兴趣的事物，最后能如韩愈作文一样，实现这种理想的境界：\n\n> 本深而末茂，形大而声宏，行峻而言厉，心醇而气和，昭晰者无疑，优游者有余。\n\n#### 涵盖主题\n\n本站最关注的是基因组学和生物信息学的各项研究以及相关的生物学，信息学，数学和统计学（将来可能还会有物理学和化学）的基础知识的学习。更多见[话题标签](https://landau1994.github.io/tags)\n\n#### 关于作者\n\n生物信息学博士。知乎主页[https://www.zhihu.com/people/landau1994]。"},{"title":"QA","url":"/help/index.html","content":"\n### Q: 这网站怎么弄的，还挺好看的啊？\n\n> A: 本站是静态博客，通过[hexo]()生成，主题为[jsimple](),做了个性化的定制（换了白天和晚上的图片，修改了配置支持数学公式）。\n\n### Q: 为什么博客有时候打开速度很慢，一直在加载，或者有时压根打不开呢？\n\n> A: 本站尚无独立域名，暂时使用[Github Page]()。所以github卡或者崩的时候就gg了。不过这都是小概率事件。开源万岁，github万岁,拥抱开源的微软万岁。\n\n### Q: 我也想搭个博客，不知道怎么弄？\n\n> A: 参考问题1，学会看官网教程，以及自己动手修改和调试配置文档。 \n\n\n### Q: 本站是否接受投稿或转载？\n\n> 站长本人之外的投稿和转载暂不支持。\n\n### Q: 其他的...\n\n> 本站为站长自己的学习记录，因个人知识水平所限，如有错误，还望[批评指正](https://github.com/Landau1994/landau1994.github.io/issues)。\n"},{"title":"tags","url":"/tags/index.html"},{"title":"links","url":"/links/index.html","content":"\n#### 以下摘录有趣、有意义、有影响力的链接\n\n持续更新中。\n\n> 此处不做网址导航，排序不分先后...\n\n`基本资源`篇\n\n- github;\n\n- google;\n\n- PubMed;\n\n- CNS的官网；\n\n- stackoverflow \n\n`一些站点`篇\n\n- [实用生物信息技术](http://abc.cbi.pku.edu.cn/)\n- [CBI forum](http://202.205.131.32/forum/)\n- [生信技能树](http://www.biotrainee.com/)\n- [生信坑](https://www.bioinfo.info/)\n\n`微信公众号`篇\n\n- biobabble"},{"title":"timeline","url":"/timeline/index.html","content":"\n暂时没有，可以看标签和归档。"}]