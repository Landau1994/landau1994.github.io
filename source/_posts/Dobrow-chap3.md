---
title: Dobrow-chap3
author: å¤ç›®æ²‰åŸ
avatar: /images/faceicon.png
authorLink: 'https://github.com/Landau1994'
authorAbout: 'https://github.com/Landau1994'
authorDesc: A PhD student in bioinformatics
mathjax: true
categories:
  - math
tags:
  - note
  - stochastic Process
description: Markov Chains for the Long term
date: 2020-05-06 22:15:53
keywords:
photos:
---


This is a note of the textbook `Introduction to stochastic processes with R`

> There exists everywhere a medium in things, determined by equilibrium.
>                                                  â€”Dmitri Mendeleev

æ‰¿æ¥ä¸Šç« æœ€åçš„æ•°å€¼æ¡ˆä¾‹ï¼Œæœ¬ç« ä¸»è¦è®²è½¬ç§»æ­¥æ•°è¶‹äºæ— ç©·æ—¶é©¬å°”å¯å¤«é“¾çš„æ€§è´¨ã€‚

### 3.1 Limiting Distribution

#### 3.1.1 å®šä¹‰

ä½œè€…ç»™äº†æœ‰ä¸€ä¸ªå®šä¹‰å’Œä¸‰ä¸ªç­‰ä»·å®šä¹‰ã€‚è¿™ä¸ªå«ä¹‰æœ€æ¸…æ¥šï¼š

> A limiting distribution
for the Markov chain is a probability distribution ğ€ with the property that, for any initial distribution $\boldsymbol{\alpha}$:
> $$\lim_{n\rightarrow\infty}\boldsymbol{\alpha}P^n=\boldsymbol{\lambda}$$

è¿™ä¸ªå®šä¹‰ä¸åŸå®šä¹‰çš„ç­‰ä»·æ€§ä¹Ÿå¾ˆå®¹æ˜“ç†è§£ï¼š

è‹¥å¯¹æŸä¸€é©¬å°”å¯å¤«é“¾çš„çŠ¶æ€è½¬ç§»çŸ©é˜µæœ‰ï¼š
$$\lim_{n\rightarrow\infty}P^n_{ij}=\lambda_j$$
å¹¶ä¸”è®¾åˆå§‹åˆ†å¸ƒ$\boldsymbol{\alpha}=(\alpha_1,\dots,\alpha_n)$,çŠ¶æ€æ€»æ•°ä¸º$m$
åˆ™æœ‰ï¼š

$$
\begin{aligned}
  \lim_{n\rightarrow\infty}\boldsymbol{\alpha}\boldsymbol{P}^n &=(\alpha_1,\dots,\alpha_n)\begin{pmatrix}
   \lambda_1 & \lambda_2 & \cdots &  \lambda_m \\
   \lambda_1 & \lambda_2 & \cdots &  \lambda_m \\
   \vdots & \vdots & \ddots &  \lambda_m \\
   \lambda_1 & \lambda_2 & \cdots &  \lambda_m
  \end{pmatrix} \\ 
  & = \begin{pmatrix}
    \lambda_1\sum_{i=1}^n\alpha_i \\
    \lambda_2\sum_{i=1}^n\alpha_i \\
    \cdots \\
    \lambda_m\sum_{i=1}^n\alpha_i
  \end{pmatrix} \\
  & = \begin{pmatrix}
    \lambda_1 \\
    \lambda_2 \\
    \cdots \\
    \lambda_m
  \end{pmatrix} \\
  & = \boldsymbol{\lambda}
\end{aligned}
$$


Ex3.1 Two state Markov Chain
è®¡ç®—æ¡ˆä¾‹ï¼›

#### 3.1.2 Proportion of Time in Each State

åˆ©ç”¨è®¡ç®—æ¡ä»¶æœŸæœ›çš„æŠ€æœ¯ï¼Œæ¥ä»çŠ¶æ€åœ¨è¿‡ç¨‹ä¸­æ‰€å çš„æ¯”ä¾‹æ¥ç†è§£Limit distribution



Ex3.2

```r
###### Simulate discrete-time Markov chain ########################
# Simulates n steps of a Markov chain 
# markov(init,mat,n,states)
# Generates X0, ..., Xn for a Markov chain with initiial
#  distribution init and transition matrix mat
# Labels can be a character vector of states; default is 1, .... k

markov <- function(init,mat,n,labels) { 
	if (missing(labels)) labels <- 1:length(init)
simlist <- numeric(n+1)
states <- 1:length(init)
simlist[1] <- sample(states,1,prob=init)
for (i in 2:(n+1)) 
	{ simlist[i] <- sample(states,1,prob=mat[simlist[i-1],]) }
labels[simlist]
}
####################################################
P <- matrix(c(0.1,0.2,0.4,0.3,0.4,0,0.4,0.2,0.3,0.3,0,0.4,0.2,0.1,0.4,0.3),
  nrow=4, byrow=TRUE)
lab <- c("Aerobics","Massage","Weights","Yoga")
rownames(P) <- lab
colnames(P) <- lab
P
init <- c(1/4,1/4,1/4,1/4) # initial distribution
states <- c("a","m","w","y")
# simulate chain for 100 steps
simlist <- markov(init,P,100,states)
simlist
table(simlist)/100
steps <- 1000000
simlist <- markov(init,P,steps,states)
table(simlist)/steps
```

### 3.2 Stationary Distribution

#### 3.2.1 å®šä¹‰
æ³¨æ„Stationary Distributionçš„å®šä¹‰æ²¡æœ‰å‡ºç°æé™ã€‚

> If the initial distributino is a stationary distribution, Then $X_0,X_1,\cdots,X_n$ is a sequence of identically distributed random variables. But it doesn't mean that the random variables are independent.

> Lemma 3.1: Limiting Distributions are stationary Distribution

The reverse is false, åä¾‹ï¼š

$$
  P = \begin{pmatrix}
    0 & 1 \\
    1 & 0
  \end{pmatrix}
  ï¼Œ\pi=(\frac{1}{2},\frac{1}{2}) 
$$

ä»¥åŠ

$$
   P = \begin{pmatrix}
    1 & 0 \\
    0 & 1
  \end{pmatrix}
$$

#### 3.2.2 Regular Matrices

ä¸€ä¸ªè‡ªç„¶çš„æƒ³æ³•æ˜¯é—®ï¼Œä»€ä¹ˆæ ·çš„æ¡ä»¶ä¸‹ï¼Œä¸€ä¸ªé©¬å°”å¯å¤«é“¾æœ‰æé™åˆ†å¸ƒï¼Œè€Œä¸”æé™åˆ†å¸ƒå°±æ˜¯å¹³ç¨³åˆ†å¸ƒå‘¢ã€‚

æ»¡è¶³å¦‚ä¸‹æ€§è´¨çš„é©¬å°”å¯å¤«é“¾æ˜¯ç¬¦åˆè¿™ä¸ªè¦æ±‚çš„

> Regular Transition Matrix
> A transition matrix $\boldsymbol{P}$ is said to be regular if some power of $\boldsymbol{P}$ is positive. That is $\boldsymbol{P}^n > 0 $, for some $n\ge 1$

æœ‰å®šç†ï¼š

> Theorem 3.2: A markov chain whose transition matrix $\boldsymbol{P}$ is regular has a limiting distribution, which is teh unique, positive, stationary distribution of the chanin.

Ex 3.3-3.4 å…·ä½“ç®—ä¾‹ï¼Œ

#### 3.2.3 Finding the stationary distribution

æœ¬è´¨ä¸Šè¿™æ˜¯ä¸€ä¸ªç‰¹å¾å€¼é—®é¢˜ã€‚

```r
### Stationary distribution of discrete-time Markov chain
###  (uses eigenvectors)
###
stationary <- function(mat) {
x = eigen(t(mat))$vectors[,1]
as.double(x/sum(x))
}
```

Ex 3.5-3.6; è®¡ç®—æŠ€å·§ï¼Œä»¤$x_1=1$

Ex 3.7 The Ehrenfest dog-flea model

Ex 3.8 Random walk on a graph;

On weighted graph

> Stationiary Distribution for Random walk on a weighted graph
> 
> Let $G=(V,E)$ be  a weighted graph with edge weight function $w(i,j)$. For random walk on G, the stationary distribution $pi$ is proportion to the sum of teh edge weights incident to each vertex. That is.
> $$ \pi = \frac{w(v)}{\sum_{z}w(z)},\forall v\in V$$ 
> where
> $$ w(v) = \sum_{z \sim v }w(v,z) $$

On simple graph

> Stationary Distribution for simple Random Walk on a graph
> 
> For simple random walk on a weighted graph, set $w(i,j)=1,\forall i,j \in V $, then, $w(v)=deg(v)$,which gives
> $$ \pi_{v}=\frac{\deg(v)}{\sum_z \deg(z)}=\frac{\deg(v)}{2e} $$

Ex 3.9-3.10 å¦‚ä½•è®¡ç®—çš„æ¡ˆä¾‹ï¼›

#### 3.2.4 The Eigenvalue Connection

è½¬ç½®ï¼Œçœ‹å‡ºä¸ç‰¹å¾å€¼çš„å…³è”ã€‚

Ex 3.10 ç†è®ºæ¡ˆä¾‹ï¼š random walk in regular graph.ã€‚

### 3.3 Can you find the way to state $a$

#### 3.3.1 çŠ¶æ€å¯åˆ°è¾¾ä¸çŠ¶æ€äº’é€š

> Say that state $j$ is accessible from state i, if $P_{ij}^n > 0$. That is,there is positive probability of reaching $j$ from $i$ in a finite number of steps. State $i$ and $j$ communicate if $i$ is accessible from $j$ and $j$ is accessible from $i$

eg 3.11 æœ¬ä¾‹è®²è¿°äº†ç”¨ Transition graphs å±•ç¤º Communication classes

#### 3.3.2 ä¸å¯çº¦

> Irreducibility
> A Markov chain is called irreducible if it has exactly one cmmunication class. That is, all states communicate with each other

Ex 3.12 ä¸€ä¸ªä¸å¯çº¦é“¾çš„ä¾‹å­ï¼›

#### 3.3.3 Recurrence and Transience

> Given a Markov chain $X_0,X_1,\dots$, let $T_j=\min\{n>0:X_n=j\}$be the first passage time to state $j$. If $X_n\ne j,\forall n>0$, see$T_j=\infty$. Let
> $$ f_j = P(T_j < \infty | X_0=j)$$
> be the probability started in $j$ eventually returns to $j$.
> 
> State $j$ is said to be recurrent if the Markov chain started in $j$ eventually revists $j$. That is $f_j=1$
> 
> State $j$ is said to be transient if there is positive probability that the Markov chain started in j never returns to $j$. That is $f_j < 1$

å¦‚ä½•æ ¹æ®çŠ¶æ€è½¬ç§»çŸ©é˜µåˆ¤å®šï¼ŒæŸä¸€ä¸ªçŠ¶æ€æ˜¯Recurrentæˆ–Transient Statesã€‚ç”¨ç¤ºæ€§å‡½æ•°ï¼Œ

$E(\sum_{n=0}^{\infty}I_n)=\sum_{n=0}^{\infty}E(I_n)=\sum_{n=0}^{\infty}P(X_n=j|X_0=i)=\sum_{n=0}^{\infty}P_{ij}^{n}$

ç”±æ­¤å¯ä»¥æ¨å‡ºå¦å¤–ä¸€ä¸ªåˆ¤å®šæ¡ä»¶ï¼›

> Recurrence, Transience
> 
> (i) State $j$ is recurrent if and only if
> $$ \sum_{n=0}^{\infty}P_{ij}^n=\infty $$
> 
> (ii) State j is transient if and only if
> 
> $$\sum_{n=0}^{\infty}P_{ij}^n<\infty$$

> Recuurence and Transience are Class Properties
> 
> Theorem 3.3 The states of a communication class are either all recurrent or all transient.
> Corollary 3.4 For a finite irreducible Markov chain, all states are recuurent.

Ex 3.13 æ¥ä¸‹æ¥çš„ä¾‹å­æ˜¯ç®€å•çš„ä¸€ç»´éšæœºæ¸¸èµ°ï¼›è¿™ä¸ªä¾‹å­å¯ä»¥æ¨å¹¿åˆ°é«˜ç»´ã€‚

#### 3.3.4 Canonical Decomposition

Closed Communication Class

> Lemma 3.5 A communication class is closed if it consists of all recurrent states. A finite communication class is closed only if it consits of all recurrent states.

åè¯æ³•å³å¯è¯å¾—ï¼›æœ€åä¾¿å¯ä»¥å¾—åˆ°ï¼Œæˆ‘ä»¬æƒ³å®šä¹‰çš„ï¼›

> The state space S of a finite Markov chain can be partitioned into transient and reccurent states as $S=T \cup R_1 \cup \cdots R_m$, where T is the set of all transient states and $R_i$ are closed communiction classes of recurrent states. This is called the canonical decomposition.

æ³¨ï¼šç”±ç­‰ä»·ç±»çš„å®šä¹‰å¯ä»¥ä¿éšœè¿™ä¹ˆé‡æ’çŠ¶æ€è½¬ç§»çŸ©é˜µï¼Œæ˜¯ä¸åŸçŸ©é˜µç­‰ä»·çš„ã€‚

> Given a canonical decomposition, the state space can be reordered so that the Markov transition matrix has the block matrix form

$$
	\boldsymbol{P}=
	\left(
	\begin{array}{c|c}
	\boldsymbol{Q} & \ast & \ast & \cdots & \ast \\ \hline 
	\boldsymbol{O} & \boldsymbol{P_1} & \boldsymbol{O} &\cdots & \boldsymbol{O}\\
  \boldsymbol{O} & \boldsymbol{O} & \boldsymbol{P_2} &\cdots & \boldsymbol{O} \\
  \vdots & \vdots & \vdots &\ddots & \vdots \\
  \boldsymbol{O} & \boldsymbol{O} & \boldsymbol{O} &\cdots & \boldsymbol{P_m}
	\end{array}
	\right)
$$

å…¶ä¸­$\boldsymbol{O}=(p_{ij}=0),\boldsymbol{Q}=(p_{ij})_{i,j \in T},\boldsymbol{P_l}=(p_{ij})_{i,j \in R_l},l=1,2,\cdots,m$

Ex3.14 å…·ä½“ case;

æ›´è¿›ä¸€æ­¥æœ‰ï¼š

$$
	\lim_{n\rightarrow\infty} \boldsymbol{P}^n=
	\left(
	\begin{array}{c|c}
	\boldsymbol{O} & \ast & \ast & \cdots & \ast \\ \hline 
	\boldsymbol{O} & \lim_{n\rightarrow\infty}\boldsymbol{P_1}^n & \boldsymbol{O} &\cdots & \boldsymbol{O}\\
  \boldsymbol{O} & \boldsymbol{O} & \lim_{n\rightarrow\infty}\boldsymbol{P_2}^n &\cdots & \boldsymbol{O} \\
  \vdots & \vdots & \vdots &\ddots & \vdots \\
  \boldsymbol{O} & \boldsymbol{O} & \boldsymbol{O} &\cdots & \lim_{n\rightarrow\infty}\boldsymbol{P_m}^n
	\end{array}
	\right)
$$

---
edit: 2020-05-05

### 3.4 Irreducible Markov Chains

#### 3.4.1 å®šç†

å‡è®¾åœ°éœ‡å°é€šè¿‡ç»Ÿè®¡æ•°æ®å¾—åˆ°äº†ä¸€ä¸ªä¸åŒçº§åˆ«çš„åœ°éœ‡çš„çŠ¶æ€æ¦‚ç‡çŸ©é˜µï¼Œæ”¿åºœå’Œå¤§ä¼—å…³å¿ƒçš„é—®é¢˜æ˜¯ï¼Œä»ä¸Šä¸€æ¬¡åœ°éœ‡åˆ°å³å°†å‘ç”Ÿçš„ä¸‹ä¸€æ¬¡åœ°éœ‡ï¼Œå¤§æ¦‚éœ€è¦å¤šå°‘å¹´ï¼Ÿ

ä¸€ä¸ªæ˜¾ç„¶çš„ç›´è§‰ä¸Šçš„æ„Ÿè§‰æ˜¯ï¼Œæ ¹æ®åœ°éœ‡çš„å¼ºåº¦è€Œå®šã€‚ä¸åŒå¼ºåº¦ä¸åŒã€‚

å¯ä»¥ç”¨é©¬å°”å¯å¤«é“¾ä½œä¸ºä¸Šè¿°é—®é¢˜çš„æ•°å­¦æ¨¡å‹ã€‚

å›é¡¾åœæ—¶çš„å®šä¹‰$T_j = \min\{n>0:X_n=j\}$ï¼Œæœ‰å¦‚ä¸‹å®šç†ï¼š

> **Theorem 3.6 Limit Theorem for Finite Irreducible Markov Chains**. Assume that $X_0,X_1,\dots$, is a finite irreducible Markov Chain. For each state $j$. let $\mu_j=E(T_j|X_0=j)$ be the expected return time to $j$. Then, $\mu_j$ is finite, and there exisits a unique, positve stationary distribution $\boldsymbol{\pi}$ such that 
> $$ \pi_j =\frac{1}{\mu_j} , \forall j$$
> Furthermore, $\forall i$
> $$ \pi_j = \lim_{n\rightarrow\infty}\frac{1}{n}\sum_{m=0}^{n-1}P^m_{ij} $$


Ex 3.15 åœ°éœ‡çš„ recurrenceçš„ä¾‹å­ï¼›

Ex 3.16 Frog jumping ç®—ä¾‹ï¼›

#### 3.4.2 First-Step Analysis

Ex 3.17 ä¸€ä¸ªè®¡ç®—Expected return timeçš„ä¾‹å­ï¼›

### 3.5 Periodicity

å¦‚æœè¯¥é©¬å°”å¯å¤«é“¾æ²¡æœ‰å¹³ç¨³åˆ†å¸ƒï¼Œè¯¥å¦‚ä½•ç ”ç©¶ï¼Ÿ

å…·ä½“çš„æ¡ˆä¾‹å¯ä»¥çœ‹ç¬¬ä¸€ç« æœ€åçš„é‚£å‡ ä¸ªç®—ä¾‹ã€‚

ç­”æ¡ˆæ˜¯å¼•å…¥å‘¨æœŸçš„æ¦‚å¿µã€‚

> **Lemma 3.7** The states of a communication class all have the same period

æ ¹æ®è¿™æ¡å¼•ç†ï¼Œæœ‰

> A Markov chain is called perioidic if is irreducible and all states have period greater than 1. A Markov chain is called aperiodic if it is irreducible and all states have period equal 1.

### 3.6 Ergoidc Markov Chains 

A Markov chain is called ergodic if it is irreducible, aperiodic, and all states have finite expected return times.

> **Fundamental Limit Theorem for Ergodic Markov Chains**
> **Therem 3.8** Let $X_0,X_1,\cdots$ be an ergodic Markov chain. There exists a unique, positive, stationary distribution $\boldsymbol{\pi}$,which is the limiting distribution of the chain. That is, 
> $$\pi_j = \lim_{n\rightarrow\infty}P^n_{ij},\forall i,j$$

Ex 3.19ï¼ˆModified Ehrenfrest Modelï¼‰; 

Ex 3.20 ç®—ä¾‹;

Ex 3.21 PageRank; (damping factor,p=0.85)

---
edit: 2020-05-06

### 3.7 Time reversibility

#### 3.7.1 Definition

The property of time reversibility can be explained intuitively as follows. If you were to take a movie of Markov chain moving forward in time and then run the movie backwards, you could not tell the difference between the two.

æ¢æˆæ•°å­¦ä¸Šçš„è¯­è¨€å°±æ˜¯ï¼Œå¦‚æœå‡è®¾é©¬å°”å¯å¤«é“¾å¤„äºç¨³æ€ï¼Œè¿™æ—¶å­˜åœ¨ï¼š

$$ P(X_0=i,X_1=j)=P(X_0=j,X_1=i) $$

ç”±å…¨æ¦‚ç‡å…¬å¼å¯çŸ¥ï¼›

> **Time Reversibility**
> 
> An irreducible Markov chain with transition matrix P and stationary distribution $\boldsymbol{\pi}$, if
> $$ \pi_iP_{ij}=\pi_jP_{ji},\forall i,j $$


Ex 3.23; 

Ex 3.24ï¼›Simple random walk on a graph is time 



#### 3.7.2 Reversible Markov Chains and Radom walk

Every reversible Markov chain can be considered as a random walk on a weighted graph

Ex 3.25 ç®—ä¾‹ï¼›

#### 3.7.3 The key benifit of reversibility

> **Proposition 3.9** Let $\boldsymbol{P}$ be the transition matrix of a Markov chain. If $\boldsymbol{x}$ is a probability distribution which satisfies
> $$ x_iP_{ij}=x_jP_{ji},\forall i,j $$
> then $\boldsymbol{x}$ is the stationary distribution, and the markov chain is reversible.

Ex 3.26 **Birth-and-death chain**

Case: random walk with a partialy relecting boundary.

### 3.8 Absorbing Chains

#### 3.8.1 å®šä¹‰

> **Absorbing State, Absorbing Chain**
> 
> State $i$ is an absorbing state if $P_{ii}=1$. A Markov chain is called an absorbing chain if it has at leat one absorbing state.

æ ¹æ®è¿™ä¸ªå®šä¹‰ï¼Œä¸€ä¸ªå¸æ”¶çš„é©¬å°”å¯å¤«é“¾çš„canoical decompostionå¯ä»¥å†™ä¸ºï¼š
$$
  \boldsymbol{P} = \lim_{n\rightarrow\infty} \boldsymbol{P}^n=
	\left(
	\begin{array}{c|c}
	\boldsymbol{Q} & \boldsymbol{R} \\
  \hline
  \boldsymbol{O} & \boldsymbol{I}
	\end{array}
	\right)
$$

Ex 3.31

#### 3.8.2 Expected Number of Visits to Transient States

> **Theorem 3.11** Consider an absorbing Markov chain with t transient states. Let $\boldsymbol{F}$ be a $t\times t$ matrix indexed by transient states. where $F_{ij}$ is the expected number of visits to $j$ given that the chain starts in $i$, Then, 
> $$F=(I-Q)^{-1}$$

#### 3.8.3 Expected Time to Absorption

> **Absorbing Markov Chains**
> For an absorbing Markov chain with all states either transient or absorbing. Let $F=(I-Q)^{-1}$
> 1. (Absorption probability) The probability that from transient state $i$ the chain is absorbed in state $j$ is $(FR)_{ij}$
> 2. (Absorption time) The expected number of steps from transient state $i$ until the chain is absorbed in some absorbing state is $(F1)_i$  

#### 3.8.4 Expected Hitting Time for Irreducible chain

#### 3.8.5 Patterns in Sequences

#### 3.9 Regenration and strong Markov property

#### 3.10 Proofs of limiting Theorem

å‰©ä¸‹çš„éƒ½æ˜¯å¸¸è§„å†…å®¹ã€‚ä¸å†èµ˜è¿°ã€‚

æ€»çš„æ¥è¯´ï¼Œè¿™ä¸€ç« çš„ç« èŠ‚ç»„ç»‡å¾ˆæœ‰æ¡ç†ï¼Œå¯¹åˆå­¦è€…å‹å¥½ã€‚è€Œä¸”é€‰çš„ä¾‹å­å¾ˆæœ‰å¯å‘æ€§ã€‚

---
é¢˜å¤–è¯ï¼š

æœ‰ä¸ªå€¼å¾—æ€è€ƒçš„é—®é¢˜ï¼Œè¿™ç§åå‘åº”ç”¨çš„æ•°å­¦å†…å®¹çš„æ•™æï¼Œå¦‚ä½•å¹³è¡¡è®ºè¿°çš„é€»è¾‘ï¼Œç†è®ºä»¥åŠæ€è€ƒçš„æ·±åº¦ï¼Œä»¥åŠåº”ç”¨çš„å¹¿åº¦ï¼Œä»¥åŠå¯¹äºè¯»è€…çš„å¸å¼•æ€§å’Œå®ç”¨æ€§ï¼Œéƒ½æ˜¯å¾ˆéœ€è¦åŠŸåº•çš„ã€‚ä½†æ˜¯è¿™æ–¹é¢åšçš„å¥½çš„æ•™æçœŸçš„å¤ªå°‘äº†ã€‚

ç†æƒ³çš„å¤§å­¦æ•™å¸ˆï¼Œæ˜¯å­¦æœ¯æˆå°±å’Œæ•™å­¦æˆå°±éƒ½å¾ˆå‡ºè‰²ï¼Œä½†æ˜¯è¿™æ¯•ç«Ÿæ˜¯å°‘æ•°ã€‚

æœ‰ä¸€å¯¹å„¿ç›¸äº’çŸ›ç›¾çš„å‘½é¢˜ï¼š

+ å¤§å­¦ç”Ÿåº”è¯¥æé«˜è‡ªå­¦èƒ½åŠ›ï¼Œä¸è¦æŒ‡æœ›è€å¸ˆæ‰‹æŠŠæ‰‹çš„æ•™ï¼Ÿ
+ å›½å®¶ç»™äº†å¤§å­¦è€å¸ˆå·¥èµ„ï¼Œå¤§å­¦ç”Ÿä¹Ÿä»˜äº†å­¦è´¹ï¼Œå¦‚æœéƒ½é è‡ªå­¦çš„è¯ï¼Œè¦å¤§å­¦å¹²ä»€ä¹ˆï¼Ÿ

ä¾ç¬”è€…çœ‹æ¥ï¼Œå¤§å­¦æä¾›çš„æ˜¯ä¸€å¥—é€‚åˆå­¦ä¹ å’Œç ”ç©¶çš„ç¡¬ä»¶è®¾æ–½ï¼Œä¸€å¼ å¹³é™çš„ä¹¦æ¡Œï¼Œä¸€ç¾¤å¿—åŒé“åˆçš„è‰¯å¸ˆç›Šå‹ã€‚è¿™äº›ç¯å¢ƒå’Œäººï¼Œæ˜¯ä»»ä½•å…¶å®ƒæœºæ„æˆ–è€…ç½‘è¯¾ä»£æ›¿ä¸äº†çš„ã€‚